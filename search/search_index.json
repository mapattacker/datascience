{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This documentation summarises various machine learning techniques in Python. While they cover the entire process of machine learning, only classical models are introduced. Neural networks & other important concepts like model deployment are not covered here. I have two accompanying websites to fill in those gaps: Neural Networks (WIP) AI Engineering A lot of the content are compiled from various resources, so please cite them appropriately if you are using.","title":"Introduction"},{"location":"#introduction","text":"This documentation summarises various machine learning techniques in Python. While they cover the entire process of machine learning, only classical models are introduced. Neural networks & other important concepts like model deployment are not covered here. I have two accompanying websites to fill in those gaps: Neural Networks (WIP) AI Engineering A lot of the content are compiled from various resources, so please cite them appropriately if you are using.","title":"Introduction"},{"location":"data-classimbalance/","text":"Class Imbalance In domains like predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. Techniques are available to correct for this. The imbalance-learn package ( pip install -U imbalanced-learn ) provides an excellent range of algorithms for adjusting for imbalanced data. An important thing to note is that resampling must be done AFTER the train-test split , so as to prevent data leakage. Over-Sampling SMOTE (synthetic minority over-sampling technique) is a common and popular up-sampling technique. from imblearn.over_sampling import SMOTE smote = SMOTE () X_resampled , y_resampled = smote . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) ADASYN is one of the more advanced over sampling algorithms. from imblearn.over_sampling import ADASYN ada = ADASYN () X_resampled , y_resampled = ada . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) Under-Sampling from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler () X_resampled , y_resampled = rus . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) Cost-Sensitive Classification One can also make the classifier aware of the imbalanced data by incorporating the weights of the classes into a cost function. Intuitively, we want to give higher weight to minority class and lower weight to majority class. http://albahnsen.github.io/CostSensitiveClassification/index.html","title":"Class Imbalance"},{"location":"data-classimbalance/#class-imbalance","text":"In domains like predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. Techniques are available to correct for this. The imbalance-learn package ( pip install -U imbalanced-learn ) provides an excellent range of algorithms for adjusting for imbalanced data. An important thing to note is that resampling must be done AFTER the train-test split , so as to prevent data leakage.","title":"Class Imbalance"},{"location":"data-classimbalance/#over-sampling","text":"SMOTE (synthetic minority over-sampling technique) is a common and popular up-sampling technique. from imblearn.over_sampling import SMOTE smote = SMOTE () X_resampled , y_resampled = smote . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) ADASYN is one of the more advanced over sampling algorithms. from imblearn.over_sampling import ADASYN ada = ADASYN () X_resampled , y_resampled = ada . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled )","title":"Over-Sampling"},{"location":"data-classimbalance/#under-sampling","text":"from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler () X_resampled , y_resampled = rus . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled )","title":"Under-Sampling"},{"location":"data-classimbalance/#cost-sensitive-classification","text":"One can also make the classifier aware of the imbalanced data by incorporating the weights of the classes into a cost function. Intuitively, we want to give higher weight to minority class and lower weight to majority class. http://albahnsen.github.io/CostSensitiveClassification/index.html","title":"Cost-Sensitive Classification"},{"location":"data-engineering/","text":"Feature Engineering Feature Engineering is one of the most important part of model building. Collecting and creating of relevant features from existing ones are most often the determinant of a high prediction value. They can be classified broadly as follows. Type Desc Aggregations recalculation of a column in the feature by calculation before/after it Transformations change a feature to something meaningful, e.g., address to its spatial coordinates Decompositions break a feature into several ones, e.g. time series decomposition Interactions new feature created by interacting between two or more features Feature engineering usually require a good understand of the domain in order to generate useful features. Below are just some non-exhaustive examples to get you started. Decomposition Datetime Breakdown Very often, various dates and times of the day have strong interactions with your predictors. Here\u2019s a script to pull those values out. def extract_time ( df ): df [ 'timestamp' ] = pd . to_datetime ( df [ 'timestamp' ]) df [ 'hour' ] = df [ 'timestamp' ] . dt . hour df [ 'mth' ] = df [ 'timestamp' ] . dt . month df [ 'day' ] = df [ 'timestamp' ] . dt . day df [ 'dayofweek' ] = df [ 'timestamp' ] . dt . dayofweek return df To get holidays, use the package holidays. import holidays train [ 'holiday' ] = train [ 'timestamp' ] . apply ( lambda x : 0 if holidays . US () . get ( x ) is None else 1 ) Time Series Decomposition This is a popular decomposition method for time-series, whereby it is divided into trend (long-term), seaonality (short-term), residuals (noise). There are two methods to decompose: Type Desc Additive The component is present and is added to the other components to create the overall forecast value Multiplicative The component is present and is multiplied by the other components to create the overall forecast value Usually, an additive time-series will be used if there are no seasonal variations over time. import statsmodels.api as sm import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline res = sm . tsa . seasonal_decompose ( final2 [ 'avg_mth_elect' ], \\ model = 'multiplicative' ) # plot res . plot () # set decomposed parts into dataframe decomp = pd . concat ([ res . observed , res . trend , res . seasonal , res . resid ], axis = 1 ) decomp . columns = [ 'avg_mth' , 'trend' , 'seasonal' , 'residual' ] decomp . head () Automated Feature Engineering FeatureTools FeatureTools is extremely useful if you have datasets with a base data, with other tables that have relationships to it. We first create an EntitySet, which is like a database. Then we create entities, i.e., individual tables with a unique id for each table, and showing their relationships between each other. import featuretools as ft def make_entityset ( data ): es = ft . EntitySet ( 'Dataset' ) es . entity_from_dataframe ( dataframe = data , entity_id = 'recordings' , index = 'index' , time_index = 'time' ) es . normalize_entity ( base_entity_id = 'recordings' , new_entity_id = 'engines' , index = 'engine_no' ) es . normalize_entity ( base_entity_id = 'recordings' , new_entity_id = 'cycles' , index = 'time_in_cycles' ) return es es = make_entityset ( data ) es We then use something called Deep Feature Synthesis (dfs) to generate features automatically. Primitives are the type of new features to be extracted from the datasets. They can be aggregations (data is combined) or transformation (data is changed via a function) type of extractors. The list can be found via ft.primitives.list_primitives(). External primitives like tsfresh, or custom calculations can also be input into FeatureTools. feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'normal' , agg_primitives = [ 'last' , 'max' , 'min' ], trans_primitives = [], max_depth = 2 , verbose = 1 , n_jobs = 3 ) # see all old & new features created feature_matrix . columns FeatureTools appears to be a very powerful auto-feature extractor. Some resources to read further are as follows: brendanhasz towardsdatascience medium tsfresh tsfresh is a feature extraction package for time-series. It can extract more than 1200 different features, and filter out features that are deemed relevant. In essence, it is a univariate feature extractor. To extract all possible features... from tsfresh import extract_features def list_union_df ( fault_list ): \"\"\"Convert list of faults with a single signal value into a dataframe with an id for each fault sample Data transformation prior to feature extraction \"\"\" # convert nested list into dataframe dflist = [] # give an id field for each fault sample for a , i in enumerate ( verified_faults ): df = pd . DataFrame ( i ) df [ 'id' ] = a dflist . append ( df ) df = pd . concat ( dflist ) return df df = list_union_df ( fault_list ) # tsfresh extracted_features = extract_features ( df , column_id = 'id' ) # delete columns which only have one value for all rows for i in extracted_features . columns : col = extracted_features [ i ] if len ( col . unique ()) == 1 : del extracted_features [ i ] To generate only relevant features... from tsfresh import extract_relevant_features # y = is the target vector # length of y = no. of samples in timeseries, not length of the entire timeseries # column_sort = for each sample in timeseries, time_steps column will restart # fdr_level = false discovery rate, is default at 0.05, # it is the expected percentage of irrelevant features # tune down to reduce number of created features retained, tune up to increase features_filtered_direct = extract_relevant_features ( timeseries , y , column_id = 'id' , column_sort = 'time_steps' , fdr_level = 0.05 )","title":"Feature Engineering"},{"location":"data-engineering/#feature-engineering","text":"Feature Engineering is one of the most important part of model building. Collecting and creating of relevant features from existing ones are most often the determinant of a high prediction value. They can be classified broadly as follows. Type Desc Aggregations recalculation of a column in the feature by calculation before/after it Transformations change a feature to something meaningful, e.g., address to its spatial coordinates Decompositions break a feature into several ones, e.g. time series decomposition Interactions new feature created by interacting between two or more features Feature engineering usually require a good understand of the domain in order to generate useful features. Below are just some non-exhaustive examples to get you started.","title":"Feature Engineering"},{"location":"data-engineering/#decomposition","text":"","title":"Decomposition"},{"location":"data-engineering/#datetime-breakdown","text":"Very often, various dates and times of the day have strong interactions with your predictors. Here\u2019s a script to pull those values out. def extract_time ( df ): df [ 'timestamp' ] = pd . to_datetime ( df [ 'timestamp' ]) df [ 'hour' ] = df [ 'timestamp' ] . dt . hour df [ 'mth' ] = df [ 'timestamp' ] . dt . month df [ 'day' ] = df [ 'timestamp' ] . dt . day df [ 'dayofweek' ] = df [ 'timestamp' ] . dt . dayofweek return df To get holidays, use the package holidays. import holidays train [ 'holiday' ] = train [ 'timestamp' ] . apply ( lambda x : 0 if holidays . US () . get ( x ) is None else 1 )","title":"Datetime Breakdown"},{"location":"data-engineering/#time-series-decomposition","text":"This is a popular decomposition method for time-series, whereby it is divided into trend (long-term), seaonality (short-term), residuals (noise). There are two methods to decompose: Type Desc Additive The component is present and is added to the other components to create the overall forecast value Multiplicative The component is present and is multiplied by the other components to create the overall forecast value Usually, an additive time-series will be used if there are no seasonal variations over time. import statsmodels.api as sm import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline res = sm . tsa . seasonal_decompose ( final2 [ 'avg_mth_elect' ], \\ model = 'multiplicative' ) # plot res . plot () # set decomposed parts into dataframe decomp = pd . concat ([ res . observed , res . trend , res . seasonal , res . resid ], axis = 1 ) decomp . columns = [ 'avg_mth' , 'trend' , 'seasonal' , 'residual' ] decomp . head ()","title":"Time Series Decomposition"},{"location":"data-engineering/#automated-feature-engineering","text":"","title":"Automated Feature Engineering"},{"location":"data-engineering/#featuretools","text":"FeatureTools is extremely useful if you have datasets with a base data, with other tables that have relationships to it. We first create an EntitySet, which is like a database. Then we create entities, i.e., individual tables with a unique id for each table, and showing their relationships between each other. import featuretools as ft def make_entityset ( data ): es = ft . EntitySet ( 'Dataset' ) es . entity_from_dataframe ( dataframe = data , entity_id = 'recordings' , index = 'index' , time_index = 'time' ) es . normalize_entity ( base_entity_id = 'recordings' , new_entity_id = 'engines' , index = 'engine_no' ) es . normalize_entity ( base_entity_id = 'recordings' , new_entity_id = 'cycles' , index = 'time_in_cycles' ) return es es = make_entityset ( data ) es We then use something called Deep Feature Synthesis (dfs) to generate features automatically. Primitives are the type of new features to be extracted from the datasets. They can be aggregations (data is combined) or transformation (data is changed via a function) type of extractors. The list can be found via ft.primitives.list_primitives(). External primitives like tsfresh, or custom calculations can also be input into FeatureTools. feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'normal' , agg_primitives = [ 'last' , 'max' , 'min' ], trans_primitives = [], max_depth = 2 , verbose = 1 , n_jobs = 3 ) # see all old & new features created feature_matrix . columns FeatureTools appears to be a very powerful auto-feature extractor. Some resources to read further are as follows: brendanhasz towardsdatascience medium","title":"FeatureTools"},{"location":"data-engineering/#tsfresh","text":"tsfresh is a feature extraction package for time-series. It can extract more than 1200 different features, and filter out features that are deemed relevant. In essence, it is a univariate feature extractor. To extract all possible features... from tsfresh import extract_features def list_union_df ( fault_list ): \"\"\"Convert list of faults with a single signal value into a dataframe with an id for each fault sample Data transformation prior to feature extraction \"\"\" # convert nested list into dataframe dflist = [] # give an id field for each fault sample for a , i in enumerate ( verified_faults ): df = pd . DataFrame ( i ) df [ 'id' ] = a dflist . append ( df ) df = pd . concat ( dflist ) return df df = list_union_df ( fault_list ) # tsfresh extracted_features = extract_features ( df , column_id = 'id' ) # delete columns which only have one value for all rows for i in extracted_features . columns : col = extracted_features [ i ] if len ( col . unique ()) == 1 : del extracted_features [ i ] To generate only relevant features... from tsfresh import extract_relevant_features # y = is the target vector # length of y = no. of samples in timeseries, not length of the entire timeseries # column_sort = for each sample in timeseries, time_steps column will restart # fdr_level = false discovery rate, is default at 0.05, # it is the expected percentage of irrelevant features # tune down to reduce number of created features retained, tune up to increase features_filtered_direct = extract_relevant_features ( timeseries , y , column_id = 'id' , column_sort = 'time_steps' , fdr_level = 0.05 )","title":"tsfresh"},{"location":"data-exploration/","text":"Exploratory Data Analysis Exploratory data analysis (EDA) is an essential step to understand the data better; in order to engineer and select features before modelling. This often requires skills in visualisation to better interpret the data. We can analyse the data from a univariate (single feature) or multivariate (multiple features) perspective. Univariate Distribution Plots When plotting distributions, it is important to compare the distribution of both train and test sets. If the test set very specific to certain features, the model will underfit and have a low accuarcy. import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline for i in X . columns : plt . figure ( figsize = ( 15 , 5 )) sns . distplot ( X [ i ]) sns . distplot ( pred [ i ]) Count Plots For categorical features, you may want to see if they have enough sample size for each category. import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline df [ 'Wildnerness' ] . value_counts () # Comanche Peak 6349 # Cache la Poudre 4675 # Rawah 3597 # Neota 499 # Name: Wildnerness, dtype: int64 cmap = sns . color_palette ( \"Set2\" ) sns . countplot ( x = 'Wildnerness' , data = df , palette = cmap ); plt . xticks ( rotation = 45 ); To check for possible relationships with the target, place the feature under hue. plt . figure ( figsize = ( 12 , 6 )) sns . countplot ( x = 'Cover_Type' , data = wild , hue = 'Wilderness' ); plt . xticks ( rotation = 45 ); fig , axes = plt . subplots ( ncols = 3 , nrows = 1 , figsize = ( 15 , 5 )) # note only for 1 row or 1 col, else need to flatten nested list in axes col = [ 'Winner' , 'Second' , 'Third' ] for cnt , ax in enumerate ( axes ): sns . countplot ( x = col [ cnt ], data = df2 , ax = ax , order = df2 [ col [ cnt ]] . value_counts () . index ); for ax in fig . axes : plt . sca ( ax ) plt . xticks ( rotation = 90 ) Box Plots Using the 50 percentile to compare among different classes, it is easy to find feature that can have high prediction importance if they do not overlap. Also can be use for outlier detection. Features have to be continuous. From different dataframes, displaying the same feature. df = pd . DataFrame ({ 'normal' : normal [ 'Pressure' ], 's1' : cf6 [ 'Pressure' ], 's2' : cf12 [ 'Pressure' ], 's3' : cf20 [ 'Pressure' ], 's4' : cf30 [ 'Pressure' ], 's5' : cf45 [ 'Pressure' ]}) df . boxplot ( figsize = ( 10 , 5 )); Start screen From same dataframe with of a feature split by different y-labels plt . figure ( figsize = ( 7 , 5 )) cmap = sns . color_palette ( \"Set3\" ) sns . boxplot ( x = 'Cover_Type' , y = 'Elevation' , data = df , palette = cmap ); plt . xticks ( rotation = 45 ); Multiple Plots cmap = sns . color_palette ( \"Set2\" ) fig , axes = plt . subplots ( ncols = 2 , nrows = 5 , figsize = ( 10 , 18 )) # axes is nested if >1 row & >1 col, need to flatten a = [ i for i in axes for i in i ] for i , ax in enumerate ( a ): sns . boxplot ( x = 'Cover_Type' , y = eda2 . columns [ i ], data = eda , palette = cmap , width = 0.5 , ax = ax ); # rotate x-axis for every single plot for ax in fig . axes : plt . sca ( ax ) plt . xticks ( rotation = 45 ) # set spacing for every subplot, else x-axis will be covered plt . tight_layout () Multi-Variate Correlation Plots Heatmaps show a quick overall correlation between features. from plotly.offline import iplot from plotly.offline import init_notebook_mode import plotly.graph_objs as go init_notebook_mode ( connected = True ) # create correlation in dataframe corr = df [ df . columns [ 1 :]] . corr () layout = go . Layout ( width = 1000 , height = 600 , \\ title = 'Correlation Plot' , \\ font = dict ( size = 10 )) data = go . Heatmap ( z = corr . values , x = corr . columns , y = corr . columns ) fig = go . Figure ( data = [ data ], layout = layout ) iplot ( fig ) Using Plotly import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline # create correlation in dataframe corr = df [ df . columns [ 1 :]] . corr () plt . figure ( figsize = ( 15 , 8 )) sns . heatmap ( corr , cmap = sns . color_palette ( \"RdBu_r\" , 20 )); Using Seaborn","title":"Data Exploration"},{"location":"data-exploration/#exploratory-data-analysis","text":"Exploratory data analysis (EDA) is an essential step to understand the data better; in order to engineer and select features before modelling. This often requires skills in visualisation to better interpret the data. We can analyse the data from a univariate (single feature) or multivariate (multiple features) perspective.","title":"Exploratory Data Analysis"},{"location":"data-exploration/#univariate","text":"","title":"Univariate"},{"location":"data-exploration/#distribution-plots","text":"When plotting distributions, it is important to compare the distribution of both train and test sets. If the test set very specific to certain features, the model will underfit and have a low accuarcy. import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline for i in X . columns : plt . figure ( figsize = ( 15 , 5 )) sns . distplot ( X [ i ]) sns . distplot ( pred [ i ])","title":"Distribution Plots"},{"location":"data-exploration/#count-plots","text":"For categorical features, you may want to see if they have enough sample size for each category. import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline df [ 'Wildnerness' ] . value_counts () # Comanche Peak 6349 # Cache la Poudre 4675 # Rawah 3597 # Neota 499 # Name: Wildnerness, dtype: int64 cmap = sns . color_palette ( \"Set2\" ) sns . countplot ( x = 'Wildnerness' , data = df , palette = cmap ); plt . xticks ( rotation = 45 ); To check for possible relationships with the target, place the feature under hue. plt . figure ( figsize = ( 12 , 6 )) sns . countplot ( x = 'Cover_Type' , data = wild , hue = 'Wilderness' ); plt . xticks ( rotation = 45 ); fig , axes = plt . subplots ( ncols = 3 , nrows = 1 , figsize = ( 15 , 5 )) # note only for 1 row or 1 col, else need to flatten nested list in axes col = [ 'Winner' , 'Second' , 'Third' ] for cnt , ax in enumerate ( axes ): sns . countplot ( x = col [ cnt ], data = df2 , ax = ax , order = df2 [ col [ cnt ]] . value_counts () . index ); for ax in fig . axes : plt . sca ( ax ) plt . xticks ( rotation = 90 )","title":"Count Plots"},{"location":"data-exploration/#box-plots","text":"Using the 50 percentile to compare among different classes, it is easy to find feature that can have high prediction importance if they do not overlap. Also can be use for outlier detection. Features have to be continuous. From different dataframes, displaying the same feature. df = pd . DataFrame ({ 'normal' : normal [ 'Pressure' ], 's1' : cf6 [ 'Pressure' ], 's2' : cf12 [ 'Pressure' ], 's3' : cf20 [ 'Pressure' ], 's4' : cf30 [ 'Pressure' ], 's5' : cf45 [ 'Pressure' ]}) df . boxplot ( figsize = ( 10 , 5 )); Start screen From same dataframe with of a feature split by different y-labels plt . figure ( figsize = ( 7 , 5 )) cmap = sns . color_palette ( \"Set3\" ) sns . boxplot ( x = 'Cover_Type' , y = 'Elevation' , data = df , palette = cmap ); plt . xticks ( rotation = 45 ); Multiple Plots cmap = sns . color_palette ( \"Set2\" ) fig , axes = plt . subplots ( ncols = 2 , nrows = 5 , figsize = ( 10 , 18 )) # axes is nested if >1 row & >1 col, need to flatten a = [ i for i in axes for i in i ] for i , ax in enumerate ( a ): sns . boxplot ( x = 'Cover_Type' , y = eda2 . columns [ i ], data = eda , palette = cmap , width = 0.5 , ax = ax ); # rotate x-axis for every single plot for ax in fig . axes : plt . sca ( ax ) plt . xticks ( rotation = 45 ) # set spacing for every subplot, else x-axis will be covered plt . tight_layout ()","title":"Box Plots"},{"location":"data-exploration/#multi-variate","text":"","title":"Multi-Variate"},{"location":"data-exploration/#correlation-plots","text":"Heatmaps show a quick overall correlation between features. from plotly.offline import iplot from plotly.offline import init_notebook_mode import plotly.graph_objs as go init_notebook_mode ( connected = True ) # create correlation in dataframe corr = df [ df . columns [ 1 :]] . corr () layout = go . Layout ( width = 1000 , height = 600 , \\ title = 'Correlation Plot' , \\ font = dict ( size = 10 )) data = go . Heatmap ( z = corr . values , x = corr . columns , y = corr . columns ) fig = go . Figure ( data = [ data ], layout = layout ) iplot ( fig ) Using Plotly import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline # create correlation in dataframe corr = df [ df . columns [ 1 :]] . corr () plt . figure ( figsize = ( 15 , 8 )) sns . heatmap ( corr , cmap = sns . color_palette ( \"RdBu_r\" , 20 )); Using Seaborn","title":"Correlation Plots"},{"location":"data-preprocessing/","text":"Feature Preprocessing Missing Values Machine learning models cannot accept null/NaN values. We will need to either remove them or fill them with a logical value. To investigate how many nulls in each column. def null_analysis ( df ): ''' desc: get nulls for each column in counts & percentages arg: dataframe return: dataframe ''' null_cnt = df . isnull () . sum () # calculate null counts null_cnt = null_cnt [ null_cnt != 0 ] # remove non-null cols null_percent = null_cnt / len ( df ) * 100 # calculate null percentages null_table = pd . concat ([ pd . DataFrame ( null_cnt ), pd . DataFrame ( null_percent )], axis = 1 ) null_table . columns = [ 'counts' , 'percentage' ] null_table . sort_values ( 'counts' , ascending = False , inplace = True ) return null_table # visualise null table import plotly_express as px null_table = null_analysis ( weather_train ) px . bar ( null_table . reset_index (), x = 'index' , y = 'percentage' , text = 'counts' , height = 500 ) Threshold It makes no sense to fill in the null values if there are too many of them. We can set a threshold to delete the entire column if there are too many nulls. def null_threshold ( df , threshold = 25 ): ''' desc: delete columns based on a null percentage threshold arg: df=dataframe; threshold=percentage of nulls in column return: dataframe ''' null_table = null_analysis ( df ) null_table = null_table [ null_table [ 'percentage' ] >= 25 ] df . drop ( null_table . index , axis = 1 , inplace = True ) return df Impute We can change missing values for the entire dataframe into their individual column means or medians. import pandas as pd import numpy as np from sklearn.impute import SimpleImputer imp_mean = SimpleImputer ( missing_values = np . nan , strategy = 'median' , copy = False ) imp_mean . fit ( df ) # output is in numpy, so convert to df df2 = pd . DataFrame ( imp_mean . transform ( df ), columns = df . columns ) Interpolation We can also use interpolation via pandas default function to fill in the missing values. import pandas as pd # limit: Maximum number of consecutive NaNs to fill. # Must be greater than 0. df [ 'colname' ] . interpolate ( method = 'linear' , limit = 2 ) Outliers Especially sensitive in linear models. They can be (1) removed manually by defining the lower and upper bound limit, or (2) grouping the features into ranks. Below is a simple method to detect & remove outliers that is defined by being outside a boxplot's whiskers. def boxplot_outlier_removal ( X , exclude = [ '' ]): ''' remove outliers detected by boxplot (Q1/Q3 -/+ IQR*1.5) Parameters ---------- X : dataframe dataset to remove outliers from exclude : list of str column names to exclude from outlier removal Returns ------- X : dataframe dataset with outliers removed ''' before = len ( X ) # iterate each column for col in X . columns : if col not in exclude : # get Q1, Q3 & Interquantile Range Q1 = X [ col ] . quantile ( 0.25 ) Q3 = X [ col ] . quantile ( 0.75 ) IQR = Q3 - Q1 # define outliers and remove them filter_ = ( X [ col ] > Q1 - 1.5 * IQR ) & ( X [ col ] < Q3 + 1.5 * IQR ) X = X [ filter_ ] after = len ( X ) diff = before - after percent = diff / before * 100 print ( ' {} ( {:.2f} %) outliers removed' . format ( diff , percent )) return X There are also unsupervised modelling techniques to find outliers. Encodings Label Encoding This converts a category of a label or feature into integer values. There are two methods to do this, via alphabetical order ( sklearn.preprocessing.LabelEncoder ), or order of appearance ( pd.factorize ). from sklearn import preprocessing from pandas import pd df = DataFrame ([ 'A' , 'B' , 'B' , 'C' ], columns = [ 'Col' ]) # factorise df [ 'Fact' ] = pd . factorize ( df [ 'Col' ])[ 0 ] # label encoder le = preprocessing . LabelEncoder () df [ 'Lab' ] = le . fit_transform ( df [ 'Col' ]) print ( df ) # Col Fact Lab # 0 A 0 0 # 1 B 1 1 # 2 B 1 1 # 3 C 2 2 Frequency Encoding This involves the conversion of categories into frequencies. # size of each category encoding = titanic . groupby ( 'Embarked' ) . size () # get frequency of each category encoding = encoding / len ( titanic ) titanic [ 'enc' ] = titanic . Embarked . map ( encoding ) # if categories have same frequency it can be an issue # will need to change it to ranked frequency encoding from scipy.stats import rankdata One-Hot Encoding We could use an integer encoding directly, rescaled where needed. This may work for problems where there is a natural ordinal relationship between the categories, and in turn their integer values, such as labels for temperature \u2018cold\u2019, warm\u2019, and \u2018hot\u2019. However, there may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels \u2018dog\u2019 and \u2018cat\u2019. This is especially so for non-tree based models. Again this can be done two ways. get_dummies converts the result into a pandas dataframe, while sklearn's OneHotEncoder converts into a numpy array. import pandas as pd pd . get_dummies ( data = df , columns = [ 'A' , 'B' ]) from sklearn.preprocessing import OneHotEncoder onehotencoder = OneHotEncoder ( categorical_features = \"all\" ) X = onehotencoder . fit_transform ( X ) . toarray () Coordinates It is necessary to define a projection for a coordinate reference system if there is a classification in space, e.g. k-means clustering. This basically change the coordinates from a spherical component to a flat surface. Also take note of spatial auto-correlation.","title":"Feature Preprocessing"},{"location":"data-preprocessing/#feature-preprocessing","text":"","title":"Feature Preprocessing"},{"location":"data-preprocessing/#missing-values","text":"Machine learning models cannot accept null/NaN values. We will need to either remove them or fill them with a logical value. To investigate how many nulls in each column. def null_analysis ( df ): ''' desc: get nulls for each column in counts & percentages arg: dataframe return: dataframe ''' null_cnt = df . isnull () . sum () # calculate null counts null_cnt = null_cnt [ null_cnt != 0 ] # remove non-null cols null_percent = null_cnt / len ( df ) * 100 # calculate null percentages null_table = pd . concat ([ pd . DataFrame ( null_cnt ), pd . DataFrame ( null_percent )], axis = 1 ) null_table . columns = [ 'counts' , 'percentage' ] null_table . sort_values ( 'counts' , ascending = False , inplace = True ) return null_table # visualise null table import plotly_express as px null_table = null_analysis ( weather_train ) px . bar ( null_table . reset_index (), x = 'index' , y = 'percentage' , text = 'counts' , height = 500 )","title":"Missing Values"},{"location":"data-preprocessing/#threshold","text":"It makes no sense to fill in the null values if there are too many of them. We can set a threshold to delete the entire column if there are too many nulls. def null_threshold ( df , threshold = 25 ): ''' desc: delete columns based on a null percentage threshold arg: df=dataframe; threshold=percentage of nulls in column return: dataframe ''' null_table = null_analysis ( df ) null_table = null_table [ null_table [ 'percentage' ] >= 25 ] df . drop ( null_table . index , axis = 1 , inplace = True ) return df","title":"Threshold"},{"location":"data-preprocessing/#impute","text":"We can change missing values for the entire dataframe into their individual column means or medians. import pandas as pd import numpy as np from sklearn.impute import SimpleImputer imp_mean = SimpleImputer ( missing_values = np . nan , strategy = 'median' , copy = False ) imp_mean . fit ( df ) # output is in numpy, so convert to df df2 = pd . DataFrame ( imp_mean . transform ( df ), columns = df . columns )","title":"Impute"},{"location":"data-preprocessing/#interpolation","text":"We can also use interpolation via pandas default function to fill in the missing values. import pandas as pd # limit: Maximum number of consecutive NaNs to fill. # Must be greater than 0. df [ 'colname' ] . interpolate ( method = 'linear' , limit = 2 )","title":"Interpolation"},{"location":"data-preprocessing/#outliers","text":"Especially sensitive in linear models. They can be (1) removed manually by defining the lower and upper bound limit, or (2) grouping the features into ranks. Below is a simple method to detect & remove outliers that is defined by being outside a boxplot's whiskers. def boxplot_outlier_removal ( X , exclude = [ '' ]): ''' remove outliers detected by boxplot (Q1/Q3 -/+ IQR*1.5) Parameters ---------- X : dataframe dataset to remove outliers from exclude : list of str column names to exclude from outlier removal Returns ------- X : dataframe dataset with outliers removed ''' before = len ( X ) # iterate each column for col in X . columns : if col not in exclude : # get Q1, Q3 & Interquantile Range Q1 = X [ col ] . quantile ( 0.25 ) Q3 = X [ col ] . quantile ( 0.75 ) IQR = Q3 - Q1 # define outliers and remove them filter_ = ( X [ col ] > Q1 - 1.5 * IQR ) & ( X [ col ] < Q3 + 1.5 * IQR ) X = X [ filter_ ] after = len ( X ) diff = before - after percent = diff / before * 100 print ( ' {} ( {:.2f} %) outliers removed' . format ( diff , percent )) return X There are also unsupervised modelling techniques to find outliers.","title":"Outliers"},{"location":"data-preprocessing/#encodings","text":"","title":"Encodings"},{"location":"data-preprocessing/#label-encoding","text":"This converts a category of a label or feature into integer values. There are two methods to do this, via alphabetical order ( sklearn.preprocessing.LabelEncoder ), or order of appearance ( pd.factorize ). from sklearn import preprocessing from pandas import pd df = DataFrame ([ 'A' , 'B' , 'B' , 'C' ], columns = [ 'Col' ]) # factorise df [ 'Fact' ] = pd . factorize ( df [ 'Col' ])[ 0 ] # label encoder le = preprocessing . LabelEncoder () df [ 'Lab' ] = le . fit_transform ( df [ 'Col' ]) print ( df ) # Col Fact Lab # 0 A 0 0 # 1 B 1 1 # 2 B 1 1 # 3 C 2 2","title":"Label Encoding"},{"location":"data-preprocessing/#frequency-encoding","text":"This involves the conversion of categories into frequencies. # size of each category encoding = titanic . groupby ( 'Embarked' ) . size () # get frequency of each category encoding = encoding / len ( titanic ) titanic [ 'enc' ] = titanic . Embarked . map ( encoding ) # if categories have same frequency it can be an issue # will need to change it to ranked frequency encoding from scipy.stats import rankdata","title":"Frequency Encoding"},{"location":"data-preprocessing/#one-hot-encoding","text":"We could use an integer encoding directly, rescaled where needed. This may work for problems where there is a natural ordinal relationship between the categories, and in turn their integer values, such as labels for temperature \u2018cold\u2019, warm\u2019, and \u2018hot\u2019. However, there may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels \u2018dog\u2019 and \u2018cat\u2019. This is especially so for non-tree based models. Again this can be done two ways. get_dummies converts the result into a pandas dataframe, while sklearn's OneHotEncoder converts into a numpy array. import pandas as pd pd . get_dummies ( data = df , columns = [ 'A' , 'B' ]) from sklearn.preprocessing import OneHotEncoder onehotencoder = OneHotEncoder ( categorical_features = \"all\" ) X = onehotencoder . fit_transform ( X ) . toarray ()","title":"One-Hot Encoding"},{"location":"data-preprocessing/#coordinates","text":"It is necessary to define a projection for a coordinate reference system if there is a classification in space, e.g. k-means clustering. This basically change the coordinates from a spherical component to a flat surface. Also take note of spatial auto-correlation.","title":"Coordinates"},{"location":"data-scaling/","text":"Feature Scaling Scaling is an important concept needed to change all features to the same scale. This allows for faster convergence during model training, and more uniform influence for all weights. Types of Scaling Introduction to Machine Learning in Python Tree-based models is not dependent on scaling, but non-tree models models, very often are hugely dependent on it. Outliers can affect certain scalers, and it is important to either remove them or choose a scalar that is robust towards them. sklearn sklearn's examples description by benalexkeen Standard Scaler It standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as z = (x - u) / s . import pandas pd from sklearn.preprocessing import StandardScaler X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) # note that the test set using the fitted scaler in train dataset # to transform in the test set X_test_scaled = scaler . transform ( X_test ) Min Max Scale Another way to normalise is to use the Min Max Scaler, which changes all features to be between 0 and 1, as defined below: .. figure:: images/minmaxscaler.png import pandas pd from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () from sklearn.linear_model import Ridge X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) linridge = Ridge ( alpha = 20.0 ) . fit ( X_train_scaled , y_train ) RobustScaler Works similarly to standard scaler except that it uses median and quartiles, instead of mean and variance. Good as it ignores data points that are outliers. Normalizer Scales each data point such that the feature vector has a Euclidean length of 1. Often used when the direction of the data matters, not the length of the feature vector. Pipeline Scaling have a chance of leaking the part of the test data in train-test split into the training data. This is especially inevitable when using cross-validation. We can scale the train and test datasets separately to avoid this.However, a more convenient way is to use the pipeline function in sklearn, which wraps the scaler and classifier together, and scale them separately during cross validation. Any other functions can also be input here, e.g., rolling window feature extraction, which also have the potential to have data leakage. from sklearn.pipeline import Pipeline # \"scaler\" & \"svm\" can be any name. But they must be placed in the correct order of processing pipe = Pipeline ([( \"scaler\" , MinMaxScaler ()), ( \"svm\" , SVC ())]) pipe . fit ( X_train , y_train ) Pipeline ( steps = [( 'scaler' , MinMaxScaler ( copy = True , feature_range = ( 0 , 1 ))), ( 'svm' , SVC ( C = 1.0 , decision_function_shape = None , degree = 3 , gamma = 'auto' , kernel = 'rbf' , max_iter =- 1 , probability = False , random_state = None , shrinking = True , tol = 0.001 , verbose = False ))]) pipe . score ( X_test , y_test ) # 0.95104895104895104 Persistance To save the fitted scaler to normalize new datasets, we can save it using pickle or joblib for reusing in the future.","title":"Feature Scaling"},{"location":"data-scaling/#feature-scaling","text":"Scaling is an important concept needed to change all features to the same scale. This allows for faster convergence during model training, and more uniform influence for all weights.","title":"Feature Scaling"},{"location":"data-scaling/#types-of-scaling","text":"Introduction to Machine Learning in Python Tree-based models is not dependent on scaling, but non-tree models models, very often are hugely dependent on it. Outliers can affect certain scalers, and it is important to either remove them or choose a scalar that is robust towards them. sklearn sklearn's examples description by benalexkeen","title":"Types of Scaling"},{"location":"data-scaling/#standard-scaler","text":"It standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as z = (x - u) / s . import pandas pd from sklearn.preprocessing import StandardScaler X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) # note that the test set using the fitted scaler in train dataset # to transform in the test set X_test_scaled = scaler . transform ( X_test )","title":"Standard Scaler"},{"location":"data-scaling/#min-max-scale","text":"Another way to normalise is to use the Min Max Scaler, which changes all features to be between 0 and 1, as defined below: .. figure:: images/minmaxscaler.png import pandas pd from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () from sklearn.linear_model import Ridge X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) linridge = Ridge ( alpha = 20.0 ) . fit ( X_train_scaled , y_train )","title":"Min Max Scale"},{"location":"data-scaling/#robustscaler","text":"Works similarly to standard scaler except that it uses median and quartiles, instead of mean and variance. Good as it ignores data points that are outliers.","title":"RobustScaler"},{"location":"data-scaling/#normalizer","text":"Scales each data point such that the feature vector has a Euclidean length of 1. Often used when the direction of the data matters, not the length of the feature vector.","title":"Normalizer"},{"location":"data-scaling/#pipeline","text":"Scaling have a chance of leaking the part of the test data in train-test split into the training data. This is especially inevitable when using cross-validation. We can scale the train and test datasets separately to avoid this.However, a more convenient way is to use the pipeline function in sklearn, which wraps the scaler and classifier together, and scale them separately during cross validation. Any other functions can also be input here, e.g., rolling window feature extraction, which also have the potential to have data leakage. from sklearn.pipeline import Pipeline # \"scaler\" & \"svm\" can be any name. But they must be placed in the correct order of processing pipe = Pipeline ([( \"scaler\" , MinMaxScaler ()), ( \"svm\" , SVC ())]) pipe . fit ( X_train , y_train ) Pipeline ( steps = [( 'scaler' , MinMaxScaler ( copy = True , feature_range = ( 0 , 1 ))), ( 'svm' , SVC ( C = 1.0 , decision_function_shape = None , degree = 3 , gamma = 'auto' , kernel = 'rbf' , max_iter =- 1 , probability = False , random_state = None , shrinking = True , tol = 0.001 , verbose = False ))]) pipe . score ( X_test , y_test ) # 0.95104895104895104","title":"Pipeline"},{"location":"data-scaling/#persistance","text":"To save the fitted scaler to normalize new datasets, we can save it using pickle or joblib for reusing in the future.","title":"Persistance"},{"location":"datasets/","text":"Datasets Playground datasets by sklearn, statesmodel, vega allows one to practise our ML modelling without the hassle of acquiring good data, which usually is the most time-consuming process. We will also cover Kaggle, the most important data science competition platform, and an excellent resource to learn from the best. Statsmodels In statsmodels , many R datasets can be obtained from the function sm.datasets.get_rdataset() . To view each dataset's description, print(duncan_prestige.__doc__) . import statsmodels.api as sm prestige = sm . datasets . get_rdataset ( \"Duncan\" , \"car\" , cache = True ) . data print prestige . head () # type income education prestige # accountant prof 62 86 82 # pilot prof 72 76 83 # architect prof 75 92 90 # author prof 55 90 76 # chemist prof 64 86 90 Sklearn There are five common toy datasets here. For others, see . To view each dataset's description, use print boston['DESCR'] . Noun Desc load_boston Load and return the boston house-prices dataset (regression) load_iris Load and return the iris dataset (classification) load_diabetes Load and return the diabetes dataset load_digits Load and return the digits dataset (classification) load_linnerud Load and return the linnerud dataset (multivariate regression) from sklearn.datasets import load_iris iris = load_iris () # Load iris into a dataframe and set the field names df = pd . DataFrame ( iris [ 'data' ], columns = iris [ 'feature_names' ]) df . head () # sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) # 0 5.1 3.5 1.4 0.2 # 1 4.9 3.0 1.4 0.2 # 2 4.7 3.2 1.3 0.2 # 3 4.6 3.1 1.5 0.2 # 4 5.0 3.6 1.4 0.2 print ( iris . target_names [: 5 ]) # ['setosa' 'versicolor' 'virginica'] print ( iris . target ) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 # 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # 2 2] # Change target to target_names & merge with main dataframe df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) print df [ 'species' ] . head () sepal length ( cm ) sepal width ( cm ) petal length ( cm ) petal width ( cm ) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name : species , dtype : category Categories ( 3 , object ): [ setosa , versicolor , virginica ] Vega-Datasets vega_datasets have to be installed via pip install vega_datasets . from vega_datasets import data df = data . iris () df . head () petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa To list all datasets, use list_datasets() data . list_datasets () [ '7zip' , 'airports' , 'anscombe' , 'barley' , 'birdstrikes' , 'budget' , \\ 'budgets' , 'burtin' , 'cars' , 'climate' , 'co2-concentration' , 'countries' , \\ 'crimea' , 'disasters' , 'driving' , 'earthquakes' , 'ffox' , 'flare' , \\ 'flare-dependencies' , 'flights-10k' , 'flights-200k' , 'flights-20k' , \\ 'flights-2k' , 'flights-3m' , 'flights-5k' , 'flights-airport' , 'gapminder' , \\ 'gapminder-health-income' , 'gimp' , 'github' , 'graticule' , 'income' , 'iris' , \\ 'jobs' , 'londonBoroughs' , 'londonCentroids' , 'londonTubeLines' , 'lookup_groups' , \\ 'lookup_people' , 'miserables' , 'monarchs' , 'movies' , 'normal-2d' , 'obesity' , \\ 'points' , 'population' , 'population_engineers_hurricanes' , 'seattle-temps' , \\ 'seattle-weather' , 'sf-temps' , 'sp500' , 'stocks' , 'udistrict' , 'unemployment' , \\ 'unemployment-across-industries' , 'us-10m' , 'us-employment' , 'us-state-capitals' , \\ 'weather' , 'weball26' , 'wheat' , 'world-110m' , 'zipcodes' ] Kaggle Kaggle is the most recognised online data science competition, with attractive rewards and recognition for being the top competitor. With a point system that encourages sharing, one can learnt from the top practitioners in the world. Progession System The progression system in Kaggle are as follows. There are 4 types of expertise medals for specific work, namely: Competition Dataset Notebook Discussion For each expertise, it is possible to obtain bronze, silver and gold medals. Performance Tier is an overall recognition for each of the expertise stated above, base on the number of medals accumulated. The various rankings are: Novice Contributor Expert Master Grandmaster Online Notebook Kaggle's notebook has a dedicated GPU and decent RAM for deep-learning neural networks. For installation of new packages, check \"internet\" under \"Settings\" in the right panel first, then in the notebook cell, !pip install package . To read dataset, you can see the file path at the right panel for \"Data\". It goes something like /kaggle/input/competition_folder_name . To download/export the prediction for submission, we can save the prediction like df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False) . To do a direct submission, we can commit the notebook, with the output saving directly as submission.csv , e.g., df_submission.to_csv(r'submisson.csv', index=False) .","title":"Datasets"},{"location":"datasets/#datasets","text":"Playground datasets by sklearn, statesmodel, vega allows one to practise our ML modelling without the hassle of acquiring good data, which usually is the most time-consuming process. We will also cover Kaggle, the most important data science competition platform, and an excellent resource to learn from the best.","title":"Datasets"},{"location":"datasets/#statsmodels","text":"In statsmodels , many R datasets can be obtained from the function sm.datasets.get_rdataset() . To view each dataset's description, print(duncan_prestige.__doc__) . import statsmodels.api as sm prestige = sm . datasets . get_rdataset ( \"Duncan\" , \"car\" , cache = True ) . data print prestige . head () # type income education prestige # accountant prof 62 86 82 # pilot prof 72 76 83 # architect prof 75 92 90 # author prof 55 90 76 # chemist prof 64 86 90","title":"Statsmodels"},{"location":"datasets/#sklearn","text":"There are five common toy datasets here. For others, see . To view each dataset's description, use print boston['DESCR'] . Noun Desc load_boston Load and return the boston house-prices dataset (regression) load_iris Load and return the iris dataset (classification) load_diabetes Load and return the diabetes dataset load_digits Load and return the digits dataset (classification) load_linnerud Load and return the linnerud dataset (multivariate regression) from sklearn.datasets import load_iris iris = load_iris () # Load iris into a dataframe and set the field names df = pd . DataFrame ( iris [ 'data' ], columns = iris [ 'feature_names' ]) df . head () # sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) # 0 5.1 3.5 1.4 0.2 # 1 4.9 3.0 1.4 0.2 # 2 4.7 3.2 1.3 0.2 # 3 4.6 3.1 1.5 0.2 # 4 5.0 3.6 1.4 0.2 print ( iris . target_names [: 5 ]) # ['setosa' 'versicolor' 'virginica'] print ( iris . target ) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 # 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # 2 2] # Change target to target_names & merge with main dataframe df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) print df [ 'species' ] . head () sepal length ( cm ) sepal width ( cm ) petal length ( cm ) petal width ( cm ) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name : species , dtype : category Categories ( 3 , object ): [ setosa , versicolor , virginica ]","title":"Sklearn"},{"location":"datasets/#vega-datasets","text":"vega_datasets have to be installed via pip install vega_datasets . from vega_datasets import data df = data . iris () df . head () petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa To list all datasets, use list_datasets() data . list_datasets () [ '7zip' , 'airports' , 'anscombe' , 'barley' , 'birdstrikes' , 'budget' , \\ 'budgets' , 'burtin' , 'cars' , 'climate' , 'co2-concentration' , 'countries' , \\ 'crimea' , 'disasters' , 'driving' , 'earthquakes' , 'ffox' , 'flare' , \\ 'flare-dependencies' , 'flights-10k' , 'flights-200k' , 'flights-20k' , \\ 'flights-2k' , 'flights-3m' , 'flights-5k' , 'flights-airport' , 'gapminder' , \\ 'gapminder-health-income' , 'gimp' , 'github' , 'graticule' , 'income' , 'iris' , \\ 'jobs' , 'londonBoroughs' , 'londonCentroids' , 'londonTubeLines' , 'lookup_groups' , \\ 'lookup_people' , 'miserables' , 'monarchs' , 'movies' , 'normal-2d' , 'obesity' , \\ 'points' , 'population' , 'population_engineers_hurricanes' , 'seattle-temps' , \\ 'seattle-weather' , 'sf-temps' , 'sp500' , 'stocks' , 'udistrict' , 'unemployment' , \\ 'unemployment-across-industries' , 'us-10m' , 'us-employment' , 'us-state-capitals' , \\ 'weather' , 'weball26' , 'wheat' , 'world-110m' , 'zipcodes' ]","title":"Vega-Datasets"},{"location":"datasets/#kaggle","text":"Kaggle is the most recognised online data science competition, with attractive rewards and recognition for being the top competitor. With a point system that encourages sharing, one can learnt from the top practitioners in the world.","title":"Kaggle"},{"location":"datasets/#progession-system","text":"The progression system in Kaggle are as follows. There are 4 types of expertise medals for specific work, namely: Competition Dataset Notebook Discussion For each expertise, it is possible to obtain bronze, silver and gold medals. Performance Tier is an overall recognition for each of the expertise stated above, base on the number of medals accumulated. The various rankings are: Novice Contributor Expert Master Grandmaster","title":"Progession System"},{"location":"datasets/#online-notebook","text":"Kaggle's notebook has a dedicated GPU and decent RAM for deep-learning neural networks. For installation of new packages, check \"internet\" under \"Settings\" in the right panel first, then in the notebook cell, !pip install package . To read dataset, you can see the file path at the right panel for \"Data\". It goes something like /kaggle/input/competition_folder_name . To download/export the prediction for submission, we can save the prediction like df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False) . To do a direct submission, we can commit the notebook, with the output saving directly as submission.csv , e.g., df_submission.to_csv(r'submisson.csv', index=False) .","title":"Online Notebook"},{"location":"model-crossvalidation/","text":"K-fold Cross-Validation Traditional train-test split (or 1 fold split) runs the risk that the split might unwittingly be bias towards certain features or labels. By iterating the model training into k-times, with each iteration using a different training & validation split, we can avoid such biasness, though it is k-times computationally expensive. cross_val_score is a compact function to obtain the all scoring values using kfold in one line. from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier X = df [ df . columns [ 1 : - 1 ]] y = df [ 'Cover_Type' ] # using 5-fold cross validation mean scores model = RandomForestClassifier () cv_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = 5 , n_jobs =- 1 ) print ( np . mean ( cv_scores )) For greater control, like to define our own evaluation metrics etc., we can use KFold to obtain the train & test indexes for each fold iteration. Sklearn's grid/random searches also allow cross validation together with model tuning. from sklearn.model_selection import KFold from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score def kfold_custom ( fold = 4 , X , y , model , eval_metric ): kf = KFold ( n_splits = fold ) score_total = [] for train_index , test_index in kf . split ( X ): X_train , y_train = train [ train_index ][ X_features ], \\ train [ train_index ][ y_feature ] X_test , y_test = test [ test_index ][ X_features ], \\ test [ test_index ][ y_feature ] model . fit ( X_train , y_train ) y_predict = model . predict () score = eval_metric ( y_test , y_predict ) score_total . append ( score ) score = np . mean ( score_total ) return score model = RandomForestClassifier () kfold_custom ( X , y , model , f1score ) There are many other variants of cross validations available in sklearn, as shown below.","title":"Cross Validation"},{"location":"model-crossvalidation/#k-fold-cross-validation","text":"Traditional train-test split (or 1 fold split) runs the risk that the split might unwittingly be bias towards certain features or labels. By iterating the model training into k-times, with each iteration using a different training & validation split, we can avoid such biasness, though it is k-times computationally expensive. cross_val_score is a compact function to obtain the all scoring values using kfold in one line. from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier X = df [ df . columns [ 1 : - 1 ]] y = df [ 'Cover_Type' ] # using 5-fold cross validation mean scores model = RandomForestClassifier () cv_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = 5 , n_jobs =- 1 ) print ( np . mean ( cv_scores )) For greater control, like to define our own evaluation metrics etc., we can use KFold to obtain the train & test indexes for each fold iteration. Sklearn's grid/random searches also allow cross validation together with model tuning. from sklearn.model_selection import KFold from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score def kfold_custom ( fold = 4 , X , y , model , eval_metric ): kf = KFold ( n_splits = fold ) score_total = [] for train_index , test_index in kf . split ( X ): X_train , y_train = train [ train_index ][ X_features ], \\ train [ train_index ][ y_feature ] X_test , y_test = test [ test_index ][ X_features ], \\ test [ test_index ][ y_feature ] model . fit ( X_train , y_train ) y_predict = model . predict () score = eval_metric ( y_test , y_predict ) score_total . append ( score ) score = np . mean ( score_total ) return score model = RandomForestClassifier () kfold_custom ( X , y , model , f1score ) There are many other variants of cross validations available in sklearn, as shown below.","title":"K-fold Cross-Validation"},{"location":"model-evaluation/","text":"Model Evaluation A large portion of model evaluation is derived from sklearn. Do check out their documentation for their latest APIs. Classification The various commonly used evaluation metrics for classification problems are as listed. Type Formula Desc Example Accuracy TP + TN / (TP + TN + FP + FN) Get % TP & TN We never use accuracy on its own Precision TP / (TP + FP) High precision means it is important to filter off the any false positives. Spam removal Recall TP / (TP + FN) High recall means to get all positives (TP + FN) despite having some false positives. Tumour detection F1 2*((Precision * Recall) / (Precision + Recall)) Harmonic mean of precision & recall - from sklearn.metrics import ( accuracy_score , precision_score , recall_score , f1_score ) from statistics import mean accuracy = accuracy_score ( y_test , y_predicted ) precision = precision_score ( y_test , y_predicted ) recall = recall_score ( y_test , y_predicted ) f1 = f1_score ( y_test , y_predicted ) # for multiclass classification # we need to compute their mean precision = mean ( precision_score ( y_test , y_predicted , average = None )) recall = mean ( recall_score ( y_test , y_predicted , average = None )) f1 = mean ( f1_score ( y_test , y_predict , average = None )) Confusion Matrix The confusion matrix is the most important visualization to plot for a classification evaluation. It shows the absolute breakdown of each prediction to see if they are in the correct class; and is usually in a heatmap for better clarity. import matplotlib.pyplot as plt from sklearn.metrics import plot_confusion_matrix confusion = plot_confusion_matrix ( model , X_test , y_test , cmap = plt . cm . Blues ) plt . savefig ( \"logs/confusion_metrics.png\" ) Classification Report We can use the classification report to show details of precision, recall & f1-scores for each class. from sklearn.metrics import classification_report cls_report = classification_report ( y_test , y_predict ) print ( cls_report ) Precision-Recall Curve From sklearn , the precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. from sklearn.metrics import precision_recall_curve from sklearn.metrics import plot_precision_recall_curve import matplotlib.pyplot as plt average_precision = average_precision_score ( y_test , y_score ) disp = plot_precision_recall_curve ( classifier , X_test , y_test ) disp . ax_ . set_title ( '2-class Precision-Recall curve: ' 'AP= {0:0.2f} ' . format ( average_precision )) sklearn documentation ROC Curve The receiver operating characteristic (ROC) curve evaluates the performance of a classifier by plotting the True Positive Rate vs the False Positive Rate. The metric, area under curve (AUC) is used. The higher the AUC, the better the model is. The term came about in WWII where this metric is used to determined a receiver operator\u2019s ability to distinguish false positive and true postive correctly in the radar signals. from sklearn.metrics import plot_roc_curve svc_disp = plot_roc_curve ( svc , X_test , y_test ) plt . show () sklearn documentation Regression For regression problems, the response is always a continuous value, so it requires a different set of evaluation metrics. This website gives an excellent description on all the variants of errors metrics, which are briefly summarised below. Type Desc R-Squared Percentage of variability of dataset that can be explained by the model MSE (Mean Squared Error) Squaring then getting the mean of all errors (so change negatives into positives) RMSE (Squared Root of MSE) So that it gives back the error at the same scale (as it was initially squared) MAE (Mean Absolute Error) For negative errors, convert them to positive and obtain all error means RMSLE (Root Mean Square Log Error) Helps to reduce the effects of outliers compared to RMSE The RMSE result will always be larger or equal to the MAE. However, if all of the errors have the same magnitude, then RMSE=MAE. Since the errors were squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. import numpy as np from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error model = RandomForestRegressor ( n_estimators = 375 ) model3 = model . fit ( X_train , y_train ) fullmodel = model . fit ( predictor , target ) # R2 r2_full = fullmodel . score ( predictor , target ) r2_train = model3 . score ( X_train , y_train ) r2_test = model3 . score ( X_test , y_test ) # get predictions y_predicted_total = model3 . predict ( predictor ) y_predicted_train = model3 . predict ( X_train ) y_predicted_test = model3 . predict ( X_test ) # get MSE MSE_total = mean_squared_error ( target , y_predicted_total ) MSE_train = mean_squared_error ( y_train , y_predicted_train ) MSE_test = mean_squared_error ( y_test , y_predicted_test ) # get RMSE by squared root RMSE_total = np . sqrt ( MSE_total ) RMSE_train = np . sqrt ( MSE_train ) RMSE_test = np . sqrt ( MSE_test ) # get MAE MAE_total = mean_absolute_error ( target , y_predicted_total ) MAE_train = mean_absolute_error ( y_train , y_predicted_train ) MAE_test = mean_absolute_error ( y_test , y_predicted_test ) RMSLE is a very popular evaluation metric in data science competitions. More about it in this medium article . def rmsle ( y , y0 ): assert len ( y ) == len ( y0 ) return np . sqrt ( np . mean ( np . power ( np . log1p ( y ) - np . log1p ( y0 ), 2 )))","title":"Model Evaluation"},{"location":"model-evaluation/#model-evaluation","text":"A large portion of model evaluation is derived from sklearn. Do check out their documentation for their latest APIs.","title":"Model Evaluation"},{"location":"model-evaluation/#classification","text":"The various commonly used evaluation metrics for classification problems are as listed. Type Formula Desc Example Accuracy TP + TN / (TP + TN + FP + FN) Get % TP & TN We never use accuracy on its own Precision TP / (TP + FP) High precision means it is important to filter off the any false positives. Spam removal Recall TP / (TP + FN) High recall means to get all positives (TP + FN) despite having some false positives. Tumour detection F1 2*((Precision * Recall) / (Precision + Recall)) Harmonic mean of precision & recall - from sklearn.metrics import ( accuracy_score , precision_score , recall_score , f1_score ) from statistics import mean accuracy = accuracy_score ( y_test , y_predicted ) precision = precision_score ( y_test , y_predicted ) recall = recall_score ( y_test , y_predicted ) f1 = f1_score ( y_test , y_predicted ) # for multiclass classification # we need to compute their mean precision = mean ( precision_score ( y_test , y_predicted , average = None )) recall = mean ( recall_score ( y_test , y_predicted , average = None )) f1 = mean ( f1_score ( y_test , y_predict , average = None ))","title":"Classification"},{"location":"model-evaluation/#confusion-matrix","text":"The confusion matrix is the most important visualization to plot for a classification evaluation. It shows the absolute breakdown of each prediction to see if they are in the correct class; and is usually in a heatmap for better clarity. import matplotlib.pyplot as plt from sklearn.metrics import plot_confusion_matrix confusion = plot_confusion_matrix ( model , X_test , y_test , cmap = plt . cm . Blues ) plt . savefig ( \"logs/confusion_metrics.png\" )","title":"Confusion Matrix"},{"location":"model-evaluation/#classification-report","text":"We can use the classification report to show details of precision, recall & f1-scores for each class. from sklearn.metrics import classification_report cls_report = classification_report ( y_test , y_predict ) print ( cls_report )","title":"Classification Report"},{"location":"model-evaluation/#precision-recall-curve","text":"From sklearn , the precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. from sklearn.metrics import precision_recall_curve from sklearn.metrics import plot_precision_recall_curve import matplotlib.pyplot as plt average_precision = average_precision_score ( y_test , y_score ) disp = plot_precision_recall_curve ( classifier , X_test , y_test ) disp . ax_ . set_title ( '2-class Precision-Recall curve: ' 'AP= {0:0.2f} ' . format ( average_precision )) sklearn documentation","title":"Precision-Recall Curve"},{"location":"model-evaluation/#roc-curve","text":"The receiver operating characteristic (ROC) curve evaluates the performance of a classifier by plotting the True Positive Rate vs the False Positive Rate. The metric, area under curve (AUC) is used. The higher the AUC, the better the model is. The term came about in WWII where this metric is used to determined a receiver operator\u2019s ability to distinguish false positive and true postive correctly in the radar signals. from sklearn.metrics import plot_roc_curve svc_disp = plot_roc_curve ( svc , X_test , y_test ) plt . show () sklearn documentation","title":"ROC Curve"},{"location":"model-evaluation/#regression","text":"For regression problems, the response is always a continuous value, so it requires a different set of evaluation metrics. This website gives an excellent description on all the variants of errors metrics, which are briefly summarised below. Type Desc R-Squared Percentage of variability of dataset that can be explained by the model MSE (Mean Squared Error) Squaring then getting the mean of all errors (so change negatives into positives) RMSE (Squared Root of MSE) So that it gives back the error at the same scale (as it was initially squared) MAE (Mean Absolute Error) For negative errors, convert them to positive and obtain all error means RMSLE (Root Mean Square Log Error) Helps to reduce the effects of outliers compared to RMSE The RMSE result will always be larger or equal to the MAE. However, if all of the errors have the same magnitude, then RMSE=MAE. Since the errors were squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. import numpy as np from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error model = RandomForestRegressor ( n_estimators = 375 ) model3 = model . fit ( X_train , y_train ) fullmodel = model . fit ( predictor , target ) # R2 r2_full = fullmodel . score ( predictor , target ) r2_train = model3 . score ( X_train , y_train ) r2_test = model3 . score ( X_test , y_test ) # get predictions y_predicted_total = model3 . predict ( predictor ) y_predicted_train = model3 . predict ( X_train ) y_predicted_test = model3 . predict ( X_test ) # get MSE MSE_total = mean_squared_error ( target , y_predicted_total ) MSE_train = mean_squared_error ( y_train , y_predicted_train ) MSE_test = mean_squared_error ( y_test , y_predicted_test ) # get RMSE by squared root RMSE_total = np . sqrt ( MSE_total ) RMSE_train = np . sqrt ( MSE_train ) RMSE_test = np . sqrt ( MSE_test ) # get MAE MAE_total = mean_absolute_error ( target , y_predicted_total ) MAE_train = mean_absolute_error ( y_train , y_predicted_train ) MAE_test = mean_absolute_error ( y_test , y_predicted_test ) RMSLE is a very popular evaluation metric in data science competitions. More about it in this medium article . def rmsle ( y , y0 ): assert len ( y ) == len ( y0 ) return np . sqrt ( np . mean ( np . power ( np . log1p ( y ) - np . log1p ( y0 ), 2 )))","title":"Regression"},{"location":"model-explainability/","text":"Model Explainability While sklearn\u2019s supervised models are black boxes, we can derive certain plots and metrics to interprete the outcome and model better. Feature Importance Decision trees and other tree ensemble models, by default, allow us to obtain the importance of features. These are known as impurity-based feature importances. While powerful, we need to understand its limitations, as described by sklearn. they are biased towards high cardinality (numerical) features they are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity). import pandas as pd from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline rf = RandomForestClassifier () model = rf . fit ( X_train , y_train ) def feature_impt ( model , columns , figsize = ( 10 , 2 )): \"\"\"plot feature importance barchart for tree models\"\"\" # sort feature importance in df f_impt = pd . DataFrame ( model . feature_importances_ , index = columns ) f_impt = f_impt . sort_values ( by = 0 , ascending = False ) f_impt . columns = [ 'feature importance' ] # plot bar chart plt . figure ( figsize = figsize ) plt . bar ( f_impt . index , f_impt [ 'feature importance' ]) plt . xticks ( rotation = 'vertical' ) plt . title ( 'Feature Importance' ); return f_impt f_impt = feature_impt ( model ) Permutation Importance To overcome the limitations of feature importance, a variant known as permutation importance is available. It also has the benefits of being about to use for any model. This Kaggle article provides a good clear explanation How it works is the shuffling of individual features and see how it affects model accuarcy. If a feature is important, the model accuarcy will be reduced more. If not important, the accuarcy should be affected a lot less. From Kaggle from sklearn.inspection import permutation_importance result = permutation_importance ( rf , X_test , y_test , n_repeats = 10 , random_state = 42 , n_jobs = 2 ) sorted_idx = result . importances_mean . argsort () plt . figure ( figsize = ( 12 , 10 )) plt . boxplot ( result . importances [ sorted_idx ] . T , vert = False , labels = X . columns [ sorted_idx ]); Another library also provides the same API. import eli5 from eli5.sklearn import PermutationImportance perm = PermutationImportance ( my_model , random_state = 1 ) . fit ( test_X , test_y ) eli5 . show_weights ( perm , feature_names = test_X . columns . tolist ()) The output is as below. +/- refers to the randomness that shuffling resulted in. The higher the weight, the more important the feature is. Negative values are possible, but actually refer to 0; though random chance caused the predictions on shuffled data to be more accurate. From Kaggle Partial Dependence Plots While feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions. Using the fitted model to predict our outcome, and by repeatedly altering the value of just one variable, we can trace the predicted outcomes in a plot to show its dependence on the variable and when it plateaus. from matplotlib import pyplot as plt from pdpbox import pdp , get_dataset , info_plots # Create the data that we will plot pdp_goals = pdp . pdp_isolate ( model = tree_model , dataset = val_X , model_features = feature_names , feature = 'Goal Scored' ) # plot it pdp . pdp_plot ( pdp_goals , 'Goal Scored' ) plt . show () From Kaggle 2D Partial Dependence Plots are also useful for interactions between features. # just need to change pdp_isolate to pdp_interact features_to_plot = [ 'Goal Scored' , 'Distance Covered (Kms)' ] inter1 = pdp . pdp_interact ( model = tree_model , dataset = val_X , model_features = feature_names , features = features_to_plot ) pdp . pdp_interact_plot ( pdp_interact_out = inter1 , feature_names = features_to_plot , plot_type = 'contour' ) plt . show () From Kaggle SHAP SHapley Additive exPlanations ( SHAP ) break down a prediction to show the impact of each feature. Type Desc shap.TreeExplainer(my_model) for tree models shap.DeepExplainer(my_model) for neural networks shap.KernelExplainer(my_model) for all models, but slower, and gives approximate SHAP values import shap explainer = shap . TreeExplainer ( my_model ) shap_values = explainer . shap_values ( data_for_prediction ) # load JS lib in notebook shap . initjs () shap . force_plot ( explainer . expected_value [ 1 ], shap_values [ 1 ], data_for_prediction ) From Kaggle","title":"Model Explainability"},{"location":"model-explainability/#model-explainability","text":"While sklearn\u2019s supervised models are black boxes, we can derive certain plots and metrics to interprete the outcome and model better.","title":"Model Explainability"},{"location":"model-explainability/#feature-importance","text":"Decision trees and other tree ensemble models, by default, allow us to obtain the importance of features. These are known as impurity-based feature importances. While powerful, we need to understand its limitations, as described by sklearn. they are biased towards high cardinality (numerical) features they are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity). import pandas as pd from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline rf = RandomForestClassifier () model = rf . fit ( X_train , y_train ) def feature_impt ( model , columns , figsize = ( 10 , 2 )): \"\"\"plot feature importance barchart for tree models\"\"\" # sort feature importance in df f_impt = pd . DataFrame ( model . feature_importances_ , index = columns ) f_impt = f_impt . sort_values ( by = 0 , ascending = False ) f_impt . columns = [ 'feature importance' ] # plot bar chart plt . figure ( figsize = figsize ) plt . bar ( f_impt . index , f_impt [ 'feature importance' ]) plt . xticks ( rotation = 'vertical' ) plt . title ( 'Feature Importance' ); return f_impt f_impt = feature_impt ( model )","title":"Feature Importance"},{"location":"model-explainability/#permutation-importance","text":"To overcome the limitations of feature importance, a variant known as permutation importance is available. It also has the benefits of being about to use for any model. This Kaggle article provides a good clear explanation How it works is the shuffling of individual features and see how it affects model accuarcy. If a feature is important, the model accuarcy will be reduced more. If not important, the accuarcy should be affected a lot less. From Kaggle from sklearn.inspection import permutation_importance result = permutation_importance ( rf , X_test , y_test , n_repeats = 10 , random_state = 42 , n_jobs = 2 ) sorted_idx = result . importances_mean . argsort () plt . figure ( figsize = ( 12 , 10 )) plt . boxplot ( result . importances [ sorted_idx ] . T , vert = False , labels = X . columns [ sorted_idx ]); Another library also provides the same API. import eli5 from eli5.sklearn import PermutationImportance perm = PermutationImportance ( my_model , random_state = 1 ) . fit ( test_X , test_y ) eli5 . show_weights ( perm , feature_names = test_X . columns . tolist ()) The output is as below. +/- refers to the randomness that shuffling resulted in. The higher the weight, the more important the feature is. Negative values are possible, but actually refer to 0; though random chance caused the predictions on shuffled data to be more accurate. From Kaggle","title":"Permutation Importance"},{"location":"model-explainability/#partial-dependence-plots","text":"While feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions. Using the fitted model to predict our outcome, and by repeatedly altering the value of just one variable, we can trace the predicted outcomes in a plot to show its dependence on the variable and when it plateaus. from matplotlib import pyplot as plt from pdpbox import pdp , get_dataset , info_plots # Create the data that we will plot pdp_goals = pdp . pdp_isolate ( model = tree_model , dataset = val_X , model_features = feature_names , feature = 'Goal Scored' ) # plot it pdp . pdp_plot ( pdp_goals , 'Goal Scored' ) plt . show () From Kaggle 2D Partial Dependence Plots are also useful for interactions between features. # just need to change pdp_isolate to pdp_interact features_to_plot = [ 'Goal Scored' , 'Distance Covered (Kms)' ] inter1 = pdp . pdp_interact ( model = tree_model , dataset = val_X , model_features = feature_names , features = features_to_plot ) pdp . pdp_interact_plot ( pdp_interact_out = inter1 , feature_names = features_to_plot , plot_type = 'contour' ) plt . show () From Kaggle","title":"Partial Dependence Plots"},{"location":"model-explainability/#shap","text":"SHapley Additive exPlanations ( SHAP ) break down a prediction to show the impact of each feature. Type Desc shap.TreeExplainer(my_model) for tree models shap.DeepExplainer(my_model) for neural networks shap.KernelExplainer(my_model) for all models, but slower, and gives approximate SHAP values import shap explainer = shap . TreeExplainer ( my_model ) shap_values = explainer . shap_values ( data_for_prediction ) # load JS lib in notebook shap . initjs () shap . force_plot ( explainer . expected_value [ 1 ], shap_values [ 1 ], data_for_prediction ) From Kaggle","title":"SHAP"},{"location":"model-supervised1/","text":"","title":"Classification"},{"location":"model-supervised2/","text":"","title":"Regression"},{"location":"model-tuning/","text":"Model Split & Tuning Each model usually have 1 or more hyperparameters to adjust. A change in the hyperparameters during training will often result in a change in model performance. Therefore we need to \"tune\" the models by finding the best values of hyperparameters that would derive an overall best model performance. Grid Search In grid search, we indicate a list of values for each hyperparameter, and use grid search to permutate to give you the best parameters for a given evaluation metric. Below is an example for a classification problem. from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier () grid_values = { 'n_estimators' :[ 150 , 175 , 200 , 225 ]} grid = GridSearchCV ( model , param_grid = grid_values , scoring = \"f1\" , cv = 5 ) grid . fit ( predictor , target ) print ( grid . best_params_ ) print ( grid . best_score_ ) Auto Tuners Grid search or random search, while semi-automated, is still very tedious. The list of values you give for searching might not be out of range or far off from the optimal, hence it might result in several iterations of changing the values to find a good one. Bayesian optimization with gaussian processes is a popular technique to overcome this. It involves just defining the start and end value of each hyperparameter, and using bayesian probability, get the best parameters for a model. Bayesian Optimization This package is my favourite, due to its high level api and therefore, ease of use. We just need to define two things: a dictionary which parameters and a range of values to tune within a function with arguments that accepts the parameters, and output with the evaluation scoring. The model fitting and prediction, and scoring will thus reside here. Besides its ease of use: The api also allows an exploration option n_iter\u200b , which balances the exploitation space defined by the bayesian optimiser. It auto generates a table output for each iteration with the scoring and parameter values selected. from bayes_opt import BayesianOptimization from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Function output with scores def black_box ( n_estimators , max_depth ): params = { \"n_jobs\" : 5 , \"n_estimators\" : int ( round ( n_estimators )), \"max_depth\" : max_depth } model = RandomForestRegressor ( ** params ) model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) score = np . sqrt ( mean_squared_error ( y_test , y_predict )) return - score # Parameter search space pbounds = { 'n_estimators' : ( 1 , 5 ), 'max_depth' : ( 10 , 50 )} optimizer = BayesianOptimization ( black_box , pbounds , random_state = 2100 ) optimizer . maximize ( init_points = 10 , n_iter = 5 ) best_param = optimizer . max [ 'params' ] However, it has a few downsides: The most glaring is that it does not allow an option for parameters which should only be in integers (notice that n_estimators above has to be wrapped in an int function, else there will be an error). The best parameter output will still output as a float, and we have to change it back to an integer before retraining our final model. The optimiser assumes that the best score is the highest value. However, for regression models, it is usually measured by loss or error metrics, like RMSE; which means the lower value the better. We will need to reverse this manually by changing the score to a negative value (as with above code) Bayesian Tuning and Bandits Bayesian Tuning and Bandits is a more recent library developed by MIT. Its main advantage to me over the above package is: Allows specification for int or float options for parameter values. from btb.tuning import GP from btb import HyperParameter , ParamTypes from sklearn.metrics import mean_squared_error def auto_tuning ( tunables , epoch , X_train , X_test , y_train , y_test ): \"\"\"Auto-tuner using BTB library\"\"\" tuner = GP ( tunables ) parameters = tuner . propose () score_list = [] param_list = [] for i in range ( epoch ): model = RandomForestRegressor ( ** parameters , n_jobs = 10 , verbose = 3 ) model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) score = np . sqrt ( mean_squared_error ( y_test , y_predict )) # store scores & parameters score_list . append ( score ) param_list . append ( parameters ) print ( 'epoch: {} , rmse: {} , param: {} ' . format ( i + 1 , score , parameters )) score = - score # get new parameters tuner . add ( parameters , score ) parameters = tuner . propose () best_s = tuner . _best_score best_score_index = score_list . index ( best_s ) best_param = param_list [ best_score_index ] print ( ' \\n best rmse: {} ' . format ( best_s )) print ( 'best parameters: {} ' . format ( best_param )) return best_param tunables = [( 'n_estimators' , HyperParameter ( ParamTypes . INT , [ 500 , 2000 ])), ( 'max_depth' , HyperParameter ( ParamTypes . INT , [ 3 , 20 ]))] best_param = auto_tuning ( tunables , 5 , X_train , X_test , y_train , y_test ) The disadvantages are: Certain options requires me to code manually, which I thought the authors can easily package them into a function. examples include: Number of iterations; I have to write a for-loop for this Printing of results for each iteration Obtaining of parameters for the best_scores; I have to store all the scores and parameters in a list and then search back the index to get the best parameters. Scikit-Optimize scikit-optimize appears to be another popular package that implements bayesian optimization.","title":"Model Tuning"},{"location":"model-tuning/#model-split-tuning","text":"Each model usually have 1 or more hyperparameters to adjust. A change in the hyperparameters during training will often result in a change in model performance. Therefore we need to \"tune\" the models by finding the best values of hyperparameters that would derive an overall best model performance.","title":"Model Split &amp; Tuning"},{"location":"model-tuning/#grid-search","text":"In grid search, we indicate a list of values for each hyperparameter, and use grid search to permutate to give you the best parameters for a given evaluation metric. Below is an example for a classification problem. from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier () grid_values = { 'n_estimators' :[ 150 , 175 , 200 , 225 ]} grid = GridSearchCV ( model , param_grid = grid_values , scoring = \"f1\" , cv = 5 ) grid . fit ( predictor , target ) print ( grid . best_params_ ) print ( grid . best_score_ )","title":"Grid Search"},{"location":"model-tuning/#auto-tuners","text":"Grid search or random search, while semi-automated, is still very tedious. The list of values you give for searching might not be out of range or far off from the optimal, hence it might result in several iterations of changing the values to find a good one. Bayesian optimization with gaussian processes is a popular technique to overcome this. It involves just defining the start and end value of each hyperparameter, and using bayesian probability, get the best parameters for a model.","title":"Auto Tuners"},{"location":"model-tuning/#bayesian-optimization","text":"This package is my favourite, due to its high level api and therefore, ease of use. We just need to define two things: a dictionary which parameters and a range of values to tune within a function with arguments that accepts the parameters, and output with the evaluation scoring. The model fitting and prediction, and scoring will thus reside here. Besides its ease of use: The api also allows an exploration option n_iter\u200b , which balances the exploitation space defined by the bayesian optimiser. It auto generates a table output for each iteration with the scoring and parameter values selected. from bayes_opt import BayesianOptimization from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Function output with scores def black_box ( n_estimators , max_depth ): params = { \"n_jobs\" : 5 , \"n_estimators\" : int ( round ( n_estimators )), \"max_depth\" : max_depth } model = RandomForestRegressor ( ** params ) model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) score = np . sqrt ( mean_squared_error ( y_test , y_predict )) return - score # Parameter search space pbounds = { 'n_estimators' : ( 1 , 5 ), 'max_depth' : ( 10 , 50 )} optimizer = BayesianOptimization ( black_box , pbounds , random_state = 2100 ) optimizer . maximize ( init_points = 10 , n_iter = 5 ) best_param = optimizer . max [ 'params' ] However, it has a few downsides: The most glaring is that it does not allow an option for parameters which should only be in integers (notice that n_estimators above has to be wrapped in an int function, else there will be an error). The best parameter output will still output as a float, and we have to change it back to an integer before retraining our final model. The optimiser assumes that the best score is the highest value. However, for regression models, it is usually measured by loss or error metrics, like RMSE; which means the lower value the better. We will need to reverse this manually by changing the score to a negative value (as with above code)","title":"Bayesian Optimization"},{"location":"model-tuning/#bayesian-tuning-and-bandits","text":"Bayesian Tuning and Bandits is a more recent library developed by MIT. Its main advantage to me over the above package is: Allows specification for int or float options for parameter values. from btb.tuning import GP from btb import HyperParameter , ParamTypes from sklearn.metrics import mean_squared_error def auto_tuning ( tunables , epoch , X_train , X_test , y_train , y_test ): \"\"\"Auto-tuner using BTB library\"\"\" tuner = GP ( tunables ) parameters = tuner . propose () score_list = [] param_list = [] for i in range ( epoch ): model = RandomForestRegressor ( ** parameters , n_jobs = 10 , verbose = 3 ) model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) score = np . sqrt ( mean_squared_error ( y_test , y_predict )) # store scores & parameters score_list . append ( score ) param_list . append ( parameters ) print ( 'epoch: {} , rmse: {} , param: {} ' . format ( i + 1 , score , parameters )) score = - score # get new parameters tuner . add ( parameters , score ) parameters = tuner . propose () best_s = tuner . _best_score best_score_index = score_list . index ( best_s ) best_param = param_list [ best_score_index ] print ( ' \\n best rmse: {} ' . format ( best_s )) print ( 'best parameters: {} ' . format ( best_param )) return best_param tunables = [( 'n_estimators' , HyperParameter ( ParamTypes . INT , [ 500 , 2000 ])), ( 'max_depth' , HyperParameter ( ParamTypes . INT , [ 3 , 20 ]))] best_param = auto_tuning ( tunables , 5 , X_train , X_test , y_train , y_test ) The disadvantages are: Certain options requires me to code manually, which I thought the authors can easily package them into a function. examples include: Number of iterations; I have to write a for-loop for this Printing of results for each iteration Obtaining of parameters for the best_scores; I have to store all the scores and parameters in a list and then search back the index to get the best parameters.","title":"Bayesian Tuning and Bandits"},{"location":"model-tuning/#scikit-optimize","text":"scikit-optimize appears to be another popular package that implements bayesian optimization.","title":"Scikit-Optimize"},{"location":"model-unsupervised1/","text":"","title":"Transformation"},{"location":"model-unsupervised2/","text":"","title":"Clustering"},{"location":"model-unsupervised3/","text":"","title":"One-Class"},{"location":"model-unsupervised4/","text":"","title":"Distance"},{"location":"process/","text":"Machine Learning Process","title":"ML Process"},{"location":"process/#machine-learning-process","text":"","title":"Machine Learning Process"}]}