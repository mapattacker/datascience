{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This documentation summarises various machine learning techniques in Python. I have cut away the mathematics behind it so that one can, learn can understand the key concepts behind machine learning, without feeling overwhelmed. While they cover the entire process of machine learning, only classical models are introduced. Neural networks & other important concepts like model deployment are not covered here. I have two accompanying websites to fill in those gaps: Neural Networks (WIP) AI Engineering A lot of the content are compiled from various resources, so please cite them appropriately if you are using.","title":"Introduction"},{"location":"#introduction","text":"This documentation summarises various machine learning techniques in Python. I have cut away the mathematics behind it so that one can, learn can understand the key concepts behind machine learning, without feeling overwhelmed. While they cover the entire process of machine learning, only classical models are introduced. Neural networks & other important concepts like model deployment are not covered here. I have two accompanying websites to fill in those gaps: Neural Networks (WIP) AI Engineering A lot of the content are compiled from various resources, so please cite them appropriately if you are using.","title":"Introduction"},{"location":"agile/","text":"Agile This is somewhat of an off-topic, still related as development, even model development are transiting to an Agile project management, from a traditional waterfall one. The Atlassian page provides an excellent overview of what it entails and how Scrum, one of the most popular an agile methodology, is practised. Scrum Members","title":"Agile"},{"location":"agile/#agile","text":"This is somewhat of an off-topic, still related as development, even model development are transiting to an Agile project management, from a traditional waterfall one. The Atlassian page provides an excellent overview of what it entails and how Scrum, one of the most popular an agile methodology, is practised.","title":"Agile"},{"location":"agile/#scrum-members","text":"","title":"Scrum Members"},{"location":"data-classimbalance/","text":"Class Imbalance In domains like predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. Techniques are available to correct for this. The imbalance-learn package ( pip install -U imbalanced-learn ) provides an excellent range of algorithms for adjusting for imbalanced data. An important thing to note is that resampling must be done AFTER the train-test split , so as to prevent data leakage. Over-Sampling SMOTE (synthetic minority over-sampling technique) is a common and popular up-sampling technique. from imblearn.over_sampling import SMOTE smote = SMOTE () X_resampled , y_resampled = smote . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) ADASYN is one of the more advanced over sampling algorithms. from imblearn.over_sampling import ADASYN ada = ADASYN () X_resampled , y_resampled = ada . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) Under-Sampling from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler () X_resampled , y_resampled = rus . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) Cost-Sensitive Classification One can also make the classifier aware of the imbalanced data by incorporating the weights of the classes into a cost function. Intuitively, we want to give higher weight to minority class and lower weight to majority class. http://albahnsen.github.io/CostSensitiveClassification/index.html","title":"Class Imbalance"},{"location":"data-classimbalance/#class-imbalance","text":"In domains like predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. Techniques are available to correct for this. The imbalance-learn package ( pip install -U imbalanced-learn ) provides an excellent range of algorithms for adjusting for imbalanced data. An important thing to note is that resampling must be done AFTER the train-test split , so as to prevent data leakage.","title":"Class Imbalance"},{"location":"data-classimbalance/#over-sampling","text":"SMOTE (synthetic minority over-sampling technique) is a common and popular up-sampling technique. from imblearn.over_sampling import SMOTE smote = SMOTE () X_resampled , y_resampled = smote . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) ADASYN is one of the more advanced over sampling algorithms. from imblearn.over_sampling import ADASYN ada = ADASYN () X_resampled , y_resampled = ada . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled )","title":"Over-Sampling"},{"location":"data-classimbalance/#under-sampling","text":"from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler () X_resampled , y_resampled = rus . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled )","title":"Under-Sampling"},{"location":"data-classimbalance/#cost-sensitive-classification","text":"One can also make the classifier aware of the imbalanced data by incorporating the weights of the classes into a cost function. Intuitively, we want to give higher weight to minority class and lower weight to majority class. http://albahnsen.github.io/CostSensitiveClassification/index.html","title":"Cost-Sensitive Classification"},{"location":"data-engineering/","text":"Feature Engineering Feature Engineering is one of the most important part of model building. Collecting and creating of relevant features from existing ones are most often the determinant of a high prediction value. They can be classified broadly as follows. Type Desc Aggregations recalculation of a column in the feature by calculation before/after it Transformations change a feature to something meaningful, e.g., address to its spatial coordinates Decompositions break a feature into several ones, e.g. time series decomposition Interactions new feature created by interacting between two or more features Feature engineering usually require a good understand of the domain in order to generate useful features. Below are just some non-exhaustive examples to get you started. Decomposition Datetime Breakdown Very often, various dates and times of the day have strong interactions with your predictors. Here\u2019s a script to pull those values out. def extract_time ( df ): df [ 'timestamp' ] = pd . to_datetime ( df [ 'timestamp' ]) df [ 'hour' ] = df [ 'timestamp' ] . dt . hour df [ 'mth' ] = df [ 'timestamp' ] . dt . month df [ 'day' ] = df [ 'timestamp' ] . dt . day df [ 'dayofweek' ] = df [ 'timestamp' ] . dt . dayofweek return df To get holidays, use the package holidays. import holidays train [ 'holiday' ] = train [ 'timestamp' ] . apply ( lambda x : 0 if holidays . US () . get ( x ) is None else 1 ) Time Series Decomposition This is a popular decomposition method for time-series, whereby it is divided into trend (long-term), seaonality (short-term), residuals (noise). There are two methods to decompose: Type Desc Additive The component is present and is added to the other components to create the overall forecast value Multiplicative The component is present and is multiplied by the other components to create the overall forecast value Usually, an additive time-series will be used if there are no seasonal variations over time. import statsmodels.api as sm import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline res = sm . tsa . seasonal_decompose ( final2 [ 'avg_mth_elect' ], \\ model = 'multiplicative' ) # plot res . plot () # set decomposed parts into dataframe decomp = pd . concat ([ res . observed , res . trend , res . seasonal , res . resid ], axis = 1 ) decomp . columns = [ 'avg_mth' , 'trend' , 'seasonal' , 'residual' ] decomp . head () Automated Feature Engineering FeatureTools FeatureTools is extremely useful if you have datasets with a base data, with other tables that have relationships to it. We first create an EntitySet, which is like a database. Then we create entities, i.e., individual tables with a unique id for each table, and showing their relationships between each other. import featuretools as ft def make_entityset ( data ): es = ft . EntitySet ( 'Dataset' ) es . entity_from_dataframe ( dataframe = data , entity_id = 'recordings' , index = 'index' , time_index = 'time' ) es . normalize_entity ( base_entity_id = 'recordings' , new_entity_id = 'engines' , index = 'engine_no' ) es . normalize_entity ( base_entity_id = 'recordings' , new_entity_id = 'cycles' , index = 'time_in_cycles' ) return es es = make_entityset ( data ) es We then use something called Deep Feature Synthesis (dfs) to generate features automatically. Primitives are the type of new features to be extracted from the datasets. They can be aggregations (data is combined) or transformation (data is changed via a function) type of extractors. The list can be found via ft.primitives.list_primitives(). External primitives like tsfresh, or custom calculations can also be input into FeatureTools. feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'normal' , agg_primitives = [ 'last' , 'max' , 'min' ], trans_primitives = [], max_depth = 2 , verbose = 1 , n_jobs = 3 ) # see all old & new features created feature_matrix . columns FeatureTools appears to be a very powerful auto-feature extractor. Some resources to read further are as follows: brendanhasz towardsdatascience medium tsfresh tsfresh is a feature extraction package for time-series. It can extract more than 1200 different features, and filter out features that are deemed relevant. In essence, it is a univariate feature extractor. To extract all possible features... from tsfresh import extract_features def list_union_df ( fault_list ): \"\"\"Convert list of faults with a single signal value into a dataframe with an id for each fault sample Data transformation prior to feature extraction \"\"\" # convert nested list into dataframe dflist = [] # give an id field for each fault sample for a , i in enumerate ( verified_faults ): df = pd . DataFrame ( i ) df [ 'id' ] = a dflist . append ( df ) df = pd . concat ( dflist ) return df df = list_union_df ( fault_list ) # tsfresh extracted_features = extract_features ( df , column_id = 'id' ) # delete columns which only have one value for all rows for i in extracted_features . columns : col = extracted_features [ i ] if len ( col . unique ()) == 1 : del extracted_features [ i ] To generate only relevant features... from tsfresh import extract_relevant_features # y = is the target vector # length of y = no. of samples in timeseries, not length of the entire timeseries # column_sort = for each sample in timeseries, time_steps column will restart # fdr_level = false discovery rate, is default at 0.05, # it is the expected percentage of irrelevant features # tune down to reduce number of created features retained, tune up to increase features_filtered_direct = extract_relevant_features ( timeseries , y , column_id = 'id' , column_sort = 'time_steps' , fdr_level = 0.05 )","title":"Feature Engineering"},{"location":"data-engineering/#feature-engineering","text":"Feature Engineering is one of the most important part of model building. Collecting and creating of relevant features from existing ones are most often the determinant of a high prediction value. They can be classified broadly as follows. Type Desc Aggregations recalculation of a column in the feature by calculation before/after it Transformations change a feature to something meaningful, e.g., address to its spatial coordinates Decompositions break a feature into several ones, e.g. time series decomposition Interactions new feature created by interacting between two or more features Feature engineering usually require a good understand of the domain in order to generate useful features. Below are just some non-exhaustive examples to get you started.","title":"Feature Engineering"},{"location":"data-engineering/#decomposition","text":"","title":"Decomposition"},{"location":"data-engineering/#datetime-breakdown","text":"Very often, various dates and times of the day have strong interactions with your predictors. Here\u2019s a script to pull those values out. def extract_time ( df ): df [ 'timestamp' ] = pd . to_datetime ( df [ 'timestamp' ]) df [ 'hour' ] = df [ 'timestamp' ] . dt . hour df [ 'mth' ] = df [ 'timestamp' ] . dt . month df [ 'day' ] = df [ 'timestamp' ] . dt . day df [ 'dayofweek' ] = df [ 'timestamp' ] . dt . dayofweek return df To get holidays, use the package holidays. import holidays train [ 'holiday' ] = train [ 'timestamp' ] . apply ( lambda x : 0 if holidays . US () . get ( x ) is None else 1 )","title":"Datetime Breakdown"},{"location":"data-engineering/#time-series-decomposition","text":"This is a popular decomposition method for time-series, whereby it is divided into trend (long-term), seaonality (short-term), residuals (noise). There are two methods to decompose: Type Desc Additive The component is present and is added to the other components to create the overall forecast value Multiplicative The component is present and is multiplied by the other components to create the overall forecast value Usually, an additive time-series will be used if there are no seasonal variations over time. import statsmodels.api as sm import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline res = sm . tsa . seasonal_decompose ( final2 [ 'avg_mth_elect' ], \\ model = 'multiplicative' ) # plot res . plot () # set decomposed parts into dataframe decomp = pd . concat ([ res . observed , res . trend , res . seasonal , res . resid ], axis = 1 ) decomp . columns = [ 'avg_mth' , 'trend' , 'seasonal' , 'residual' ] decomp . head ()","title":"Time Series Decomposition"},{"location":"data-engineering/#automated-feature-engineering","text":"","title":"Automated Feature Engineering"},{"location":"data-engineering/#featuretools","text":"FeatureTools is extremely useful if you have datasets with a base data, with other tables that have relationships to it. We first create an EntitySet, which is like a database. Then we create entities, i.e., individual tables with a unique id for each table, and showing their relationships between each other. import featuretools as ft def make_entityset ( data ): es = ft . EntitySet ( 'Dataset' ) es . entity_from_dataframe ( dataframe = data , entity_id = 'recordings' , index = 'index' , time_index = 'time' ) es . normalize_entity ( base_entity_id = 'recordings' , new_entity_id = 'engines' , index = 'engine_no' ) es . normalize_entity ( base_entity_id = 'recordings' , new_entity_id = 'cycles' , index = 'time_in_cycles' ) return es es = make_entityset ( data ) es We then use something called Deep Feature Synthesis (dfs) to generate features automatically. Primitives are the type of new features to be extracted from the datasets. They can be aggregations (data is combined) or transformation (data is changed via a function) type of extractors. The list can be found via ft.primitives.list_primitives(). External primitives like tsfresh, or custom calculations can also be input into FeatureTools. feature_matrix , feature_names = ft . dfs ( entityset = es , target_entity = 'normal' , agg_primitives = [ 'last' , 'max' , 'min' ], trans_primitives = [], max_depth = 2 , verbose = 1 , n_jobs = 3 ) # see all old & new features created feature_matrix . columns FeatureTools appears to be a very powerful auto-feature extractor. Some resources to read further are as follows: brendanhasz towardsdatascience medium","title":"FeatureTools"},{"location":"data-engineering/#tsfresh","text":"tsfresh is a feature extraction package for time-series. It can extract more than 1200 different features, and filter out features that are deemed relevant. In essence, it is a univariate feature extractor. To extract all possible features... from tsfresh import extract_features def list_union_df ( fault_list ): \"\"\"Convert list of faults with a single signal value into a dataframe with an id for each fault sample Data transformation prior to feature extraction \"\"\" # convert nested list into dataframe dflist = [] # give an id field for each fault sample for a , i in enumerate ( verified_faults ): df = pd . DataFrame ( i ) df [ 'id' ] = a dflist . append ( df ) df = pd . concat ( dflist ) return df df = list_union_df ( fault_list ) # tsfresh extracted_features = extract_features ( df , column_id = 'id' ) # delete columns which only have one value for all rows for i in extracted_features . columns : col = extracted_features [ i ] if len ( col . unique ()) == 1 : del extracted_features [ i ] To generate only relevant features... from tsfresh import extract_relevant_features # y = is the target vector # length of y = no. of samples in timeseries, not length of the entire timeseries # column_sort = for each sample in timeseries, time_steps column will restart # fdr_level = false discovery rate, is default at 0.05, # it is the expected percentage of irrelevant features # tune down to reduce number of created features retained, tune up to increase features_filtered_direct = extract_relevant_features ( timeseries , y , column_id = 'id' , column_sort = 'time_steps' , fdr_level = 0.05 )","title":"tsfresh"},{"location":"data-exploration/","text":"Exploratory Data Analysis Exploratory data analysis (EDA) is an essential step to understand the data better; in order to engineer and select features before modelling. This often requires skills in visualisation to better interpret the data. We can analyse the data from a univariate (single feature) or multivariate (multiple features) perspective. Univariate Distribution Plots When plotting distributions, it is important to compare the distribution of both train and test sets. If the test set very specific to certain features, the model will underfit and have a low accuarcy. import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline for i in X . columns : plt . figure ( figsize = ( 15 , 5 )) sns . distplot ( X [ i ]) sns . distplot ( pred [ i ]) Count Plots For categorical features, you may want to see if they have enough sample size for each category. import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline df [ 'Wildnerness' ] . value_counts () # Comanche Peak 6349 # Cache la Poudre 4675 # Rawah 3597 # Neota 499 # Name: Wildnerness, dtype: int64 cmap = sns . color_palette ( \"Set2\" ) sns . countplot ( x = 'Wildnerness' , data = df , palette = cmap ); plt . xticks ( rotation = 45 ); To check for possible relationships with the target, place the feature under hue. plt . figure ( figsize = ( 12 , 6 )) sns . countplot ( x = 'Cover_Type' , data = wild , hue = 'Wilderness' ); plt . xticks ( rotation = 45 ); fig , axes = plt . subplots ( ncols = 3 , nrows = 1 , figsize = ( 15 , 5 )) # note only for 1 row or 1 col, else need to flatten nested list in axes col = [ 'Winner' , 'Second' , 'Third' ] for cnt , ax in enumerate ( axes ): sns . countplot ( x = col [ cnt ], data = df2 , ax = ax , order = df2 [ col [ cnt ]] . value_counts () . index ); for ax in fig . axes : plt . sca ( ax ) plt . xticks ( rotation = 90 ) Box Plots Using the 50 percentile to compare among different classes, it is easy to find feature that can have high prediction importance if they do not overlap. Also can be use for outlier detection. Features have to be continuous. From different dataframes, displaying the same feature. df = pd . DataFrame ({ 'normal' : normal [ 'Pressure' ], 's1' : cf6 [ 'Pressure' ], 's2' : cf12 [ 'Pressure' ], 's3' : cf20 [ 'Pressure' ], 's4' : cf30 [ 'Pressure' ], 's5' : cf45 [ 'Pressure' ]}) df . boxplot ( figsize = ( 10 , 5 )); Start screen From same dataframe with of a feature split by different y-labels plt . figure ( figsize = ( 7 , 5 )) cmap = sns . color_palette ( \"Set3\" ) sns . boxplot ( x = 'Cover_Type' , y = 'Elevation' , data = df , palette = cmap ); plt . xticks ( rotation = 45 ); Multiple Plots cmap = sns . color_palette ( \"Set2\" ) fig , axes = plt . subplots ( ncols = 2 , nrows = 5 , figsize = ( 10 , 18 )) # axes is nested if >1 row & >1 col, need to flatten a = [ i for i in axes for i in i ] for i , ax in enumerate ( a ): sns . boxplot ( x = 'Cover_Type' , y = eda2 . columns [ i ], data = eda , palette = cmap , width = 0.5 , ax = ax ); # rotate x-axis for every single plot for ax in fig . axes : plt . sca ( ax ) plt . xticks ( rotation = 45 ) # set spacing for every subplot, else x-axis will be covered plt . tight_layout () Multi-Variate Correlation Plots Heatmaps show a quick overall correlation between features. from plotly.offline import iplot from plotly.offline import init_notebook_mode import plotly.graph_objs as go init_notebook_mode ( connected = True ) # create correlation in dataframe corr = df [ df . columns [ 1 :]] . corr () layout = go . Layout ( width = 1000 , height = 600 , \\ title = 'Correlation Plot' , \\ font = dict ( size = 10 )) data = go . Heatmap ( z = corr . values , x = corr . columns , y = corr . columns ) fig = go . Figure ( data = [ data ], layout = layout ) iplot ( fig ) Using Plotly import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline # create correlation in dataframe corr = df [ df . columns [ 1 :]] . corr () plt . figure ( figsize = ( 15 , 8 )) sns . heatmap ( corr , cmap = sns . color_palette ( \"RdBu_r\" , 20 )); Using Seaborn","title":"Data Exploration"},{"location":"data-exploration/#exploratory-data-analysis","text":"Exploratory data analysis (EDA) is an essential step to understand the data better; in order to engineer and select features before modelling. This often requires skills in visualisation to better interpret the data. We can analyse the data from a univariate (single feature) or multivariate (multiple features) perspective.","title":"Exploratory Data Analysis"},{"location":"data-exploration/#univariate","text":"","title":"Univariate"},{"location":"data-exploration/#distribution-plots","text":"When plotting distributions, it is important to compare the distribution of both train and test sets. If the test set very specific to certain features, the model will underfit and have a low accuarcy. import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline for i in X . columns : plt . figure ( figsize = ( 15 , 5 )) sns . distplot ( X [ i ]) sns . distplot ( pred [ i ])","title":"Distribution Plots"},{"location":"data-exploration/#count-plots","text":"For categorical features, you may want to see if they have enough sample size for each category. import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline df [ 'Wildnerness' ] . value_counts () # Comanche Peak 6349 # Cache la Poudre 4675 # Rawah 3597 # Neota 499 # Name: Wildnerness, dtype: int64 cmap = sns . color_palette ( \"Set2\" ) sns . countplot ( x = 'Wildnerness' , data = df , palette = cmap ); plt . xticks ( rotation = 45 ); To check for possible relationships with the target, place the feature under hue. plt . figure ( figsize = ( 12 , 6 )) sns . countplot ( x = 'Cover_Type' , data = wild , hue = 'Wilderness' ); plt . xticks ( rotation = 45 ); fig , axes = plt . subplots ( ncols = 3 , nrows = 1 , figsize = ( 15 , 5 )) # note only for 1 row or 1 col, else need to flatten nested list in axes col = [ 'Winner' , 'Second' , 'Third' ] for cnt , ax in enumerate ( axes ): sns . countplot ( x = col [ cnt ], data = df2 , ax = ax , order = df2 [ col [ cnt ]] . value_counts () . index ); for ax in fig . axes : plt . sca ( ax ) plt . xticks ( rotation = 90 )","title":"Count Plots"},{"location":"data-exploration/#box-plots","text":"Using the 50 percentile to compare among different classes, it is easy to find feature that can have high prediction importance if they do not overlap. Also can be use for outlier detection. Features have to be continuous. From different dataframes, displaying the same feature. df = pd . DataFrame ({ 'normal' : normal [ 'Pressure' ], 's1' : cf6 [ 'Pressure' ], 's2' : cf12 [ 'Pressure' ], 's3' : cf20 [ 'Pressure' ], 's4' : cf30 [ 'Pressure' ], 's5' : cf45 [ 'Pressure' ]}) df . boxplot ( figsize = ( 10 , 5 )); Start screen From same dataframe with of a feature split by different y-labels plt . figure ( figsize = ( 7 , 5 )) cmap = sns . color_palette ( \"Set3\" ) sns . boxplot ( x = 'Cover_Type' , y = 'Elevation' , data = df , palette = cmap ); plt . xticks ( rotation = 45 ); Multiple Plots cmap = sns . color_palette ( \"Set2\" ) fig , axes = plt . subplots ( ncols = 2 , nrows = 5 , figsize = ( 10 , 18 )) # axes is nested if >1 row & >1 col, need to flatten a = [ i for i in axes for i in i ] for i , ax in enumerate ( a ): sns . boxplot ( x = 'Cover_Type' , y = eda2 . columns [ i ], data = eda , palette = cmap , width = 0.5 , ax = ax ); # rotate x-axis for every single plot for ax in fig . axes : plt . sca ( ax ) plt . xticks ( rotation = 45 ) # set spacing for every subplot, else x-axis will be covered plt . tight_layout ()","title":"Box Plots"},{"location":"data-exploration/#multi-variate","text":"","title":"Multi-Variate"},{"location":"data-exploration/#correlation-plots","text":"Heatmaps show a quick overall correlation between features. from plotly.offline import iplot from plotly.offline import init_notebook_mode import plotly.graph_objs as go init_notebook_mode ( connected = True ) # create correlation in dataframe corr = df [ df . columns [ 1 :]] . corr () layout = go . Layout ( width = 1000 , height = 600 , \\ title = 'Correlation Plot' , \\ font = dict ( size = 10 )) data = go . Heatmap ( z = corr . values , x = corr . columns , y = corr . columns ) fig = go . Figure ( data = [ data ], layout = layout ) iplot ( fig ) Using Plotly import seaborn as sns import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline # create correlation in dataframe corr = df [ df . columns [ 1 :]] . corr () plt . figure ( figsize = ( 15 , 8 )) sns . heatmap ( corr , cmap = sns . color_palette ( \"RdBu_r\" , 20 )); Using Seaborn","title":"Correlation Plots"},{"location":"data-preprocessing/","text":"Feature Preprocessing Missing Values Machine learning models cannot accept null/NaN values. We will need to either remove them or fill them with a logical value. To investigate how many nulls in each column. def null_analysis ( df ): ''' desc: get nulls for each column in counts & percentages arg: dataframe return: dataframe ''' null_cnt = df . isnull () . sum () # calculate null counts null_cnt = null_cnt [ null_cnt != 0 ] # remove non-null cols null_percent = null_cnt / len ( df ) * 100 # calculate null percentages null_table = pd . concat ([ pd . DataFrame ( null_cnt ), pd . DataFrame ( null_percent )], axis = 1 ) null_table . columns = [ 'counts' , 'percentage' ] null_table . sort_values ( 'counts' , ascending = False , inplace = True ) return null_table # visualise null table import plotly_express as px null_table = null_analysis ( weather_train ) px . bar ( null_table . reset_index (), x = 'index' , y = 'percentage' , text = 'counts' , height = 500 ) Threshold It makes no sense to fill in the null values if there are too many of them. We can set a threshold to delete the entire column if there are too many nulls. def null_threshold ( df , threshold = 25 ): ''' desc: delete columns based on a null percentage threshold arg: df=dataframe; threshold=percentage of nulls in column return: dataframe ''' null_table = null_analysis ( df ) null_table = null_table [ null_table [ 'percentage' ] >= 25 ] df . drop ( null_table . index , axis = 1 , inplace = True ) return df Impute We can change missing values for the entire dataframe into their individual column means or medians. import pandas as pd import numpy as np from sklearn.impute import SimpleImputer imp_mean = SimpleImputer ( missing_values = np . nan , strategy = 'median' , copy = False ) imp_mean . fit ( df ) # output is in numpy, so convert to df df2 = pd . DataFrame ( imp_mean . transform ( df ), columns = df . columns ) Interpolation We can also use interpolation via pandas default function to fill in the missing values. import pandas as pd # limit: Maximum number of consecutive NaNs to fill. # Must be greater than 0. df [ 'colname' ] . interpolate ( method = 'linear' , limit = 2 ) Outliers Especially sensitive in linear models. They can be (1) removed manually by defining the lower and upper bound limit, or (2) grouping the features into ranks. Below is a simple method to detect & remove outliers that is defined by being outside a boxplot's whiskers. def boxplot_outlier_removal ( X , exclude = [ '' ]): ''' remove outliers detected by boxplot (Q1/Q3 -/+ IQR*1.5) Parameters ---------- X : dataframe dataset to remove outliers from exclude : list of str column names to exclude from outlier removal Returns ------- X : dataframe dataset with outliers removed ''' before = len ( X ) # iterate each column for col in X . columns : if col not in exclude : # get Q1, Q3 & Interquantile Range Q1 = X [ col ] . quantile ( 0.25 ) Q3 = X [ col ] . quantile ( 0.75 ) IQR = Q3 - Q1 # define outliers and remove them filter_ = ( X [ col ] > Q1 - 1.5 * IQR ) & ( X [ col ] < Q3 + 1.5 * IQR ) X = X [ filter_ ] after = len ( X ) diff = before - after percent = diff / before * 100 print ( ' {} ( {:.2f} %) outliers removed' . format ( diff , percent )) return X There are also unsupervised modelling techniques to find outliers. Encodings Label Encoding This converts a category of a label or feature into integer values. There are two methods to do this, via alphabetical order ( sklearn.preprocessing.LabelEncoder ), or order of appearance ( pd.factorize ). from sklearn import preprocessing from pandas import pd df = DataFrame ([ 'A' , 'B' , 'B' , 'C' ], columns = [ 'Col' ]) # factorise df [ 'Fact' ] = pd . factorize ( df [ 'Col' ])[ 0 ] # label encoder le = preprocessing . LabelEncoder () df [ 'Lab' ] = le . fit_transform ( df [ 'Col' ]) print ( df ) # Col Fact Lab # 0 A 0 0 # 1 B 1 1 # 2 B 1 1 # 3 C 2 2 Frequency Encoding This involves the conversion of categories into frequencies. # size of each category encoding = titanic . groupby ( 'Embarked' ) . size () # get frequency of each category encoding = encoding / len ( titanic ) titanic [ 'enc' ] = titanic . Embarked . map ( encoding ) # if categories have same frequency it can be an issue # will need to change it to ranked frequency encoding from scipy.stats import rankdata One-Hot Encoding We could use an integer encoding directly, rescaled where needed. This may work for problems where there is a natural ordinal relationship between the categories, and in turn their integer values, such as labels for temperature \u2018cold\u2019, warm\u2019, and \u2018hot\u2019. However, there may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels \u2018dog\u2019 and \u2018cat\u2019. This is especially so for non-tree based models. Again this can be done two ways. get_dummies converts the result into a pandas dataframe, while sklearn's OneHotEncoder converts into a numpy array. import pandas as pd pd . get_dummies ( data = df , columns = [ 'A' , 'B' ]) from sklearn.preprocessing import OneHotEncoder onehotencoder = OneHotEncoder ( categorical_features = \"all\" ) X = onehotencoder . fit_transform ( X ) . toarray () Coordinates It is necessary to define a projection for a coordinate reference system if there is a classification in space, e.g. k-means clustering. This basically change the coordinates from a spherical component to a flat surface. Also take note of spatial auto-correlation.","title":"Feature Preprocessing"},{"location":"data-preprocessing/#feature-preprocessing","text":"","title":"Feature Preprocessing"},{"location":"data-preprocessing/#missing-values","text":"Machine learning models cannot accept null/NaN values. We will need to either remove them or fill them with a logical value. To investigate how many nulls in each column. def null_analysis ( df ): ''' desc: get nulls for each column in counts & percentages arg: dataframe return: dataframe ''' null_cnt = df . isnull () . sum () # calculate null counts null_cnt = null_cnt [ null_cnt != 0 ] # remove non-null cols null_percent = null_cnt / len ( df ) * 100 # calculate null percentages null_table = pd . concat ([ pd . DataFrame ( null_cnt ), pd . DataFrame ( null_percent )], axis = 1 ) null_table . columns = [ 'counts' , 'percentage' ] null_table . sort_values ( 'counts' , ascending = False , inplace = True ) return null_table # visualise null table import plotly_express as px null_table = null_analysis ( weather_train ) px . bar ( null_table . reset_index (), x = 'index' , y = 'percentage' , text = 'counts' , height = 500 )","title":"Missing Values"},{"location":"data-preprocessing/#threshold","text":"It makes no sense to fill in the null values if there are too many of them. We can set a threshold to delete the entire column if there are too many nulls. def null_threshold ( df , threshold = 25 ): ''' desc: delete columns based on a null percentage threshold arg: df=dataframe; threshold=percentage of nulls in column return: dataframe ''' null_table = null_analysis ( df ) null_table = null_table [ null_table [ 'percentage' ] >= 25 ] df . drop ( null_table . index , axis = 1 , inplace = True ) return df","title":"Threshold"},{"location":"data-preprocessing/#impute","text":"We can change missing values for the entire dataframe into their individual column means or medians. import pandas as pd import numpy as np from sklearn.impute import SimpleImputer imp_mean = SimpleImputer ( missing_values = np . nan , strategy = 'median' , copy = False ) imp_mean . fit ( df ) # output is in numpy, so convert to df df2 = pd . DataFrame ( imp_mean . transform ( df ), columns = df . columns )","title":"Impute"},{"location":"data-preprocessing/#interpolation","text":"We can also use interpolation via pandas default function to fill in the missing values. import pandas as pd # limit: Maximum number of consecutive NaNs to fill. # Must be greater than 0. df [ 'colname' ] . interpolate ( method = 'linear' , limit = 2 )","title":"Interpolation"},{"location":"data-preprocessing/#outliers","text":"Especially sensitive in linear models. They can be (1) removed manually by defining the lower and upper bound limit, or (2) grouping the features into ranks. Below is a simple method to detect & remove outliers that is defined by being outside a boxplot's whiskers. def boxplot_outlier_removal ( X , exclude = [ '' ]): ''' remove outliers detected by boxplot (Q1/Q3 -/+ IQR*1.5) Parameters ---------- X : dataframe dataset to remove outliers from exclude : list of str column names to exclude from outlier removal Returns ------- X : dataframe dataset with outliers removed ''' before = len ( X ) # iterate each column for col in X . columns : if col not in exclude : # get Q1, Q3 & Interquantile Range Q1 = X [ col ] . quantile ( 0.25 ) Q3 = X [ col ] . quantile ( 0.75 ) IQR = Q3 - Q1 # define outliers and remove them filter_ = ( X [ col ] > Q1 - 1.5 * IQR ) & ( X [ col ] < Q3 + 1.5 * IQR ) X = X [ filter_ ] after = len ( X ) diff = before - after percent = diff / before * 100 print ( ' {} ( {:.2f} %) outliers removed' . format ( diff , percent )) return X There are also unsupervised modelling techniques to find outliers.","title":"Outliers"},{"location":"data-preprocessing/#encodings","text":"","title":"Encodings"},{"location":"data-preprocessing/#label-encoding","text":"This converts a category of a label or feature into integer values. There are two methods to do this, via alphabetical order ( sklearn.preprocessing.LabelEncoder ), or order of appearance ( pd.factorize ). from sklearn import preprocessing from pandas import pd df = DataFrame ([ 'A' , 'B' , 'B' , 'C' ], columns = [ 'Col' ]) # factorise df [ 'Fact' ] = pd . factorize ( df [ 'Col' ])[ 0 ] # label encoder le = preprocessing . LabelEncoder () df [ 'Lab' ] = le . fit_transform ( df [ 'Col' ]) print ( df ) # Col Fact Lab # 0 A 0 0 # 1 B 1 1 # 2 B 1 1 # 3 C 2 2","title":"Label Encoding"},{"location":"data-preprocessing/#frequency-encoding","text":"This involves the conversion of categories into frequencies. # size of each category encoding = titanic . groupby ( 'Embarked' ) . size () # get frequency of each category encoding = encoding / len ( titanic ) titanic [ 'enc' ] = titanic . Embarked . map ( encoding ) # if categories have same frequency it can be an issue # will need to change it to ranked frequency encoding from scipy.stats import rankdata","title":"Frequency Encoding"},{"location":"data-preprocessing/#one-hot-encoding","text":"We could use an integer encoding directly, rescaled where needed. This may work for problems where there is a natural ordinal relationship between the categories, and in turn their integer values, such as labels for temperature \u2018cold\u2019, warm\u2019, and \u2018hot\u2019. However, there may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels \u2018dog\u2019 and \u2018cat\u2019. This is especially so for non-tree based models. Again this can be done two ways. get_dummies converts the result into a pandas dataframe, while sklearn's OneHotEncoder converts into a numpy array. import pandas as pd pd . get_dummies ( data = df , columns = [ 'A' , 'B' ]) from sklearn.preprocessing import OneHotEncoder onehotencoder = OneHotEncoder ( categorical_features = \"all\" ) X = onehotencoder . fit_transform ( X ) . toarray ()","title":"One-Hot Encoding"},{"location":"data-preprocessing/#coordinates","text":"It is necessary to define a projection for a coordinate reference system if there is a classification in space, e.g. k-means clustering. This basically change the coordinates from a spherical component to a flat surface. Also take note of spatial auto-correlation.","title":"Coordinates"},{"location":"data-scaling/","text":"Feature Scaling Scaling is an important concept needed to change all features to the same scale. This allows for faster convergence during model training, and more uniform influence for all weights. Types of Scaling Introduction to Machine Learning in Python Tree-based models is not dependent on scaling, but non-tree models models, very often are hugely dependent on it. Outliers can affect certain scalers, and it is important to either remove them or choose a scalar that is robust towards them. sklearn sklearn's examples description by benalexkeen Standard Scaler It standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as z = (x - u) / s . import pandas pd from sklearn.preprocessing import StandardScaler X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) # note that the test set using the fitted scaler in train dataset # to transform in the test set X_test_scaled = scaler . transform ( X_test ) Min Max Scale Another way to normalise is to use the Min Max Scaler, which changes all features to be between 0 and 1. import pandas pd from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () from sklearn.linear_model import Ridge X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) linridge = Ridge ( alpha = 20.0 ) . fit ( X_train_scaled , y_train ) RobustScaler Works similarly to standard scaler except that it uses median and quartiles, instead of mean and variance. Good as it ignores data points that are outliers. Normalizer Scales each data point such that the feature vector has a Euclidean length of 1. Often used when the direction of the data matters, not the length of the feature vector. Pipeline Scaling have a chance of leaking the part of the test data in train-test split into the training data. This is especially inevitable when using cross-validation. We can scale the train and test datasets separately to avoid this.However, a more convenient way is to use the pipeline function in sklearn, which wraps the scaler and classifier together, and scale them separately during cross validation. Any other functions can also be input here, e.g., rolling window feature extraction, which also have the potential to have data leakage. from sklearn.pipeline import Pipeline # \"scaler\" & \"svm\" can be any name. But they must be placed in the correct order of processing pipe = Pipeline ([( \"scaler\" , MinMaxScaler ()), ( \"svm\" , SVC ())]) pipe . fit ( X_train , y_train ) Pipeline ( steps = [( 'scaler' , MinMaxScaler ( copy = True , feature_range = ( 0 , 1 ))), ( 'svm' , SVC ( C = 1.0 , decision_function_shape = None , degree = 3 , gamma = 'auto' , kernel = 'rbf' , max_iter =- 1 , probability = False , random_state = None , shrinking = True , tol = 0.001 , verbose = False ))]) pipe . score ( X_test , y_test ) # 0.95104895104895104 Persistance To save the fitted scaler to normalize new datasets, we can save it using pickle or joblib for reusing in the future.","title":"Feature Scaling"},{"location":"data-scaling/#feature-scaling","text":"Scaling is an important concept needed to change all features to the same scale. This allows for faster convergence during model training, and more uniform influence for all weights.","title":"Feature Scaling"},{"location":"data-scaling/#types-of-scaling","text":"Introduction to Machine Learning in Python Tree-based models is not dependent on scaling, but non-tree models models, very often are hugely dependent on it. Outliers can affect certain scalers, and it is important to either remove them or choose a scalar that is robust towards them. sklearn sklearn's examples description by benalexkeen","title":"Types of Scaling"},{"location":"data-scaling/#standard-scaler","text":"It standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as z = (x - u) / s . import pandas pd from sklearn.preprocessing import StandardScaler X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) # note that the test set using the fitted scaler in train dataset # to transform in the test set X_test_scaled = scaler . transform ( X_test )","title":"Standard Scaler"},{"location":"data-scaling/#min-max-scale","text":"Another way to normalise is to use the Min Max Scaler, which changes all features to be between 0 and 1. import pandas pd from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () from sklearn.linear_model import Ridge X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) linridge = Ridge ( alpha = 20.0 ) . fit ( X_train_scaled , y_train )","title":"Min Max Scale"},{"location":"data-scaling/#robustscaler","text":"Works similarly to standard scaler except that it uses median and quartiles, instead of mean and variance. Good as it ignores data points that are outliers.","title":"RobustScaler"},{"location":"data-scaling/#normalizer","text":"Scales each data point such that the feature vector has a Euclidean length of 1. Often used when the direction of the data matters, not the length of the feature vector.","title":"Normalizer"},{"location":"data-scaling/#pipeline","text":"Scaling have a chance of leaking the part of the test data in train-test split into the training data. This is especially inevitable when using cross-validation. We can scale the train and test datasets separately to avoid this.However, a more convenient way is to use the pipeline function in sklearn, which wraps the scaler and classifier together, and scale them separately during cross validation. Any other functions can also be input here, e.g., rolling window feature extraction, which also have the potential to have data leakage. from sklearn.pipeline import Pipeline # \"scaler\" & \"svm\" can be any name. But they must be placed in the correct order of processing pipe = Pipeline ([( \"scaler\" , MinMaxScaler ()), ( \"svm\" , SVC ())]) pipe . fit ( X_train , y_train ) Pipeline ( steps = [( 'scaler' , MinMaxScaler ( copy = True , feature_range = ( 0 , 1 ))), ( 'svm' , SVC ( C = 1.0 , decision_function_shape = None , degree = 3 , gamma = 'auto' , kernel = 'rbf' , max_iter =- 1 , probability = False , random_state = None , shrinking = True , tol = 0.001 , verbose = False ))]) pipe . score ( X_test , y_test ) # 0.95104895104895104","title":"Pipeline"},{"location":"data-scaling/#persistance","text":"To save the fitted scaler to normalize new datasets, we can save it using pickle or joblib for reusing in the future.","title":"Persistance"},{"location":"datasets/","text":"Datasets Playground datasets by sklearn, statesmodel, vega allows one to practise our ML modelling without the hassle of acquiring good data, which usually is the most time-consuming process. We will also cover Kaggle, the most important data science competition platform, and an excellent resource to learn from the best. Statsmodels In statsmodels , many R datasets can be obtained from the function sm.datasets.get_rdataset() . To view each dataset's description, print(duncan_prestige.__doc__) . import statsmodels.api as sm prestige = sm . datasets . get_rdataset ( \"Duncan\" , \"car\" , cache = True ) . data print prestige . head () # type income education prestige # accountant prof 62 86 82 # pilot prof 72 76 83 # architect prof 75 92 90 # author prof 55 90 76 # chemist prof 64 86 90 Sklearn There are five common toy datasets here. For others, see . To view each dataset's description, use print boston['DESCR'] . Noun Desc load_boston Load and return the boston house-prices dataset (regression) load_iris Load and return the iris dataset (classification) load_diabetes Load and return the diabetes dataset load_digits Load and return the digits dataset (classification) load_linnerud Load and return the linnerud dataset (multivariate regression) from sklearn.datasets import load_iris iris = load_iris () # Load iris into a dataframe and set the field names df = pd . DataFrame ( iris [ 'data' ], columns = iris [ 'feature_names' ]) df . head () # sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) # 0 5.1 3.5 1.4 0.2 # 1 4.9 3.0 1.4 0.2 # 2 4.7 3.2 1.3 0.2 # 3 4.6 3.1 1.5 0.2 # 4 5.0 3.6 1.4 0.2 print ( iris . target_names [: 5 ]) # ['setosa' 'versicolor' 'virginica'] print ( iris . target ) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 # 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # 2 2] # Change target to target_names & merge with main dataframe df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) print df [ 'species' ] . head () sepal length ( cm ) sepal width ( cm ) petal length ( cm ) petal width ( cm ) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name : species , dtype : category Categories ( 3 , object ): [ setosa , versicolor , virginica ] Vega-Datasets vega_datasets have to be installed via pip install vega_datasets . from vega_datasets import data df = data . iris () df . head () petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa To list all datasets, use list_datasets() data . list_datasets () [ '7zip' , 'airports' , 'anscombe' , 'barley' , 'birdstrikes' , 'budget' , \\ 'budgets' , 'burtin' , 'cars' , 'climate' , 'co2-concentration' , 'countries' , \\ 'crimea' , 'disasters' , 'driving' , 'earthquakes' , 'ffox' , 'flare' , \\ 'flare-dependencies' , 'flights-10k' , 'flights-200k' , 'flights-20k' , \\ 'flights-2k' , 'flights-3m' , 'flights-5k' , 'flights-airport' , 'gapminder' , \\ 'gapminder-health-income' , 'gimp' , 'github' , 'graticule' , 'income' , 'iris' , \\ 'jobs' , 'londonBoroughs' , 'londonCentroids' , 'londonTubeLines' , 'lookup_groups' , \\ 'lookup_people' , 'miserables' , 'monarchs' , 'movies' , 'normal-2d' , 'obesity' , \\ 'points' , 'population' , 'population_engineers_hurricanes' , 'seattle-temps' , \\ 'seattle-weather' , 'sf-temps' , 'sp500' , 'stocks' , 'udistrict' , 'unemployment' , \\ 'unemployment-across-industries' , 'us-10m' , 'us-employment' , 'us-state-capitals' , \\ 'weather' , 'weball26' , 'wheat' , 'world-110m' , 'zipcodes' ] Kaggle Kaggle is the most recognised online data science competition, with attractive rewards and recognition for being the top competitor. With a point system that encourages sharing, one can learnt from the top practitioners in the world. Progession System The progression system in Kaggle are as follows. There are 4 types of expertise medals for specific work, namely: Competition Dataset Notebook Discussion For each expertise, it is possible to obtain bronze, silver and gold medals. Performance Tier is an overall recognition for each of the expertise stated above, base on the number of medals accumulated. The various rankings are: Novice Contributor Expert Master Grandmaster Online Notebook Kaggle's notebook has a dedicated GPU and decent RAM for deep-learning neural networks. For installation of new packages, check \"internet\" under \"Settings\" in the right panel first, then in the notebook cell, !pip install package . To read dataset, you can see the file path at the right panel for \"Data\". It goes something like /kaggle/input/competition_folder_name . To download/export the prediction for submission, we can save the prediction like df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False) . To do a direct submission, we can commit the notebook, with the output saving directly as submission.csv , e.g., df_submission.to_csv(r'submisson.csv', index=False) .","title":"Datasets"},{"location":"datasets/#datasets","text":"Playground datasets by sklearn, statesmodel, vega allows one to practise our ML modelling without the hassle of acquiring good data, which usually is the most time-consuming process. We will also cover Kaggle, the most important data science competition platform, and an excellent resource to learn from the best.","title":"Datasets"},{"location":"datasets/#statsmodels","text":"In statsmodels , many R datasets can be obtained from the function sm.datasets.get_rdataset() . To view each dataset's description, print(duncan_prestige.__doc__) . import statsmodels.api as sm prestige = sm . datasets . get_rdataset ( \"Duncan\" , \"car\" , cache = True ) . data print prestige . head () # type income education prestige # accountant prof 62 86 82 # pilot prof 72 76 83 # architect prof 75 92 90 # author prof 55 90 76 # chemist prof 64 86 90","title":"Statsmodels"},{"location":"datasets/#sklearn","text":"There are five common toy datasets here. For others, see . To view each dataset's description, use print boston['DESCR'] . Noun Desc load_boston Load and return the boston house-prices dataset (regression) load_iris Load and return the iris dataset (classification) load_diabetes Load and return the diabetes dataset load_digits Load and return the digits dataset (classification) load_linnerud Load and return the linnerud dataset (multivariate regression) from sklearn.datasets import load_iris iris = load_iris () # Load iris into a dataframe and set the field names df = pd . DataFrame ( iris [ 'data' ], columns = iris [ 'feature_names' ]) df . head () # sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) # 0 5.1 3.5 1.4 0.2 # 1 4.9 3.0 1.4 0.2 # 2 4.7 3.2 1.3 0.2 # 3 4.6 3.1 1.5 0.2 # 4 5.0 3.6 1.4 0.2 print ( iris . target_names [: 5 ]) # ['setosa' 'versicolor' 'virginica'] print ( iris . target ) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 # 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # 2 2] # Change target to target_names & merge with main dataframe df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) print df [ 'species' ] . head () sepal length ( cm ) sepal width ( cm ) petal length ( cm ) petal width ( cm ) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name : species , dtype : category Categories ( 3 , object ): [ setosa , versicolor , virginica ]","title":"Sklearn"},{"location":"datasets/#vega-datasets","text":"vega_datasets have to be installed via pip install vega_datasets . from vega_datasets import data df = data . iris () df . head () petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa To list all datasets, use list_datasets() data . list_datasets () [ '7zip' , 'airports' , 'anscombe' , 'barley' , 'birdstrikes' , 'budget' , \\ 'budgets' , 'burtin' , 'cars' , 'climate' , 'co2-concentration' , 'countries' , \\ 'crimea' , 'disasters' , 'driving' , 'earthquakes' , 'ffox' , 'flare' , \\ 'flare-dependencies' , 'flights-10k' , 'flights-200k' , 'flights-20k' , \\ 'flights-2k' , 'flights-3m' , 'flights-5k' , 'flights-airport' , 'gapminder' , \\ 'gapminder-health-income' , 'gimp' , 'github' , 'graticule' , 'income' , 'iris' , \\ 'jobs' , 'londonBoroughs' , 'londonCentroids' , 'londonTubeLines' , 'lookup_groups' , \\ 'lookup_people' , 'miserables' , 'monarchs' , 'movies' , 'normal-2d' , 'obesity' , \\ 'points' , 'population' , 'population_engineers_hurricanes' , 'seattle-temps' , \\ 'seattle-weather' , 'sf-temps' , 'sp500' , 'stocks' , 'udistrict' , 'unemployment' , \\ 'unemployment-across-industries' , 'us-10m' , 'us-employment' , 'us-state-capitals' , \\ 'weather' , 'weball26' , 'wheat' , 'world-110m' , 'zipcodes' ]","title":"Vega-Datasets"},{"location":"datasets/#kaggle","text":"Kaggle is the most recognised online data science competition, with attractive rewards and recognition for being the top competitor. With a point system that encourages sharing, one can learnt from the top practitioners in the world.","title":"Kaggle"},{"location":"datasets/#progession-system","text":"The progression system in Kaggle are as follows. There are 4 types of expertise medals for specific work, namely: Competition Dataset Notebook Discussion For each expertise, it is possible to obtain bronze, silver and gold medals. Performance Tier is an overall recognition for each of the expertise stated above, base on the number of medals accumulated. The various rankings are: Novice Contributor Expert Master Grandmaster","title":"Progession System"},{"location":"datasets/#online-notebook","text":"Kaggle's notebook has a dedicated GPU and decent RAM for deep-learning neural networks. For installation of new packages, check \"internet\" under \"Settings\" in the right panel first, then in the notebook cell, !pip install package . To read dataset, you can see the file path at the right panel for \"Data\". It goes something like /kaggle/input/competition_folder_name . To download/export the prediction for submission, we can save the prediction like df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False) . To do a direct submission, we can commit the notebook, with the output saving directly as submission.csv , e.g., df_submission.to_csv(r'submisson.csv', index=False) .","title":"Online Notebook"},{"location":"model-concepts/","text":"Model Concepts Bias-Variance Tradeoff The best predictive model is one that has good generalization ability. With that, it will be able to give accurate predictions to new and previously unseen data. With that comes two terms bias & variance . High Bias results from Underfitting the model. This usually results from erroneous assumptions, and cause the model to be too general. High Variance results from Overfitting the model, and it will predict the training dataset very accurately, but not with unseen new datasets. This is because it will fit even the slightless noise in the dataset. The best model with the highest accuarcy is the middle ground between the two. from Andrew Ng\u2019s lecture Regularization Regularization is an important concept for some supervised models. It prevents overfitting by restricting the model, thus lowering its complexity. In regression, it is regulated by the hyperparameter alpha , where a high alpha means more regularization and a simpler model. Regularization Model Desc L1 LASSO reduces sum of the absolute values of coefficients. change unimportant features' regression coefficients into 0 L2 Ridge reduces the sum of squares In classification, it is regulated by the hyperparameter C , where a lower C means more regularization and a simpler model. Model Regularization Desc SVC L2 - LinearSVC L1/L2 Can choose which regularization type Logistic Regression L1/L2/ElasticNet Can choose which regularization type","title":"Model Concepts"},{"location":"model-concepts/#model-concepts","text":"","title":"Model Concepts"},{"location":"model-concepts/#bias-variance-tradeoff","text":"The best predictive model is one that has good generalization ability. With that, it will be able to give accurate predictions to new and previously unseen data. With that comes two terms bias & variance . High Bias results from Underfitting the model. This usually results from erroneous assumptions, and cause the model to be too general. High Variance results from Overfitting the model, and it will predict the training dataset very accurately, but not with unseen new datasets. This is because it will fit even the slightless noise in the dataset. The best model with the highest accuarcy is the middle ground between the two. from Andrew Ng\u2019s lecture","title":"Bias-Variance Tradeoff"},{"location":"model-concepts/#regularization","text":"Regularization is an important concept for some supervised models. It prevents overfitting by restricting the model, thus lowering its complexity. In regression, it is regulated by the hyperparameter alpha , where a high alpha means more regularization and a simpler model. Regularization Model Desc L1 LASSO reduces sum of the absolute values of coefficients. change unimportant features' regression coefficients into 0 L2 Ridge reduces the sum of squares In classification, it is regulated by the hyperparameter C , where a lower C means more regularization and a simpler model. Model Regularization Desc SVC L2 - LinearSVC L1/L2 Can choose which regularization type Logistic Regression L1/L2/ElasticNet Can choose which regularization type","title":"Regularization"},{"location":"model-crossvalidation/","text":"K-fold Cross-Validation Traditional train-test split (or 1 fold split) runs the risk that the split might unwittingly be bias towards certain features or labels. By iterating the model training into k-times, with each iteration using a different training & validation split, we can avoid such biasness, though it is k-times computationally expensive. cross_val_score is a compact function to obtain the all scoring values using kfold in one line. from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier X = df [ df . columns [ 1 : - 1 ]] y = df [ 'Cover_Type' ] # using 5-fold cross validation mean scores model = RandomForestClassifier () cv_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = 5 , n_jobs =- 1 ) print ( np . mean ( cv_scores )) For greater control, like to define our own evaluation metrics etc., we can use KFold to obtain the train & test indexes for each fold iteration. Sklearn's grid/random searches also allow cross validation together with model tuning. from sklearn.model_selection import KFold from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score def kfold_custom ( fold = 4 , X , y , model , eval_metric ): kf = KFold ( n_splits = fold ) score_total = [] for train_index , test_index in kf . split ( X ): X_train , y_train = train [ train_index ][ X_features ], \\ train [ train_index ][ y_feature ] X_test , y_test = test [ test_index ][ X_features ], \\ test [ test_index ][ y_feature ] model . fit ( X_train , y_train ) y_predict = model . predict () score = eval_metric ( y_test , y_predict ) score_total . append ( score ) score = np . mean ( score_total ) return score model = RandomForestClassifier () kfold_custom ( X , y , model , f1score ) There are many other variants of cross validations available in sklearn, as shown below.","title":"Cross Validation"},{"location":"model-crossvalidation/#k-fold-cross-validation","text":"Traditional train-test split (or 1 fold split) runs the risk that the split might unwittingly be bias towards certain features or labels. By iterating the model training into k-times, with each iteration using a different training & validation split, we can avoid such biasness, though it is k-times computationally expensive. cross_val_score is a compact function to obtain the all scoring values using kfold in one line. from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier X = df [ df . columns [ 1 : - 1 ]] y = df [ 'Cover_Type' ] # using 5-fold cross validation mean scores model = RandomForestClassifier () cv_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = 5 , n_jobs =- 1 ) print ( np . mean ( cv_scores )) For greater control, like to define our own evaluation metrics etc., we can use KFold to obtain the train & test indexes for each fold iteration. Sklearn's grid/random searches also allow cross validation together with model tuning. from sklearn.model_selection import KFold from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score def kfold_custom ( fold = 4 , X , y , model , eval_metric ): kf = KFold ( n_splits = fold ) score_total = [] for train_index , test_index in kf . split ( X ): X_train , y_train = train [ train_index ][ X_features ], \\ train [ train_index ][ y_feature ] X_test , y_test = test [ test_index ][ X_features ], \\ test [ test_index ][ y_feature ] model . fit ( X_train , y_train ) y_predict = model . predict () score = eval_metric ( y_test , y_predict ) score_total . append ( score ) score = np . mean ( score_total ) return score model = RandomForestClassifier () kfold_custom ( X , y , model , f1score ) There are many other variants of cross validations available in sklearn, as shown below.","title":"K-fold Cross-Validation"},{"location":"model-evaluation/","text":"Model Evaluation A large portion of model evaluation is derived from sklearn. Do check out their documentation for their latest APIs. Classification The various commonly used evaluation metrics for classification problems are as listed. Type Formula Desc Example Accuracy TP + TN / (TP + TN + FP + FN) Get % TP & TN We never use accuracy on its own Precision TP / (TP + FP) High precision means it is important to filter off the any false positives. Spam removal Recall TP / (TP + FN) High recall means to get all positives (TP + FN) despite having some false positives. Tumour detection F1 2*((Precision * Recall) / (Precision + Recall)) Harmonic mean of precision & recall - from sklearn.metrics import ( accuracy_score , precision_score , recall_score , f1_score ) from statistics import mean accuracy = accuracy_score ( y_test , y_predicted ) precision = precision_score ( y_test , y_predicted ) recall = recall_score ( y_test , y_predicted ) f1 = f1_score ( y_test , y_predicted ) # for multiclass classification # we need to compute their mean precision = mean ( precision_score ( y_test , y_predicted , average = None )) recall = mean ( recall_score ( y_test , y_predicted , average = None )) f1 = mean ( f1_score ( y_test , y_predict , average = None )) Confusion Matrix The confusion matrix is the most important visualization to plot for a classification evaluation. It shows the absolute breakdown of each prediction to see if they are in the correct class; and is usually in a heatmap for better clarity. import matplotlib.pyplot as plt from sklearn.metrics import plot_confusion_matrix confusion = plot_confusion_matrix ( model , X_test , y_test , cmap = plt . cm . Blues ) plt . savefig ( \"logs/confusion_metrics.png\" ) Classification Report We can use the classification report to show details of precision, recall & f1-scores for each class. from sklearn.metrics import classification_report cls_report = classification_report ( y_test , y_predict ) print ( cls_report ) Precision-Recall Curve From sklearn , the precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. from sklearn.metrics import precision_recall_curve from sklearn.metrics import plot_precision_recall_curve import matplotlib.pyplot as plt average_precision = average_precision_score ( y_test , y_score ) disp = plot_precision_recall_curve ( classifier , X_test , y_test ) disp . ax_ . set_title ( '2-class Precision-Recall curve: ' 'AP= {0:0.2f} ' . format ( average_precision )) sklearn documentation ROC Curve The receiver operating characteristic (ROC) curve evaluates the performance of a classifier by plotting the True Positive Rate vs the False Positive Rate. The metric, area under curve (AUC) is used. The higher the AUC, the better the model is. The term came about in WWII where this metric is used to determined a receiver operator\u2019s ability to distinguish false positive and true postive correctly in the radar signals. from sklearn.metrics import plot_roc_curve svc_disp = plot_roc_curve ( svc , X_test , y_test ) plt . show () sklearn documentation Regression For regression problems, the response is always a continuous value, so it requires a different set of evaluation metrics. This website gives an excellent description on all the variants of errors metrics, which are briefly summarised below. Type Desc R-Squared Percentage of variability of dataset that can be explained by the model MSE (Mean Squared Error) Squaring then getting the mean of all errors (so change negatives into positives) RMSE (Squared Root of MSE) So that it gives back the error at the same scale (as it was initially squared) MAE (Mean Absolute Error) For negative errors, convert them to positive and obtain all error means RMSLE (Root Mean Square Log Error) Helps to reduce the effects of outliers compared to RMSE The RMSE result will always be larger or equal to the MAE. However, if all of the errors have the same magnitude, then RMSE=MAE. Since the errors were squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. import numpy as np from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error model = RandomForestRegressor ( n_estimators = 375 ) model3 = model . fit ( X_train , y_train ) fullmodel = model . fit ( predictor , target ) # R2 r2_full = fullmodel . score ( predictor , target ) r2_train = model3 . score ( X_train , y_train ) r2_test = model3 . score ( X_test , y_test ) # get predictions y_predicted_total = model3 . predict ( predictor ) y_predicted_train = model3 . predict ( X_train ) y_predicted_test = model3 . predict ( X_test ) # get MSE MSE_total = mean_squared_error ( target , y_predicted_total ) MSE_train = mean_squared_error ( y_train , y_predicted_train ) MSE_test = mean_squared_error ( y_test , y_predicted_test ) # get RMSE by squared root RMSE_total = np . sqrt ( MSE_total ) RMSE_train = np . sqrt ( MSE_train ) RMSE_test = np . sqrt ( MSE_test ) # get MAE MAE_total = mean_absolute_error ( target , y_predicted_total ) MAE_train = mean_absolute_error ( y_train , y_predicted_train ) MAE_test = mean_absolute_error ( y_test , y_predicted_test ) RMSLE is a very popular evaluation metric in data science competitions. More about it in this medium article . def rmsle ( y , y0 ): assert len ( y ) == len ( y0 ) return np . sqrt ( np . mean ( np . power ( np . log1p ( y ) - np . log1p ( y0 ), 2 )))","title":"Model Evaluation"},{"location":"model-evaluation/#model-evaluation","text":"A large portion of model evaluation is derived from sklearn. Do check out their documentation for their latest APIs.","title":"Model Evaluation"},{"location":"model-evaluation/#classification","text":"The various commonly used evaluation metrics for classification problems are as listed. Type Formula Desc Example Accuracy TP + TN / (TP + TN + FP + FN) Get % TP & TN We never use accuracy on its own Precision TP / (TP + FP) High precision means it is important to filter off the any false positives. Spam removal Recall TP / (TP + FN) High recall means to get all positives (TP + FN) despite having some false positives. Tumour detection F1 2*((Precision * Recall) / (Precision + Recall)) Harmonic mean of precision & recall - from sklearn.metrics import ( accuracy_score , precision_score , recall_score , f1_score ) from statistics import mean accuracy = accuracy_score ( y_test , y_predicted ) precision = precision_score ( y_test , y_predicted ) recall = recall_score ( y_test , y_predicted ) f1 = f1_score ( y_test , y_predicted ) # for multiclass classification # we need to compute their mean precision = mean ( precision_score ( y_test , y_predicted , average = None )) recall = mean ( recall_score ( y_test , y_predicted , average = None )) f1 = mean ( f1_score ( y_test , y_predict , average = None ))","title":"Classification"},{"location":"model-evaluation/#confusion-matrix","text":"The confusion matrix is the most important visualization to plot for a classification evaluation. It shows the absolute breakdown of each prediction to see if they are in the correct class; and is usually in a heatmap for better clarity. import matplotlib.pyplot as plt from sklearn.metrics import plot_confusion_matrix confusion = plot_confusion_matrix ( model , X_test , y_test , cmap = plt . cm . Blues ) plt . savefig ( \"logs/confusion_metrics.png\" )","title":"Confusion Matrix"},{"location":"model-evaluation/#classification-report","text":"We can use the classification report to show details of precision, recall & f1-scores for each class. from sklearn.metrics import classification_report cls_report = classification_report ( y_test , y_predict ) print ( cls_report )","title":"Classification Report"},{"location":"model-evaluation/#precision-recall-curve","text":"From sklearn , the precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. from sklearn.metrics import precision_recall_curve from sklearn.metrics import plot_precision_recall_curve import matplotlib.pyplot as plt average_precision = average_precision_score ( y_test , y_score ) disp = plot_precision_recall_curve ( classifier , X_test , y_test ) disp . ax_ . set_title ( '2-class Precision-Recall curve: ' 'AP= {0:0.2f} ' . format ( average_precision )) sklearn documentation","title":"Precision-Recall Curve"},{"location":"model-evaluation/#roc-curve","text":"The receiver operating characteristic (ROC) curve evaluates the performance of a classifier by plotting the True Positive Rate vs the False Positive Rate. The metric, area under curve (AUC) is used. The higher the AUC, the better the model is. The term came about in WWII where this metric is used to determined a receiver operator\u2019s ability to distinguish false positive and true postive correctly in the radar signals. from sklearn.metrics import plot_roc_curve svc_disp = plot_roc_curve ( svc , X_test , y_test ) plt . show () sklearn documentation","title":"ROC Curve"},{"location":"model-evaluation/#regression","text":"For regression problems, the response is always a continuous value, so it requires a different set of evaluation metrics. This website gives an excellent description on all the variants of errors metrics, which are briefly summarised below. Type Desc R-Squared Percentage of variability of dataset that can be explained by the model MSE (Mean Squared Error) Squaring then getting the mean of all errors (so change negatives into positives) RMSE (Squared Root of MSE) So that it gives back the error at the same scale (as it was initially squared) MAE (Mean Absolute Error) For negative errors, convert them to positive and obtain all error means RMSLE (Root Mean Square Log Error) Helps to reduce the effects of outliers compared to RMSE The RMSE result will always be larger or equal to the MAE. However, if all of the errors have the same magnitude, then RMSE=MAE. Since the errors were squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. import numpy as np from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error model = RandomForestRegressor ( n_estimators = 375 ) model3 = model . fit ( X_train , y_train ) fullmodel = model . fit ( predictor , target ) # R2 r2_full = fullmodel . score ( predictor , target ) r2_train = model3 . score ( X_train , y_train ) r2_test = model3 . score ( X_test , y_test ) # get predictions y_predicted_total = model3 . predict ( predictor ) y_predicted_train = model3 . predict ( X_train ) y_predicted_test = model3 . predict ( X_test ) # get MSE MSE_total = mean_squared_error ( target , y_predicted_total ) MSE_train = mean_squared_error ( y_train , y_predicted_train ) MSE_test = mean_squared_error ( y_test , y_predicted_test ) # get RMSE by squared root RMSE_total = np . sqrt ( MSE_total ) RMSE_train = np . sqrt ( MSE_train ) RMSE_test = np . sqrt ( MSE_test ) # get MAE MAE_total = mean_absolute_error ( target , y_predicted_total ) MAE_train = mean_absolute_error ( y_train , y_predicted_train ) MAE_test = mean_absolute_error ( y_test , y_predicted_test ) RMSLE is a very popular evaluation metric in data science competitions. More about it in this medium article . def rmsle ( y , y0 ): assert len ( y ) == len ( y0 ) return np . sqrt ( np . mean ( np . power ( np . log1p ( y ) - np . log1p ( y0 ), 2 )))","title":"Regression"},{"location":"model-explainability/","text":"Model Explainability While sklearn\u2019s supervised models are black boxes, we can derive certain plots and metrics to interprete the outcome and model better. Feature Importance Decision trees and other tree ensemble models, by default, allow us to obtain the importance of features. These are known as impurity-based feature importances. While powerful, we need to understand its limitations, as described by sklearn. they are biased towards high cardinality (numerical) features they are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity). import pandas as pd from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline rf = RandomForestClassifier () model = rf . fit ( X_train , y_train ) def feature_impt ( model , columns , figsize = ( 10 , 2 )): \"\"\"plot feature importance barchart for tree models\"\"\" # sort feature importance in df f_impt = pd . DataFrame ( model . feature_importances_ , index = columns ) f_impt = f_impt . sort_values ( by = 0 , ascending = False ) f_impt . columns = [ 'feature importance' ] # plot bar chart plt . figure ( figsize = figsize ) plt . bar ( f_impt . index , f_impt [ 'feature importance' ]) plt . xticks ( rotation = 'vertical' ) plt . title ( 'Feature Importance' ); return f_impt f_impt = feature_impt ( model ) Permutation Importance To overcome the limitations of feature importance, a variant known as permutation importance is available. It also has the benefits of being about to use for any model. This Kaggle article provides a good clear explanation How it works is the shuffling of individual features and see how it affects model accuarcy. If a feature is important, the model accuarcy will be reduced more. If not important, the accuarcy should be affected a lot less. From Kaggle from sklearn.inspection import permutation_importance result = permutation_importance ( rf , X_test , y_test , n_repeats = 10 , random_state = 42 , n_jobs = 2 ) sorted_idx = result . importances_mean . argsort () plt . figure ( figsize = ( 12 , 10 )) plt . boxplot ( result . importances [ sorted_idx ] . T , vert = False , labels = X . columns [ sorted_idx ]); Another library also provides the same API. import eli5 from eli5.sklearn import PermutationImportance perm = PermutationImportance ( my_model , random_state = 1 ) . fit ( test_X , test_y ) eli5 . show_weights ( perm , feature_names = test_X . columns . tolist ()) The output is as below. +/- refers to the randomness that shuffling resulted in. The higher the weight, the more important the feature is. Negative values are possible, but actually refer to 0; though random chance caused the predictions on shuffled data to be more accurate. From Kaggle Partial Dependence Plots While feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions. Using the fitted model to predict our outcome, and by repeatedly altering the value of just one variable, we can trace the predicted outcomes in a plot to show its dependence on the variable and when it plateaus. from matplotlib import pyplot as plt from pdpbox import pdp , get_dataset , info_plots # Create the data that we will plot pdp_goals = pdp . pdp_isolate ( model = tree_model , dataset = val_X , model_features = feature_names , feature = 'Goal Scored' ) # plot it pdp . pdp_plot ( pdp_goals , 'Goal Scored' ) plt . show () From Kaggle 2D Partial Dependence Plots are also useful for interactions between features. # just need to change pdp_isolate to pdp_interact features_to_plot = [ 'Goal Scored' , 'Distance Covered (Kms)' ] inter1 = pdp . pdp_interact ( model = tree_model , dataset = val_X , model_features = feature_names , features = features_to_plot ) pdp . pdp_interact_plot ( pdp_interact_out = inter1 , feature_names = features_to_plot , plot_type = 'contour' ) plt . show () From Kaggle SHAP SHapley Additive exPlanations ( SHAP ) break down a prediction to show the impact of each feature. It uses a game theory approach which is said to trump over other forms of feature importances because of its consistency. Also, it has a wide support for various classification problems including transformers amd computer vision. Type Desc shap.TreeExplainer(my_model) for tree models shap.DeepExplainer(my_model) for neural networks shap.KernelExplainer(my_model) for all models, but slower, and gives approximate SHAP values import shap explainer = shap . TreeExplainer ( my_model ) shap_values = explainer . shap_values ( data_for_prediction ) # load JS lib in notebook shap . initjs () shap . force_plot ( explainer . expected_value [ 1 ], shap_values [ 1 ], data_for_prediction ) From Kaggle","title":"Model Explainability"},{"location":"model-explainability/#model-explainability","text":"While sklearn\u2019s supervised models are black boxes, we can derive certain plots and metrics to interprete the outcome and model better.","title":"Model Explainability"},{"location":"model-explainability/#feature-importance","text":"Decision trees and other tree ensemble models, by default, allow us to obtain the importance of features. These are known as impurity-based feature importances. While powerful, we need to understand its limitations, as described by sklearn. they are biased towards high cardinality (numerical) features they are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity). import pandas as pd from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' % matplotlib inline rf = RandomForestClassifier () model = rf . fit ( X_train , y_train ) def feature_impt ( model , columns , figsize = ( 10 , 2 )): \"\"\"plot feature importance barchart for tree models\"\"\" # sort feature importance in df f_impt = pd . DataFrame ( model . feature_importances_ , index = columns ) f_impt = f_impt . sort_values ( by = 0 , ascending = False ) f_impt . columns = [ 'feature importance' ] # plot bar chart plt . figure ( figsize = figsize ) plt . bar ( f_impt . index , f_impt [ 'feature importance' ]) plt . xticks ( rotation = 'vertical' ) plt . title ( 'Feature Importance' ); return f_impt f_impt = feature_impt ( model )","title":"Feature Importance"},{"location":"model-explainability/#permutation-importance","text":"To overcome the limitations of feature importance, a variant known as permutation importance is available. It also has the benefits of being about to use for any model. This Kaggle article provides a good clear explanation How it works is the shuffling of individual features and see how it affects model accuarcy. If a feature is important, the model accuarcy will be reduced more. If not important, the accuarcy should be affected a lot less. From Kaggle from sklearn.inspection import permutation_importance result = permutation_importance ( rf , X_test , y_test , n_repeats = 10 , random_state = 42 , n_jobs = 2 ) sorted_idx = result . importances_mean . argsort () plt . figure ( figsize = ( 12 , 10 )) plt . boxplot ( result . importances [ sorted_idx ] . T , vert = False , labels = X . columns [ sorted_idx ]); Another library also provides the same API. import eli5 from eli5.sklearn import PermutationImportance perm = PermutationImportance ( my_model , random_state = 1 ) . fit ( test_X , test_y ) eli5 . show_weights ( perm , feature_names = test_X . columns . tolist ()) The output is as below. +/- refers to the randomness that shuffling resulted in. The higher the weight, the more important the feature is. Negative values are possible, but actually refer to 0; though random chance caused the predictions on shuffled data to be more accurate. From Kaggle","title":"Permutation Importance"},{"location":"model-explainability/#partial-dependence-plots","text":"While feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions. Using the fitted model to predict our outcome, and by repeatedly altering the value of just one variable, we can trace the predicted outcomes in a plot to show its dependence on the variable and when it plateaus. from matplotlib import pyplot as plt from pdpbox import pdp , get_dataset , info_plots # Create the data that we will plot pdp_goals = pdp . pdp_isolate ( model = tree_model , dataset = val_X , model_features = feature_names , feature = 'Goal Scored' ) # plot it pdp . pdp_plot ( pdp_goals , 'Goal Scored' ) plt . show () From Kaggle 2D Partial Dependence Plots are also useful for interactions between features. # just need to change pdp_isolate to pdp_interact features_to_plot = [ 'Goal Scored' , 'Distance Covered (Kms)' ] inter1 = pdp . pdp_interact ( model = tree_model , dataset = val_X , model_features = feature_names , features = features_to_plot ) pdp . pdp_interact_plot ( pdp_interact_out = inter1 , feature_names = features_to_plot , plot_type = 'contour' ) plt . show () From Kaggle","title":"Partial Dependence Plots"},{"location":"model-explainability/#shap","text":"SHapley Additive exPlanations ( SHAP ) break down a prediction to show the impact of each feature. It uses a game theory approach which is said to trump over other forms of feature importances because of its consistency. Also, it has a wide support for various classification problems including transformers amd computer vision. Type Desc shap.TreeExplainer(my_model) for tree models shap.DeepExplainer(my_model) for neural networks shap.KernelExplainer(my_model) for all models, but slower, and gives approximate SHAP values import shap explainer = shap . TreeExplainer ( my_model ) shap_values = explainer . shap_values ( data_for_prediction ) # load JS lib in notebook shap . initjs () shap . force_plot ( explainer . expected_value [ 1 ], shap_values [ 1 ], data_for_prediction ) From Kaggle","title":"SHAP"},{"location":"model-supervised1/","text":"Classification Supervised classification is done when the label is a categorical variable. KNN K-Nearest Neighbours is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. A majority vote of k (www.mathworks.com) Hyperparameter(s) Desc n_neighbors no. nearest neighbours from a point to assign a class. default 5 metric default=\u2019minkowski\u2019, i.e. euclidean import pandas as pd import numpy as np from sklearn.cross_validation import train_test_split from sklearn.neighbors import KNeighborsClassifier X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 0 ) knn = KNeighborsClassifier ( n_neighbors = 5 ) knn . fit ( X_train , y_train ) knn . score ( X_test , y_test ) # 0.53333333333333333 Naive Bayes Naive Bayes is a probabilistic model using the Bayes' theorem. Features are assumed to be independent of each other in a given class (hence naive). This makes the math very easy. E.g., words that are unrelated multiply together to form the final probability. There are 5 variants of Naive Bayes in sklearn . Bernouli and Multinomial models are commonly used for sparse count data like text classification. The latter normally works better. Gaussian model is used for high-dimensional data. Hyperparameter(s) Desc alpha smoothing (generalisation) parameter (default 1.0) University of Michigan: Coursera Data Science in Python Support Vector Machines Support Vector Machines (SVM) involves locating the support vectors of two boundaries to find a maximum tolerance hyperplane. University of Michigan: Coursera Data Science in Python Key hyperparameter(s) Desc C lower C more L2 regularization kernel linear or radial basis function (rbf) gamma from v0.22, gamma is auto or scaled, rather than a float from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) clf = SVC ( kernel = 'rbf' , C = 10 ) . fit ( X_train_scaled , y_train ) If we require linear SVM, we should use LinearSVC with its flexibility of regularization and loss functions, together with faster compute time for large datasets. from sklearn.svm import LinearSVC clf = LinearSVC ( penalty = 'l2' , loss = 'squared_hinge' , C = 1.0 ) Logistic Regression While it is a type of regression, it only outputs a binary value hence it is considered a classification model. Key hyperparameter(s) Desc penalty l1/l2/elasticnet C lower C more regularization from sklearn.linear_model import LogisticRegression clf = LogisticRegression ( C = 100 ) . fit ( X_train , y_train ) acc_train = clf . score ( X_train , y_train ) acc_test = clf . score ( X_test , y_test ) Decision Tree Uses gini index (default) or entropy to split the data at binary level. Strengths: Can select a large number of features that best determine the targets. Weakness: Tends to overfit the data as it will split till the end. Pruning (using max_depth & min_samples_leaf) can be done to remove the leaves to prevent overfitting. Small changes in data can lead to different splits. Not very reproducible for future data (tree ensemble methods are better). Key hyperparameter(s) Desc max_depth The maximum depth of the tree min_samples_leaf The minimum number of samples required before splitting import pandas as pd import numpy as np from sklearn.tree import DecisionTreeClassifier X_train , y_train , x_test , y_test = \\ train_test_split ( predictor , target , test_size = 0.25 ) clf = DecisionTreeClassifier () model = clf . fit ( X_train , y_train , max_depth = 4 , min_samples_leaf = 8 , max_features ) predictions = model . predict ( x_test ) accuracy = sklearn . metrics . accuracy_score ( y_test , predictions ) print ( accuracy ) # 0.973684210526 # Feature importance f_impt = pd . DataFrame ( model . feature_importances_ , index = df . columns [: - 2 ]) f_impt = f_impt . sort_values ( by = 0 , ascending = False ) f_impt . columns = [ 'feature importance' ] print ( f_impt ) # petal width (cm) 0.952542 # petal length (cm) 0.029591 # sepal length (cm) 0.017867 # sepal width (cm) 0.000000 Viewing the decision tree requires installing of the two packages conda install graphviz & conda install pydotplus. from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus dot_data = StringIO () export_graphviz ( dtree , out_file = dot_data , filled = True , rounded = True , special_characters = True ) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) Image ( graph . create_png ()) Tree Ensembles Random Forest An ensemble of decision trees. Used to be one of the most popular tree classifiers. But now generally superceded by other variants like XGBoost, LightGBM etc. Each decision tree is random, introduced through bootstrap (aka, bagging), i.e. sample of size N is created by just repeatedly picking one of the N dataset rows at random with replacement, as well as random feature splits , i.e., when picking the best split for a node, instead of finding the best split across all possible features (decision tree), a random subset of features is chosen and the best split is found within that smaller subset of features As a result of this randomness, the model is very generalized. Key hyperparameter(s) Desc n_estimators no. decision trees max_features max random features to split a leaf node import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.cross_validation import train_test_split import sklearn.metrics train_feature , test_feature , train_target , test_target = \\ train_test_split ( feature , target , test_size = 0.2 ) clf = RandomForestClassifier ( n_estimators = 100 , n_jobs = 4 , verbose = 3 ) model = clf . fit ( train_feature , train_target ) predictions = model . predict ( test_feature ) accuracy = sklearn . metrics . accuracy_score ( y_test , predictions ) print ( accuracy ) 0.823529411765 # feature importance f_impt = pd . DataFrame ( model . feature_importances_ , index = df . columns [: - 2 ]) f_impt = f_impt . sort_values ( by = 0 , ascending = False ) f_impt . columns = [ 'feature importance' ] print ( f_impt ) To see how many decision trees are minimally required make the accuracy plateau. import numpy as np import matplotlib.pylab as plt import seaborn as sns % matplotlib inline trees = range ( 100 ) accuracy = np . zeros ( 100 ) for i in range ( len ( trees )): clf = RandomForestClassifier ( n_estimators = i + 1 ) model = clf . fit ( train_feature , train_target ) predictions = model . predict ( test_feature ) accuracy [ i ] = sklearn . metrics . accuracy_score ( test_target , predictions ) plt . plot ( trees , accuracy ) Accuracy over no. trees trained with Gradient Boosting The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. Gradient boosting tries to fit the new predictor to the residual errors made by the previous predictor. Built in a non-random way, to create a model that makes fewer and fewer mistakes as more trees are added. Once the model is built, making predictions with a gradient boosted tree models is fast and doesn\u2019t use a lot of memory. Key hyperparameter(s) Desc n_estimators no. of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. learning_rate controls how hard each tree tries to correct mistakes from previous round. Higher learning rate, more complex trees. XGBoost XGBoost or eXtreme Gradient Boosting, is a form of gradient boosted decision trees is that designed to be highly efficient, flexible and portable. from xgboost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score X_train , X_test , y_train , y_test = train_test_split ( X , Y , random_state = 0 ) model = XGBClassifier () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) predictions = [ round ( value ) for value in y_pred ] accuracy = accuracy_score ( y_test , predictions ) print ( \"Accuracy: %.2f%% \" % ( accuracy * 100.0 )) LightGBM LightGBM (Light Gradient Boosting) is a lightweight version of gradient boosting developed by Microsoft. It has similar performance to XGBoost but touted to run much faster than it. import lightgbm X_train , X_test , y_train , y_test = \\ train_test_split ( x , y , test_size = 0.2 , random_state = 42 , stratify = y ) # Create the LightGBM data containers train_data = lightgbm . Dataset ( X_train , label = y ) test_data = lightgbm . Dataset ( X_test , label = y_test ) parameters = { 'application' : 'binary' , 'objective' : 'binary' , 'metric' : 'auc' , 'is_unbalance' : 'true' , 'boosting' : 'gbdt' , 'num_leaves' : 31 , 'feature_fraction' : 0.5 , 'bagging_fraction' : 0.5 , 'bagging_freq' : 20 , 'learning_rate' : 0.05 , 'verbose' : 0 } model = lightgbm . train ( parameters , train_data , valid_sets = test_data , num_boost_round = 5000 , early_stopping_rounds = 100 ) CatBoost Category Boosting has high performances compared to other popular models, and does not require conversion of categorical values into numbers. It is said to be even faster than LighGBM, and allows model to be ran using GPU. Voting The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote (hard vote) or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. Tree ensembles is an example of a majority voting model. Stacking Stacked generalization is a method for combining estimators to reduce their biases. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation. The fundamental difference between voting and stacking is how the final aggregation is done. In voting, user-specified weights are used to combine the classifiers whereas stacking performs this aggregation by using a blender/meta classifier.","title":"Classification"},{"location":"model-supervised1/#classification","text":"Supervised classification is done when the label is a categorical variable.","title":"Classification"},{"location":"model-supervised1/#knn","text":"K-Nearest Neighbours is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. A majority vote of k (www.mathworks.com) Hyperparameter(s) Desc n_neighbors no. nearest neighbours from a point to assign a class. default 5 metric default=\u2019minkowski\u2019, i.e. euclidean import pandas as pd import numpy as np from sklearn.cross_validation import train_test_split from sklearn.neighbors import KNeighborsClassifier X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 0 ) knn = KNeighborsClassifier ( n_neighbors = 5 ) knn . fit ( X_train , y_train ) knn . score ( X_test , y_test ) # 0.53333333333333333","title":"KNN"},{"location":"model-supervised1/#naive-bayes","text":"Naive Bayes is a probabilistic model using the Bayes' theorem. Features are assumed to be independent of each other in a given class (hence naive). This makes the math very easy. E.g., words that are unrelated multiply together to form the final probability. There are 5 variants of Naive Bayes in sklearn . Bernouli and Multinomial models are commonly used for sparse count data like text classification. The latter normally works better. Gaussian model is used for high-dimensional data. Hyperparameter(s) Desc alpha smoothing (generalisation) parameter (default 1.0) University of Michigan: Coursera Data Science in Python","title":"Naive Bayes"},{"location":"model-supervised1/#support-vector-machines","text":"Support Vector Machines (SVM) involves locating the support vectors of two boundaries to find a maximum tolerance hyperplane. University of Michigan: Coursera Data Science in Python Key hyperparameter(s) Desc C lower C more L2 regularization kernel linear or radial basis function (rbf) gamma from v0.22, gamma is auto or scaled, rather than a float from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) clf = SVC ( kernel = 'rbf' , C = 10 ) . fit ( X_train_scaled , y_train ) If we require linear SVM, we should use LinearSVC with its flexibility of regularization and loss functions, together with faster compute time for large datasets. from sklearn.svm import LinearSVC clf = LinearSVC ( penalty = 'l2' , loss = 'squared_hinge' , C = 1.0 )","title":"Support Vector Machines"},{"location":"model-supervised1/#logistic-regression","text":"While it is a type of regression, it only outputs a binary value hence it is considered a classification model. Key hyperparameter(s) Desc penalty l1/l2/elasticnet C lower C more regularization from sklearn.linear_model import LogisticRegression clf = LogisticRegression ( C = 100 ) . fit ( X_train , y_train ) acc_train = clf . score ( X_train , y_train ) acc_test = clf . score ( X_test , y_test )","title":"Logistic Regression"},{"location":"model-supervised1/#decision-tree","text":"Uses gini index (default) or entropy to split the data at binary level. Strengths: Can select a large number of features that best determine the targets. Weakness: Tends to overfit the data as it will split till the end. Pruning (using max_depth & min_samples_leaf) can be done to remove the leaves to prevent overfitting. Small changes in data can lead to different splits. Not very reproducible for future data (tree ensemble methods are better). Key hyperparameter(s) Desc max_depth The maximum depth of the tree min_samples_leaf The minimum number of samples required before splitting import pandas as pd import numpy as np from sklearn.tree import DecisionTreeClassifier X_train , y_train , x_test , y_test = \\ train_test_split ( predictor , target , test_size = 0.25 ) clf = DecisionTreeClassifier () model = clf . fit ( X_train , y_train , max_depth = 4 , min_samples_leaf = 8 , max_features ) predictions = model . predict ( x_test ) accuracy = sklearn . metrics . accuracy_score ( y_test , predictions ) print ( accuracy ) # 0.973684210526 # Feature importance f_impt = pd . DataFrame ( model . feature_importances_ , index = df . columns [: - 2 ]) f_impt = f_impt . sort_values ( by = 0 , ascending = False ) f_impt . columns = [ 'feature importance' ] print ( f_impt ) # petal width (cm) 0.952542 # petal length (cm) 0.029591 # sepal length (cm) 0.017867 # sepal width (cm) 0.000000 Viewing the decision tree requires installing of the two packages conda install graphviz & conda install pydotplus. from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus dot_data = StringIO () export_graphviz ( dtree , out_file = dot_data , filled = True , rounded = True , special_characters = True ) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) Image ( graph . create_png ())","title":"Decision Tree"},{"location":"model-supervised1/#tree-ensembles","text":"","title":"Tree Ensembles"},{"location":"model-supervised1/#random-forest","text":"An ensemble of decision trees. Used to be one of the most popular tree classifiers. But now generally superceded by other variants like XGBoost, LightGBM etc. Each decision tree is random, introduced through bootstrap (aka, bagging), i.e. sample of size N is created by just repeatedly picking one of the N dataset rows at random with replacement, as well as random feature splits , i.e., when picking the best split for a node, instead of finding the best split across all possible features (decision tree), a random subset of features is chosen and the best split is found within that smaller subset of features As a result of this randomness, the model is very generalized. Key hyperparameter(s) Desc n_estimators no. decision trees max_features max random features to split a leaf node import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.cross_validation import train_test_split import sklearn.metrics train_feature , test_feature , train_target , test_target = \\ train_test_split ( feature , target , test_size = 0.2 ) clf = RandomForestClassifier ( n_estimators = 100 , n_jobs = 4 , verbose = 3 ) model = clf . fit ( train_feature , train_target ) predictions = model . predict ( test_feature ) accuracy = sklearn . metrics . accuracy_score ( y_test , predictions ) print ( accuracy ) 0.823529411765 # feature importance f_impt = pd . DataFrame ( model . feature_importances_ , index = df . columns [: - 2 ]) f_impt = f_impt . sort_values ( by = 0 , ascending = False ) f_impt . columns = [ 'feature importance' ] print ( f_impt ) To see how many decision trees are minimally required make the accuracy plateau. import numpy as np import matplotlib.pylab as plt import seaborn as sns % matplotlib inline trees = range ( 100 ) accuracy = np . zeros ( 100 ) for i in range ( len ( trees )): clf = RandomForestClassifier ( n_estimators = i + 1 ) model = clf . fit ( train_feature , train_target ) predictions = model . predict ( test_feature ) accuracy [ i ] = sklearn . metrics . accuracy_score ( test_target , predictions ) plt . plot ( trees , accuracy ) Accuracy over no. trees trained with","title":"Random Forest"},{"location":"model-supervised1/#gradient-boosting","text":"The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. Gradient boosting tries to fit the new predictor to the residual errors made by the previous predictor. Built in a non-random way, to create a model that makes fewer and fewer mistakes as more trees are added. Once the model is built, making predictions with a gradient boosted tree models is fast and doesn\u2019t use a lot of memory. Key hyperparameter(s) Desc n_estimators no. of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. learning_rate controls how hard each tree tries to correct mistakes from previous round. Higher learning rate, more complex trees.","title":"Gradient Boosting"},{"location":"model-supervised1/#xgboost","text":"XGBoost or eXtreme Gradient Boosting, is a form of gradient boosted decision trees is that designed to be highly efficient, flexible and portable. from xgboost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score X_train , X_test , y_train , y_test = train_test_split ( X , Y , random_state = 0 ) model = XGBClassifier () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) predictions = [ round ( value ) for value in y_pred ] accuracy = accuracy_score ( y_test , predictions ) print ( \"Accuracy: %.2f%% \" % ( accuracy * 100.0 ))","title":"XGBoost"},{"location":"model-supervised1/#lightgbm","text":"LightGBM (Light Gradient Boosting) is a lightweight version of gradient boosting developed by Microsoft. It has similar performance to XGBoost but touted to run much faster than it. import lightgbm X_train , X_test , y_train , y_test = \\ train_test_split ( x , y , test_size = 0.2 , random_state = 42 , stratify = y ) # Create the LightGBM data containers train_data = lightgbm . Dataset ( X_train , label = y ) test_data = lightgbm . Dataset ( X_test , label = y_test ) parameters = { 'application' : 'binary' , 'objective' : 'binary' , 'metric' : 'auc' , 'is_unbalance' : 'true' , 'boosting' : 'gbdt' , 'num_leaves' : 31 , 'feature_fraction' : 0.5 , 'bagging_fraction' : 0.5 , 'bagging_freq' : 20 , 'learning_rate' : 0.05 , 'verbose' : 0 } model = lightgbm . train ( parameters , train_data , valid_sets = test_data , num_boost_round = 5000 , early_stopping_rounds = 100 )","title":"LightGBM"},{"location":"model-supervised1/#catboost","text":"Category Boosting has high performances compared to other popular models, and does not require conversion of categorical values into numbers. It is said to be even faster than LighGBM, and allows model to be ran using GPU.","title":"CatBoost"},{"location":"model-supervised1/#voting","text":"The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote (hard vote) or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. Tree ensembles is an example of a majority voting model.","title":"Voting"},{"location":"model-supervised1/#stacking","text":"Stacked generalization is a method for combining estimators to reduce their biases. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation. The fundamental difference between voting and stacking is how the final aggregation is done. In voting, user-specified weights are used to combine the classifiers whereas stacking performs this aggregation by using a blender/meta classifier.","title":"Stacking"},{"location":"model-supervised2/","text":"Regression Regression is done when the label is a continuous variable. The three regressors, LASSO, Ridge and Elastic Net that have regularization are elaborated here . OLS Regression Ordinary Least Squares Regression or OLS Regression is the most basic form and fundamental of regression. A best fit line \u0177 = a + bx is drawn based on the ordinary least squares method. i.e., least total area of squares (sum of squares) with length from each x,y point to regresson line. OLS can be conducted using statsmodel package. model = smf . ols ( formula = 'diameter ~ depth' , data = df3 ) . fit () print model . summary () OLS Regression Results ============================================================================== Dep . Variable : diameter R - squared : 0.512 Model : OLS Adj . R - squared : 0.512 Method : Least Squares F - statistic : 1.895e+04 Date : Tue , 02 Aug 2016 Prob ( F - statistic ): 0.00 Time : 17 : 10 : 34 Log - Likelihood : - 51812. No . Observations : 18067 AIC : 1.036e+05 Df Residuals : 18065 BIC : 1.036e+05 Df Model : 1 Covariance Type : nonrobust ============================================================================== coef std err t P >| t | [ 95.0 % Conf . Int . ] ------------------------------------------------------------------------------ Intercept 2.2523 0.054 41.656 0.000 2.146 2.358 depth 11.5836 0.084 137.675 0.000 11.419 11.749 ============================================================================== Omnibus : 12117.030 Durbin - Watson : 0.673 Prob ( Omnibus ): 0.000 Jarque - Bera ( JB ): 391356.565 Skew : 2.771 Prob ( JB ): 0.00 Kurtosis : 25.117 Cond . No . 3.46 ============================================================================== Warnings : [ 1 ] Standard Errors assume that the covariance matrix of the errors is correctly specified . Or sci-kit learn package. from sklearn import linear_model reg = linear_model . LinearRegression () model = reg . fit ([[ 0 , 0 ], [ 1 , 1 ], [ 2 , 2 ]], [ 0 , 1 , 2 ]) print ( model ) # LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) print ( reg . coef_ ) # array([ 0.5, 0.5]) r2_trains = model . score ( X_train , y_train ) r2_tests = model . score ( X_test , y_test ) LASSO Regression LASSO refers to Least Absolute Shrinkage and Selection Operator Regression. When alpha = 0, then it is a normal OLS regression. import pandas as pd import numpy as py from sklearn import preprocessing from sklearn.cross_validation import train_test_split from sklearn.linear_model import LassoLarsCV import sklearn.metrics from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error for i in df . columns [: - 1 ]: df [ i ] = preprocessing . scale ( df [ i ] . astype ( 'float64' )) df . describe () train_feature , test_feature , train_target , test_target = \\ train_test_split ( feature , target , random_state = 123 , test_size = 0.2 ) model = LassoLarsCV ( cv = 10 , precompute = False ) model = model . fit ( train_feature , train_target ) # Compare the regression coefficients, and see which one LASSO removed. # LSTAT is the most important predictor, # followed by RM, DIS, and RAD. AGE is removed by LASSO df2 = pd . DataFrame ( model . coef_ , index = feature . columns ) df2 . sort_values ( by = 0 , ascending = False ) # RM 3.050843 # RAD 2.040252 # ZN 1.004318 # B 0.629933 # CHAS 0.317948 # INDUS 0.225688 # AGE 0.000000 # CRIM -0.770291 # NOX -1.617137 # TAX -1.731576 # PTRATIO -1.923485 # DIS -2.733660 # LSTAT -3.878356 train_error = mean_squared_error ( train_target , model . predict ( train_feature )) test_error = mean_squared_error ( test_target , model . predict ( test_feature )) # MSE print ( 'training data MSE' ) print ( train_error ) print ( 'test data MSE' ) print ( test_error ) # R-square rsquared_train = model . score ( train_feature , train_target ) rsquared_test = model . score ( test_feature , test_target ) print ( 'training data R-square' ) print ( rsquared_train ) print ( 'test data R-square' ) print ( rsquared_test ) Ridge Regression import panda as pd import numpy as np from sklearn.linear_model import Ridge from sklearn.preprocessing import MinMaxScaler X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) linridge = Ridge ( alpha = 20.0 ) . fit ( X_train_scaled , y_train ) print ( 'ridge regression linear model intercept: {} ' . format ( linridge . intercept_ )) print ( 'ridge regression linear model coeff: \\n {} ' . format ( linridge . coef_ )) print ( 'R-squared score (train): {:.3f} ' . format ( linridge . score ( X_train_scaled , y_train ))) print ( 'R-squared score (test): {:.3f} ' . format ( linridge . score ( X_test_scaled , y_test ))) print ( 'Number of non-zero features: {} ' . format ( np . sum ( linridge . coef_ != 0 ))) Elastic Net Elastic Net combines the penalties of ridge regression and lasso to get the best of both worlds. Tree Regressors For each of the tree classifiers, there exists a regressor. from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor","title":"Regression"},{"location":"model-supervised2/#regression","text":"Regression is done when the label is a continuous variable. The three regressors, LASSO, Ridge and Elastic Net that have regularization are elaborated here .","title":"Regression"},{"location":"model-supervised2/#ols-regression","text":"Ordinary Least Squares Regression or OLS Regression is the most basic form and fundamental of regression. A best fit line \u0177 = a + bx is drawn based on the ordinary least squares method. i.e., least total area of squares (sum of squares) with length from each x,y point to regresson line. OLS can be conducted using statsmodel package. model = smf . ols ( formula = 'diameter ~ depth' , data = df3 ) . fit () print model . summary () OLS Regression Results ============================================================================== Dep . Variable : diameter R - squared : 0.512 Model : OLS Adj . R - squared : 0.512 Method : Least Squares F - statistic : 1.895e+04 Date : Tue , 02 Aug 2016 Prob ( F - statistic ): 0.00 Time : 17 : 10 : 34 Log - Likelihood : - 51812. No . Observations : 18067 AIC : 1.036e+05 Df Residuals : 18065 BIC : 1.036e+05 Df Model : 1 Covariance Type : nonrobust ============================================================================== coef std err t P >| t | [ 95.0 % Conf . Int . ] ------------------------------------------------------------------------------ Intercept 2.2523 0.054 41.656 0.000 2.146 2.358 depth 11.5836 0.084 137.675 0.000 11.419 11.749 ============================================================================== Omnibus : 12117.030 Durbin - Watson : 0.673 Prob ( Omnibus ): 0.000 Jarque - Bera ( JB ): 391356.565 Skew : 2.771 Prob ( JB ): 0.00 Kurtosis : 25.117 Cond . No . 3.46 ============================================================================== Warnings : [ 1 ] Standard Errors assume that the covariance matrix of the errors is correctly specified . Or sci-kit learn package. from sklearn import linear_model reg = linear_model . LinearRegression () model = reg . fit ([[ 0 , 0 ], [ 1 , 1 ], [ 2 , 2 ]], [ 0 , 1 , 2 ]) print ( model ) # LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) print ( reg . coef_ ) # array([ 0.5, 0.5]) r2_trains = model . score ( X_train , y_train ) r2_tests = model . score ( X_test , y_test )","title":"OLS Regression"},{"location":"model-supervised2/#lasso-regression","text":"LASSO refers to Least Absolute Shrinkage and Selection Operator Regression. When alpha = 0, then it is a normal OLS regression. import pandas as pd import numpy as py from sklearn import preprocessing from sklearn.cross_validation import train_test_split from sklearn.linear_model import LassoLarsCV import sklearn.metrics from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error for i in df . columns [: - 1 ]: df [ i ] = preprocessing . scale ( df [ i ] . astype ( 'float64' )) df . describe () train_feature , test_feature , train_target , test_target = \\ train_test_split ( feature , target , random_state = 123 , test_size = 0.2 ) model = LassoLarsCV ( cv = 10 , precompute = False ) model = model . fit ( train_feature , train_target ) # Compare the regression coefficients, and see which one LASSO removed. # LSTAT is the most important predictor, # followed by RM, DIS, and RAD. AGE is removed by LASSO df2 = pd . DataFrame ( model . coef_ , index = feature . columns ) df2 . sort_values ( by = 0 , ascending = False ) # RM 3.050843 # RAD 2.040252 # ZN 1.004318 # B 0.629933 # CHAS 0.317948 # INDUS 0.225688 # AGE 0.000000 # CRIM -0.770291 # NOX -1.617137 # TAX -1.731576 # PTRATIO -1.923485 # DIS -2.733660 # LSTAT -3.878356 train_error = mean_squared_error ( train_target , model . predict ( train_feature )) test_error = mean_squared_error ( test_target , model . predict ( test_feature )) # MSE print ( 'training data MSE' ) print ( train_error ) print ( 'test data MSE' ) print ( test_error ) # R-square rsquared_train = model . score ( train_feature , train_target ) rsquared_test = model . score ( test_feature , test_target ) print ( 'training data R-square' ) print ( rsquared_train ) print ( 'test data R-square' ) print ( rsquared_test )","title":"LASSO Regression"},{"location":"model-supervised2/#ridge-regression","text":"import panda as pd import numpy as np from sklearn.linear_model import Ridge from sklearn.preprocessing import MinMaxScaler X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) linridge = Ridge ( alpha = 20.0 ) . fit ( X_train_scaled , y_train ) print ( 'ridge regression linear model intercept: {} ' . format ( linridge . intercept_ )) print ( 'ridge regression linear model coeff: \\n {} ' . format ( linridge . coef_ )) print ( 'R-squared score (train): {:.3f} ' . format ( linridge . score ( X_train_scaled , y_train ))) print ( 'R-squared score (test): {:.3f} ' . format ( linridge . score ( X_test_scaled , y_test ))) print ( 'Number of non-zero features: {} ' . format ( np . sum ( linridge . coef_ != 0 )))","title":"Ridge Regression"},{"location":"model-supervised2/#elastic-net","text":"Elastic Net combines the penalties of ridge regression and lasso to get the best of both worlds.","title":"Elastic Net"},{"location":"model-supervised2/#tree-regressors","text":"For each of the tree classifiers, there exists a regressor. from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor","title":"Tree Regressors"},{"location":"model-tuning/","text":"Model Split & Tuning Each model usually have 1 or more hyperparameters to adjust. A change in the hyperparameters during training will often result in a change in model performance. Therefore we need to \"tune\" the models by finding the best values of hyperparameters that would derive an overall best model performance. Grid Search In grid search, we indicate a list of values for each hyperparameter, and use grid search to permutate to give you the best parameters for a given evaluation metric. Below is an example for a classification problem. from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier () grid_values = { 'n_estimators' :[ 150 , 175 , 200 , 225 ]} grid = GridSearchCV ( model , param_grid = grid_values , scoring = \"f1\" , cv = 5 ) grid . fit ( predictor , target ) print ( grid . best_params_ ) print ( grid . best_score_ ) Auto Tuners Grid search or random search, while semi-automated, is still very tedious. The list of values you give for searching might not be out of range or far off from the optimal, hence it might result in several iterations of changing the values to find a good one. Bayesian optimization with gaussian processes is a popular technique to overcome this. It involves just defining the start and end value of each hyperparameter, and using bayesian probability, get the best parameters for a model. Bayesian Optimization This package is my favourite, due to its high level api and therefore, ease of use. We just need to define two things: a dictionary which parameters and a range of values to tune within a function with arguments that accepts the parameters, and output with the evaluation scoring. The model fitting and prediction, and scoring will thus reside here. Besides its ease of use: The api also allows an exploration option n_iter\u200b , which balances the exploitation space defined by the bayesian optimiser. It auto generates a table output for each iteration with the scoring and parameter values selected. from bayes_opt import BayesianOptimization from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Function output with scores def black_box ( n_estimators , max_depth ): params = { \"n_jobs\" : 5 , \"n_estimators\" : int ( round ( n_estimators )), \"max_depth\" : max_depth } model = RandomForestRegressor ( ** params ) model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) score = np . sqrt ( mean_squared_error ( y_test , y_predict )) return - score # Parameter search space pbounds = { 'n_estimators' : ( 1 , 5 ), 'max_depth' : ( 10 , 50 )} optimizer = BayesianOptimization ( black_box , pbounds , random_state = 2100 ) optimizer . maximize ( init_points = 10 , n_iter = 5 ) best_param = optimizer . max [ 'params' ] However, it has a few downsides: The most glaring is that it does not allow an option for parameters which should only be in integers (notice that n_estimators above has to be wrapped in an int function, else there will be an error). The best parameter output will still output as a float, and we have to change it back to an integer before retraining our final model. The optimiser assumes that the best score is the highest value. However, for regression models, it is usually measured by loss or error metrics, like RMSE; which means the lower value the better. We will need to reverse this manually by changing the score to a negative value (as with above code) Bayesian Tuning and Bandits Bayesian Tuning and Bandits is a more recent library developed by MIT. Its main advantage to me over the above package is: Allows specification for int or float options for parameter values. from btb.tuning import GP from btb import HyperParameter , ParamTypes from sklearn.metrics import mean_squared_error def auto_tuning ( tunables , epoch , X_train , X_test , y_train , y_test ): \"\"\"Auto-tuner using BTB library\"\"\" tuner = GP ( tunables ) parameters = tuner . propose () score_list = [] param_list = [] for i in range ( epoch ): model = RandomForestRegressor ( ** parameters , n_jobs = 10 , verbose = 3 ) model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) score = np . sqrt ( mean_squared_error ( y_test , y_predict )) # store scores & parameters score_list . append ( score ) param_list . append ( parameters ) print ( 'epoch: {} , rmse: {} , param: {} ' . format ( i + 1 , score , parameters )) score = - score # get new parameters tuner . add ( parameters , score ) parameters = tuner . propose () best_s = tuner . _best_score best_score_index = score_list . index ( best_s ) best_param = param_list [ best_score_index ] print ( ' \\n best rmse: {} ' . format ( best_s )) print ( 'best parameters: {} ' . format ( best_param )) return best_param tunables = [( 'n_estimators' , HyperParameter ( ParamTypes . INT , [ 500 , 2000 ])), ( 'max_depth' , HyperParameter ( ParamTypes . INT , [ 3 , 20 ]))] best_param = auto_tuning ( tunables , 5 , X_train , X_test , y_train , y_test ) The disadvantages are: Certain options requires me to code manually, which I thought the authors can easily package them into a function. examples include: Number of iterations; I have to write a for-loop for this Printing of results for each iteration Obtaining of parameters for the best_scores; I have to store all the scores and parameters in a list and then search back the index to get the best parameters. Scikit-Optimize scikit-optimize appears to be another popular package that implements bayesian optimization.","title":"Model Tuning"},{"location":"model-tuning/#model-split-tuning","text":"Each model usually have 1 or more hyperparameters to adjust. A change in the hyperparameters during training will often result in a change in model performance. Therefore we need to \"tune\" the models by finding the best values of hyperparameters that would derive an overall best model performance.","title":"Model Split &amp; Tuning"},{"location":"model-tuning/#grid-search","text":"In grid search, we indicate a list of values for each hyperparameter, and use grid search to permutate to give you the best parameters for a given evaluation metric. Below is an example for a classification problem. from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier () grid_values = { 'n_estimators' :[ 150 , 175 , 200 , 225 ]} grid = GridSearchCV ( model , param_grid = grid_values , scoring = \"f1\" , cv = 5 ) grid . fit ( predictor , target ) print ( grid . best_params_ ) print ( grid . best_score_ )","title":"Grid Search"},{"location":"model-tuning/#auto-tuners","text":"Grid search or random search, while semi-automated, is still very tedious. The list of values you give for searching might not be out of range or far off from the optimal, hence it might result in several iterations of changing the values to find a good one. Bayesian optimization with gaussian processes is a popular technique to overcome this. It involves just defining the start and end value of each hyperparameter, and using bayesian probability, get the best parameters for a model.","title":"Auto Tuners"},{"location":"model-tuning/#bayesian-optimization","text":"This package is my favourite, due to its high level api and therefore, ease of use. We just need to define two things: a dictionary which parameters and a range of values to tune within a function with arguments that accepts the parameters, and output with the evaluation scoring. The model fitting and prediction, and scoring will thus reside here. Besides its ease of use: The api also allows an exploration option n_iter\u200b , which balances the exploitation space defined by the bayesian optimiser. It auto generates a table output for each iteration with the scoring and parameter values selected. from bayes_opt import BayesianOptimization from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Function output with scores def black_box ( n_estimators , max_depth ): params = { \"n_jobs\" : 5 , \"n_estimators\" : int ( round ( n_estimators )), \"max_depth\" : max_depth } model = RandomForestRegressor ( ** params ) model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) score = np . sqrt ( mean_squared_error ( y_test , y_predict )) return - score # Parameter search space pbounds = { 'n_estimators' : ( 1 , 5 ), 'max_depth' : ( 10 , 50 )} optimizer = BayesianOptimization ( black_box , pbounds , random_state = 2100 ) optimizer . maximize ( init_points = 10 , n_iter = 5 ) best_param = optimizer . max [ 'params' ] However, it has a few downsides: The most glaring is that it does not allow an option for parameters which should only be in integers (notice that n_estimators above has to be wrapped in an int function, else there will be an error). The best parameter output will still output as a float, and we have to change it back to an integer before retraining our final model. The optimiser assumes that the best score is the highest value. However, for regression models, it is usually measured by loss or error metrics, like RMSE; which means the lower value the better. We will need to reverse this manually by changing the score to a negative value (as with above code)","title":"Bayesian Optimization"},{"location":"model-tuning/#bayesian-tuning-and-bandits","text":"Bayesian Tuning and Bandits is a more recent library developed by MIT. Its main advantage to me over the above package is: Allows specification for int or float options for parameter values. from btb.tuning import GP from btb import HyperParameter , ParamTypes from sklearn.metrics import mean_squared_error def auto_tuning ( tunables , epoch , X_train , X_test , y_train , y_test ): \"\"\"Auto-tuner using BTB library\"\"\" tuner = GP ( tunables ) parameters = tuner . propose () score_list = [] param_list = [] for i in range ( epoch ): model = RandomForestRegressor ( ** parameters , n_jobs = 10 , verbose = 3 ) model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) score = np . sqrt ( mean_squared_error ( y_test , y_predict )) # store scores & parameters score_list . append ( score ) param_list . append ( parameters ) print ( 'epoch: {} , rmse: {} , param: {} ' . format ( i + 1 , score , parameters )) score = - score # get new parameters tuner . add ( parameters , score ) parameters = tuner . propose () best_s = tuner . _best_score best_score_index = score_list . index ( best_s ) best_param = param_list [ best_score_index ] print ( ' \\n best rmse: {} ' . format ( best_s )) print ( 'best parameters: {} ' . format ( best_param )) return best_param tunables = [( 'n_estimators' , HyperParameter ( ParamTypes . INT , [ 500 , 2000 ])), ( 'max_depth' , HyperParameter ( ParamTypes . INT , [ 3 , 20 ]))] best_param = auto_tuning ( tunables , 5 , X_train , X_test , y_train , y_test ) The disadvantages are: Certain options requires me to code manually, which I thought the authors can easily package them into a function. examples include: Number of iterations; I have to write a for-loop for this Printing of results for each iteration Obtaining of parameters for the best_scores; I have to store all the scores and parameters in a list and then search back the index to get the best parameters.","title":"Bayesian Tuning and Bandits"},{"location":"model-tuning/#scikit-optimize","text":"scikit-optimize appears to be another popular package that implements bayesian optimization.","title":"Scikit-Optimize"},{"location":"model-unsupervised1/","text":"Transformations Processes that extract or compute information. Kernel Density Estimation University of Michigan: Coursera Data Science in Python Dimensionality Reduction A very common problem in machine learning is the presence of numerous features. This lead to a problem called the Curse of Dimensionality . The curse of dimentionality makes it difficult to visualise the datasets with many dimensions. It also results in data sparsity , where information important for training a model is spread across many features. This is where dimension reduction helps by reduction the number of features while retaining much of the information. PCA PCA summarises multiple fields of data into principal components, usually just 2 so that it is easier to visualise in a 2-dimensional plot. The 1st component will show the most variance of the entire dataset in the hyperplane, while the 2nd shows the 2nd shows the most variance at a right angle to the 1st. Because of the strong variance between data points, patterns tend to be teased out from a high dimension to even when there\u2019s just two dimensions. These 2 components can serve as new features for a supervised analysis. In short, PCA finds the best possible characteristics, that summarises the classes of a feature. Two excellent sites elaborate more: setosa, quora. The most challenging part of PCA is interpreting the components. from sklearn.decomposition import PCA def pca_explained ( X , threshold ): \"\"\" prints optimal principal components based on threshold of PCA's explained variance Args ---- X: (df, array) of features threshold: (float) % of explained variance as cut off point \"\"\" features = X . shape [ 1 ] for i in range ( 2 , features ): pca = PCA ( n_components = i ) . fit ( X ) sum_ = pca . explained_variance_ratio_ percent = sum ( sum_ ) print ( ' {} components at {:.2f}% e xplained variance' . format ( i , percent * 100 )) if percent > threshold : break pca_explained ( X , 0.85 ) # 2 components at 61.64% explained variance # 3 components at 77.41% explained variance # 4 components at 86.63% explained variance MDS Multi-Dimensional Scaling (MDS) is a type of manifold learning algorithm that to visualize a high dimensional dataset and project it onto a lower dimensional space - in most cases, a two-dimensional page. PCA is weak in this aspect. sklearn gives a good overview of various manifold techniques. from adspy_shared_utilities import plot_labelled_scatter from sklearn.preprocessing import StandardScaler from sklearn.manifold import MDS # each feature should be centered (zero mean) and with unit variance X_fruits_normalized = StandardScaler () . fit ( X_fruits ) . transform ( X_fruits ) mds = MDS ( n_components = 2 ) X_fruits_mds = mds . fit_transform ( X_fruits_normalized ) t-SNE t-Distributed Stochastic Neighbor Embedding (t-SNE) is a powerful manifold learning algorithm for visualizing clusters. It finds a two-dimensional representation of your data, such that the distances between points in the 2D scatterplot match as closely as possible the distances between the same points in the original high dimensional dataset. In particular, t-SNE gives much more weight to preserving information about distances between points that are neighbors. More behind this algorithm. from sklearn.manifold import TSNE tsne = TSNE ( random_state = 0 ) X_tsne = tsne . fit_transform ( X_fruits_normalized ) plot_labelled_scatter ( X_tsne , y_fruits , [ 'apple' , 'mandarin' , 'orange' , 'lemon' ]) plt . xlabel ( 'First t-SNE feature' ) plt . ylabel ( 'Second t-SNE feature' ) plt . title ( 'Fruits dataset t-SNE' ); LDA Latent Dirichlet Allocation is another dimension reduction method, but unlike PCA, it is a supervised method. It attempts to find a feature subspace or decision boundary that maximizes class separability. It then projects the data points to new dimensions in a way that the clusters are as separate from each other as possible and the individual elements within a cluster are as close to the centroid of the cluster as possible. More from sebastianraschka.com and stackabuse.com . From sebastianraschka.com from sklearn.decomposition import LatentDirichletAllocation from sklearn.datasets import make_multilabel_classification # This produces a feature matrix of token counts, # similar to what CountVectorizer would produce on text. X , _ = make_multilabel_classification ( random_state = 0 ) lda = LatentDirichletAllocation ( n_components = 5 , random_state = 0 ) X_lda = lda . fit_transform ( X , y ) # check the explained variance percent = lda . explained_variance_ratio_ print ( percent ) print ( sum ( percent ))","title":"Transformation"},{"location":"model-unsupervised1/#transformations","text":"Processes that extract or compute information.","title":"Transformations"},{"location":"model-unsupervised1/#kernel-density-estimation","text":"University of Michigan: Coursera Data Science in Python","title":"Kernel Density Estimation"},{"location":"model-unsupervised1/#dimensionality-reduction","text":"A very common problem in machine learning is the presence of numerous features. This lead to a problem called the Curse of Dimensionality . The curse of dimentionality makes it difficult to visualise the datasets with many dimensions. It also results in data sparsity , where information important for training a model is spread across many features. This is where dimension reduction helps by reduction the number of features while retaining much of the information.","title":"Dimensionality Reduction"},{"location":"model-unsupervised1/#pca","text":"PCA summarises multiple fields of data into principal components, usually just 2 so that it is easier to visualise in a 2-dimensional plot. The 1st component will show the most variance of the entire dataset in the hyperplane, while the 2nd shows the 2nd shows the most variance at a right angle to the 1st. Because of the strong variance between data points, patterns tend to be teased out from a high dimension to even when there\u2019s just two dimensions. These 2 components can serve as new features for a supervised analysis. In short, PCA finds the best possible characteristics, that summarises the classes of a feature. Two excellent sites elaborate more: setosa, quora. The most challenging part of PCA is interpreting the components. from sklearn.decomposition import PCA def pca_explained ( X , threshold ): \"\"\" prints optimal principal components based on threshold of PCA's explained variance Args ---- X: (df, array) of features threshold: (float) % of explained variance as cut off point \"\"\" features = X . shape [ 1 ] for i in range ( 2 , features ): pca = PCA ( n_components = i ) . fit ( X ) sum_ = pca . explained_variance_ratio_ percent = sum ( sum_ ) print ( ' {} components at {:.2f}% e xplained variance' . format ( i , percent * 100 )) if percent > threshold : break pca_explained ( X , 0.85 ) # 2 components at 61.64% explained variance # 3 components at 77.41% explained variance # 4 components at 86.63% explained variance","title":"PCA"},{"location":"model-unsupervised1/#mds","text":"Multi-Dimensional Scaling (MDS) is a type of manifold learning algorithm that to visualize a high dimensional dataset and project it onto a lower dimensional space - in most cases, a two-dimensional page. PCA is weak in this aspect. sklearn gives a good overview of various manifold techniques. from adspy_shared_utilities import plot_labelled_scatter from sklearn.preprocessing import StandardScaler from sklearn.manifold import MDS # each feature should be centered (zero mean) and with unit variance X_fruits_normalized = StandardScaler () . fit ( X_fruits ) . transform ( X_fruits ) mds = MDS ( n_components = 2 ) X_fruits_mds = mds . fit_transform ( X_fruits_normalized )","title":"MDS"},{"location":"model-unsupervised1/#t-sne","text":"t-Distributed Stochastic Neighbor Embedding (t-SNE) is a powerful manifold learning algorithm for visualizing clusters. It finds a two-dimensional representation of your data, such that the distances between points in the 2D scatterplot match as closely as possible the distances between the same points in the original high dimensional dataset. In particular, t-SNE gives much more weight to preserving information about distances between points that are neighbors. More behind this algorithm. from sklearn.manifold import TSNE tsne = TSNE ( random_state = 0 ) X_tsne = tsne . fit_transform ( X_fruits_normalized ) plot_labelled_scatter ( X_tsne , y_fruits , [ 'apple' , 'mandarin' , 'orange' , 'lemon' ]) plt . xlabel ( 'First t-SNE feature' ) plt . ylabel ( 'Second t-SNE feature' ) plt . title ( 'Fruits dataset t-SNE' );","title":"t-SNE"},{"location":"model-unsupervised1/#lda","text":"Latent Dirichlet Allocation is another dimension reduction method, but unlike PCA, it is a supervised method. It attempts to find a feature subspace or decision boundary that maximizes class separability. It then projects the data points to new dimensions in a way that the clusters are as separate from each other as possible and the individual elements within a cluster are as close to the centroid of the cluster as possible. More from sebastianraschka.com and stackabuse.com . From sebastianraschka.com from sklearn.decomposition import LatentDirichletAllocation from sklearn.datasets import make_multilabel_classification # This produces a feature matrix of token counts, # similar to what CountVectorizer would produce on text. X , _ = make_multilabel_classification ( random_state = 0 ) lda = LatentDirichletAllocation ( n_components = 5 , random_state = 0 ) X_lda = lda . fit_transform ( X , y ) # check the explained variance percent = lda . explained_variance_ratio_ print ( percent ) print ( sum ( percent ))","title":"LDA"},{"location":"model-unsupervised2/","text":"Clustering Clustering involves finding related groups in the data and assigning every point in the dataset to one of the groups. To validate that the model used is good, a verification needs to be done by a person labelling the dataset, and seeing the percentage matched. # concat actual & predicted clusters together y = pd . DataFrame ( y . values , columns = [ 'actual' ]) cluster = pd . DataFrame ( kmeans . labels_ , columns = [ 'cluster' ]) df = pd . concat ([ y , cluster ], axis = 1 ) # view absolute numbers res = df . groupby ( 'actual' )[ 'cluster' ] . value_counts () print ( res ) # view percentages res2 = df . groupby ( 'actual' )[ 'cluster' ] . value_counts ( normalize = True ) * 100 print ( res2 ) K-Means Key Hyperparameter Desc n_clusters no. of clusters Specify number of clusters (3) 3 random data points are randomly selected as cluster centers Each data point is assigned to the cluster center it is cloest to Cluster centers are updated to the mean of the assigned points Steps 3-4 are repeated, till cluster centers remain unchanged Introduction to Machine Learning with Python (book) It is important to scale the features before applying K-means, unless the fields are not meant to be scaled, like distances. Categorical data is not appropriate as clustering calculated using euclidean distance (means). For long distances over an lat/long coordinates, they need to be projected to a flat surface. One aspect of k means is that different random starting points for the cluster centers often result in very different clustering solutions. So typically, the k-means algorithm is run in scikit-learn with ten different random initializations and the solution occurring the most number of times is chosen. Limitations Very sensitive to outliers. They have to be removed before fitting the model For simple globular shapes, not irregular complex clusters Might need to reduce dimensions if very high no. of features or the distance separation might not be obvious Two variants, K-medians & K-Medoids are less sensitive to outliers and work with categorical features. from sklearn.cluster import KMeans from sklearn.preprocessing import MinMaxScaler X_scaled = MinMaxScaler () . fit ( X ) . transform ( X ) kmeans = KMeans ( n_clusters = 4 , random_state = 0 ) kmeans . fit ( X ) To determine the best K, we will usually plot an elbow chart. Looking at the significant bend in the chart shows that the average distance value might be leveling off such that adding more clusters doesn't decrease the average distance as much. We will thus choose that K value. There is also the silhouette chart used to determine the number of k. from sklearn.cluster import KMeans from scipy.spatial.distance import cdist import numpy as np import matplotlib.pylab as plt import seaborn as sns % matplotlib inline meandist = [] clusters = range ( 1 , 10 ) for k in clusters : # prepare the model model = KMeans ( n_clusters = k ) # fit the model model . fit ( train_feature ) # test the model clusassign = model . predict ( train_feature ) # gives average distance values for each cluster solution # cdist calculates distance of each two points from centriod # get the min distance (where point is placed in clsuter) # get average distance by summing & dividing by total number of samples meandist . append ( sum ( np . min ( cdist ( train_feature , model . cluster_centers_ , 'euclidean' ), axis = 1 )) / train_feature . shape [ 0 ]) plt . plot ( clusters , meandist ) plt . xlabel ( 'Number of clusters' ) plt . ylabel ( 'Average distance' ) plt . title ( 'Selecting k with the Elbow Method' ) Sometimes we need to find the cluster centres so that we can get an absolute distance measure of centroids to new data. Each feature will have a defined centre for each cluster. # get cluster centres centroids = model.cluster_centers_ # for each row, define cluster centre centroid_labels = [centroids[i] for i in model.labels_] If we have labels or y, and want to determine which y belongs to which cluster for an evaluation score, we can use a groupby to find the most number of labels that fall in a cluster and manually label them as such. df = concat . groupby ([ 'label' , 'cluster' ])[ 'cluster' ] . count () If we want to know what is the distance of each datapoint\u2019s assign cluster distance to their centroid, we can do a fit_transform to get all distance from all cluster centroids and process from there. from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = n_clusters , random_state = 0 ) # get distance from each centroid for each datapoint dist_each_centroid = kmeans . fit_transform ( df ) # get all assigned centroids y = kmeans . labels_ # get distance of assigned centroid dist = [ distance [ label ] for label , distance in zip ( y , dist_each_centroid )] # concat label & distance together label_dist = pd . DataFrame ( zip ( y , dist ), columns = [ 'label' , 'distance' ]) Gaussian Mixture Model GMM is, in essence a density estimation model but can function like clustering. It has a probabilistic model under the hood so it returns a matrix of probabilities belonging to each cluster for each data point. More from here . Key Hyperparameters Desc n_components no. of clusters covariance_type diag (default, ellipse constrained to the axes), spherical (like k-means), or full (ellipse without a specific orientation) from sklearn.mixture import GaussianMixture # gmm accepts input as array, so have to convert dataframe to numpy input_gmm = normal . values gmm = GaussianMixture ( n_components = 4 , covariance_type = 'full' , random_state = 42 ) gmm . fit ( input_gmm ) result = gmm . predict ( test_set ) Python Data Science Handbook by Jake VanderPlas BIC or AIC are used to determine the optimal number of clusters using the elbow diagram, the former usually recommends a simpler model. Note that number of clusters or components measures how well GMM works as a density estimator, not as a clustering algorithm. from sklearn.mixture import GaussianMixture import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' input_gmm = normal . values bic_list = [] aic_list = [] ranges = range ( 1 , 30 ) for i in ranges : gmm = GaussianMixture ( n_components = i ) . fit ( input_gmm ) # BIC bic = gmm . bic ( input_gmm ) bic_list . append ( bic ) # AIC aic = gmm . aic ( input_gmm ) aic_list . append ( aic ) plt . figure ( figsize = ( 10 , 5 )) plt . plot ( ranges , bic_list , label = 'BIC' ); plt . plot ( ranges , aic_list , label = 'AIC' ); plt . legend ( loc = 'best' ); Python Data Science Handbook by Jake VanderPlas Agglommerative Clustering Agglomerative Clustering is a type of hierarchical clustering technique used to build clusters from bottom up. Divisive Clustering is the opposite method of building clusters from top down, which is not available in sklearn. The most useful part of such a clustering is the ability to draw a dendrogram to view the breakdown of clusters with a increasing distance. Python Data Science Handbook by Jake VanderPlas Key Hyperparameter Desc affinity methods of linking clusters (ward, average, complete) University of Michigan: Coursera Data Science in Python In essence, we can also use the 3-step method above to compute agglomerative clustering. First we conduct the agglomerative clustering, then display the dendrogram. After determining which distance it should be cut, we flatten the cluster by indicating the distance to slice the dendrogram. Python Data Science Handbook by Jake VanderPlas from scipy.cluster.hierarchy import linkage , dendrogram , fcluster # 1. clustering Z = linkage ( X , method = 'ward' , metric = 'euclidean' ) # 2. draw dendrogram plt . figure ( figsize = ( 10 , 5 )); dendrogram ( Z , orientation = 'left' , leaf_font_size = 8 ) plt . show () # 3. flatten cluster distance_threshold = 10 y = fcluster ( Z , distance_threshold , criterion = 'distance' ) sklearn's agglomerative cluster is said to be very slow. An alternative, fastcluster is faster as it is a C++ library with python interface. import fastcluster from scipy.cluster.hierarchy import dendrogram , fcluster # 1. clustering Z = fastcluster . linkage_vector ( X , method = 'ward' , metric = 'euclidean' ) Z_df = pd . DataFrame ( data = Z , columns = [ 'clusterOne' , 'clusterTwo' , 'distance' , 'newClusterSize' ]) # 2. draw dendrogram plt . figure ( figsize = ( 10 , 5 )) dendrogram ( Z , orientation = 'left' , leaf_font_size = 8 ) plt . show (); # 3. flatten cluster distance_threshold = 2000 clusters = fcluster ( Z , distance_threshold , criterion = 'distance' ) Raw results from clustering full dendrogram The dendrogram can be further enhanced by adding title and axis labels adding grids trimming the bottom branches based on max. no. of clusters to display labelling each cluster split distance a horizontal line to investigate where would be an appropriate cutoff point from scipy.cluster.hierarchy import linkage , dendrogram plt . style . use ( 'seaborn-whitegrid' ) plt . figure ( figsize = ( 8 , 5 )) plt . title ( 'Agglomerative Clustering Dendrogram' ) plt . xlabel ( 'Clusters' ) plt . ylabel ( 'Distance' ) # cluster Z = linkage ( df , method = 'ward' , metric = 'euclidean' ) # plot dendrogram ddata = dendrogram ( Z , orientation = 'top' , truncate_mode = 'lastp' , p = 5 , labels = True , get_leaves = True , show_leaf_counts = True , show_contracted = True ) # plot cluster points & distance labels limit = 4 for i , d , c in zip ( ddata [ 'icoord' ], ddata [ 'dcoord' ], ddata [ 'color_list' ]): x = sum ( i [ 1 : 3 ]) / 2 y = d [ 1 ] if y > limit : plt . plot ( x , y , 'o' , c = c , markeredgewidth = 0 ) plt . annotate ( int ( y ), ( x , y ), xytext = ( 0 , - 5 ), textcoords = 'offset points' , va = 'top' , ha = 'center' , fontsize = 9 ) # plot distance line = 1500 plt . axhline ( y = line , c = 'black' , linestyle = '--' ); The labels in brackets is the number of datapoints that are clustered under each branch. prettified dendrogram DBScan Density-Based Spatial Clustering of Applications with Noise (DBSCAN). Key Hyperparameters Desc eps epsilon. max distance btw 2 samples to be considered a cluster min_samples min. no. of samples to be considered a cluster Pick an arbitrary point to start Find all points with distance eps or less from that point If points are more than min_samples within distance of esp, point is 4.labelled as a core sample, and assigned a new cluster label Then all neighbours within eps of the point are visited If they are core samples their neighbours are visited in turn and so on The cluster thus grows till there are no more core samples within distance eps of the cluster Then, another point that has not been visited is picked, and step 1-6 is repeated 3 kinds of points are generated in the end, core points, boundary points, and noise Boundary points are core clusters but not within distance of esp Introduction to Machine Learning with Python (Book) from sklearn.cluster import DBSCAN from sklearn.datasets import make_blobs X , y = make_blobs ( random_state = 9 , n_samples = 20 ) dbscan = DBSCAN ( eps = 2 , min_samples = 2 ) cls = dbscan . fit_predict ( X ) print ( cls ) # [1 0 1 0 2 0 0 0 2 2 -1 1 2 0 0 -1 0 0 1 -1] # -1 indicates noise or outliers","title":"Clustering"},{"location":"model-unsupervised2/#clustering","text":"Clustering involves finding related groups in the data and assigning every point in the dataset to one of the groups. To validate that the model used is good, a verification needs to be done by a person labelling the dataset, and seeing the percentage matched. # concat actual & predicted clusters together y = pd . DataFrame ( y . values , columns = [ 'actual' ]) cluster = pd . DataFrame ( kmeans . labels_ , columns = [ 'cluster' ]) df = pd . concat ([ y , cluster ], axis = 1 ) # view absolute numbers res = df . groupby ( 'actual' )[ 'cluster' ] . value_counts () print ( res ) # view percentages res2 = df . groupby ( 'actual' )[ 'cluster' ] . value_counts ( normalize = True ) * 100 print ( res2 )","title":"Clustering"},{"location":"model-unsupervised2/#k-means","text":"Key Hyperparameter Desc n_clusters no. of clusters Specify number of clusters (3) 3 random data points are randomly selected as cluster centers Each data point is assigned to the cluster center it is cloest to Cluster centers are updated to the mean of the assigned points Steps 3-4 are repeated, till cluster centers remain unchanged Introduction to Machine Learning with Python (book) It is important to scale the features before applying K-means, unless the fields are not meant to be scaled, like distances. Categorical data is not appropriate as clustering calculated using euclidean distance (means). For long distances over an lat/long coordinates, they need to be projected to a flat surface. One aspect of k means is that different random starting points for the cluster centers often result in very different clustering solutions. So typically, the k-means algorithm is run in scikit-learn with ten different random initializations and the solution occurring the most number of times is chosen. Limitations Very sensitive to outliers. They have to be removed before fitting the model For simple globular shapes, not irregular complex clusters Might need to reduce dimensions if very high no. of features or the distance separation might not be obvious Two variants, K-medians & K-Medoids are less sensitive to outliers and work with categorical features. from sklearn.cluster import KMeans from sklearn.preprocessing import MinMaxScaler X_scaled = MinMaxScaler () . fit ( X ) . transform ( X ) kmeans = KMeans ( n_clusters = 4 , random_state = 0 ) kmeans . fit ( X ) To determine the best K, we will usually plot an elbow chart. Looking at the significant bend in the chart shows that the average distance value might be leveling off such that adding more clusters doesn't decrease the average distance as much. We will thus choose that K value. There is also the silhouette chart used to determine the number of k. from sklearn.cluster import KMeans from scipy.spatial.distance import cdist import numpy as np import matplotlib.pylab as plt import seaborn as sns % matplotlib inline meandist = [] clusters = range ( 1 , 10 ) for k in clusters : # prepare the model model = KMeans ( n_clusters = k ) # fit the model model . fit ( train_feature ) # test the model clusassign = model . predict ( train_feature ) # gives average distance values for each cluster solution # cdist calculates distance of each two points from centriod # get the min distance (where point is placed in clsuter) # get average distance by summing & dividing by total number of samples meandist . append ( sum ( np . min ( cdist ( train_feature , model . cluster_centers_ , 'euclidean' ), axis = 1 )) / train_feature . shape [ 0 ]) plt . plot ( clusters , meandist ) plt . xlabel ( 'Number of clusters' ) plt . ylabel ( 'Average distance' ) plt . title ( 'Selecting k with the Elbow Method' ) Sometimes we need to find the cluster centres so that we can get an absolute distance measure of centroids to new data. Each feature will have a defined centre for each cluster. # get cluster centres centroids = model.cluster_centers_ # for each row, define cluster centre centroid_labels = [centroids[i] for i in model.labels_] If we have labels or y, and want to determine which y belongs to which cluster for an evaluation score, we can use a groupby to find the most number of labels that fall in a cluster and manually label them as such. df = concat . groupby ([ 'label' , 'cluster' ])[ 'cluster' ] . count () If we want to know what is the distance of each datapoint\u2019s assign cluster distance to their centroid, we can do a fit_transform to get all distance from all cluster centroids and process from there. from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = n_clusters , random_state = 0 ) # get distance from each centroid for each datapoint dist_each_centroid = kmeans . fit_transform ( df ) # get all assigned centroids y = kmeans . labels_ # get distance of assigned centroid dist = [ distance [ label ] for label , distance in zip ( y , dist_each_centroid )] # concat label & distance together label_dist = pd . DataFrame ( zip ( y , dist ), columns = [ 'label' , 'distance' ])","title":"K-Means"},{"location":"model-unsupervised2/#gaussian-mixture-model","text":"GMM is, in essence a density estimation model but can function like clustering. It has a probabilistic model under the hood so it returns a matrix of probabilities belonging to each cluster for each data point. More from here . Key Hyperparameters Desc n_components no. of clusters covariance_type diag (default, ellipse constrained to the axes), spherical (like k-means), or full (ellipse without a specific orientation) from sklearn.mixture import GaussianMixture # gmm accepts input as array, so have to convert dataframe to numpy input_gmm = normal . values gmm = GaussianMixture ( n_components = 4 , covariance_type = 'full' , random_state = 42 ) gmm . fit ( input_gmm ) result = gmm . predict ( test_set ) Python Data Science Handbook by Jake VanderPlas BIC or AIC are used to determine the optimal number of clusters using the elbow diagram, the former usually recommends a simpler model. Note that number of clusters or components measures how well GMM works as a density estimator, not as a clustering algorithm. from sklearn.mixture import GaussianMixture import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' input_gmm = normal . values bic_list = [] aic_list = [] ranges = range ( 1 , 30 ) for i in ranges : gmm = GaussianMixture ( n_components = i ) . fit ( input_gmm ) # BIC bic = gmm . bic ( input_gmm ) bic_list . append ( bic ) # AIC aic = gmm . aic ( input_gmm ) aic_list . append ( aic ) plt . figure ( figsize = ( 10 , 5 )) plt . plot ( ranges , bic_list , label = 'BIC' ); plt . plot ( ranges , aic_list , label = 'AIC' ); plt . legend ( loc = 'best' ); Python Data Science Handbook by Jake VanderPlas","title":"Gaussian Mixture Model"},{"location":"model-unsupervised2/#agglommerative-clustering","text":"Agglomerative Clustering is a type of hierarchical clustering technique used to build clusters from bottom up. Divisive Clustering is the opposite method of building clusters from top down, which is not available in sklearn. The most useful part of such a clustering is the ability to draw a dendrogram to view the breakdown of clusters with a increasing distance. Python Data Science Handbook by Jake VanderPlas Key Hyperparameter Desc affinity methods of linking clusters (ward, average, complete) University of Michigan: Coursera Data Science in Python In essence, we can also use the 3-step method above to compute agglomerative clustering. First we conduct the agglomerative clustering, then display the dendrogram. After determining which distance it should be cut, we flatten the cluster by indicating the distance to slice the dendrogram. Python Data Science Handbook by Jake VanderPlas from scipy.cluster.hierarchy import linkage , dendrogram , fcluster # 1. clustering Z = linkage ( X , method = 'ward' , metric = 'euclidean' ) # 2. draw dendrogram plt . figure ( figsize = ( 10 , 5 )); dendrogram ( Z , orientation = 'left' , leaf_font_size = 8 ) plt . show () # 3. flatten cluster distance_threshold = 10 y = fcluster ( Z , distance_threshold , criterion = 'distance' ) sklearn's agglomerative cluster is said to be very slow. An alternative, fastcluster is faster as it is a C++ library with python interface. import fastcluster from scipy.cluster.hierarchy import dendrogram , fcluster # 1. clustering Z = fastcluster . linkage_vector ( X , method = 'ward' , metric = 'euclidean' ) Z_df = pd . DataFrame ( data = Z , columns = [ 'clusterOne' , 'clusterTwo' , 'distance' , 'newClusterSize' ]) # 2. draw dendrogram plt . figure ( figsize = ( 10 , 5 )) dendrogram ( Z , orientation = 'left' , leaf_font_size = 8 ) plt . show (); # 3. flatten cluster distance_threshold = 2000 clusters = fcluster ( Z , distance_threshold , criterion = 'distance' ) Raw results from clustering full dendrogram The dendrogram can be further enhanced by adding title and axis labels adding grids trimming the bottom branches based on max. no. of clusters to display labelling each cluster split distance a horizontal line to investigate where would be an appropriate cutoff point from scipy.cluster.hierarchy import linkage , dendrogram plt . style . use ( 'seaborn-whitegrid' ) plt . figure ( figsize = ( 8 , 5 )) plt . title ( 'Agglomerative Clustering Dendrogram' ) plt . xlabel ( 'Clusters' ) plt . ylabel ( 'Distance' ) # cluster Z = linkage ( df , method = 'ward' , metric = 'euclidean' ) # plot dendrogram ddata = dendrogram ( Z , orientation = 'top' , truncate_mode = 'lastp' , p = 5 , labels = True , get_leaves = True , show_leaf_counts = True , show_contracted = True ) # plot cluster points & distance labels limit = 4 for i , d , c in zip ( ddata [ 'icoord' ], ddata [ 'dcoord' ], ddata [ 'color_list' ]): x = sum ( i [ 1 : 3 ]) / 2 y = d [ 1 ] if y > limit : plt . plot ( x , y , 'o' , c = c , markeredgewidth = 0 ) plt . annotate ( int ( y ), ( x , y ), xytext = ( 0 , - 5 ), textcoords = 'offset points' , va = 'top' , ha = 'center' , fontsize = 9 ) # plot distance line = 1500 plt . axhline ( y = line , c = 'black' , linestyle = '--' ); The labels in brackets is the number of datapoints that are clustered under each branch. prettified dendrogram","title":"Agglommerative Clustering"},{"location":"model-unsupervised2/#dbscan","text":"Density-Based Spatial Clustering of Applications with Noise (DBSCAN). Key Hyperparameters Desc eps epsilon. max distance btw 2 samples to be considered a cluster min_samples min. no. of samples to be considered a cluster Pick an arbitrary point to start Find all points with distance eps or less from that point If points are more than min_samples within distance of esp, point is 4.labelled as a core sample, and assigned a new cluster label Then all neighbours within eps of the point are visited If they are core samples their neighbours are visited in turn and so on The cluster thus grows till there are no more core samples within distance eps of the cluster Then, another point that has not been visited is picked, and step 1-6 is repeated 3 kinds of points are generated in the end, core points, boundary points, and noise Boundary points are core clusters but not within distance of esp Introduction to Machine Learning with Python (Book) from sklearn.cluster import DBSCAN from sklearn.datasets import make_blobs X , y = make_blobs ( random_state = 9 , n_samples = 20 ) dbscan = DBSCAN ( eps = 2 , min_samples = 2 ) cls = dbscan . fit_predict ( X ) print ( cls ) # [1 0 1 0 2 0 0 0 2 2 -1 1 2 0 0 -1 0 0 1 -1] # -1 indicates noise or outliers","title":"DBScan"},{"location":"model-unsupervised3/","text":"One-Class Classification These requires the training of a normal state(s), allows outliers to be detected when they lie outside trained state. A common use for this is to detect anomalies. One Class SVM One-class SVM is an unsupervised algorithm that learns a decision function for outlier detection: classifying new data as similar or different to the training set. Besides the kernel, two other parameters are impt: nu : proportion of outliers you expect to observe gamma : determines the smoothing of the contour lines. from sklearn.svm import OneClassSVM train , test = train_test_split ( data , test_size =. 2 ) train_normal = train [ train [ 'y' ] == 0 ] train_outliers = train [ train [ 'y' ] == 1 ] outlier_prop = len ( train_outliers ) / len ( train_normal ) model = OneClassSVM ( kernel = 'rbf' , nu = outlier_prop , gamma = 0.000001 ) svm . fit ( train_normal [[ 'col1' , 'col2' , 'col3' ]]) Isolation Forest One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. Isolation Forest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. from sklearn.ensemble import IsolationForest clf = IsolationForest ( behaviour = 'new' , max_samples = 100 , random_state = rng , contamination = 'auto' ) clf . fit ( X_train ) y_pred_test = clf . predict ( X_test ) # -1 are outliers y_pred_test # array([ 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1]) # calculate the no. of anomalies pd . DataFrame ( save )[ 0 ] . value_counts () # -1 23330 # 1 687 # Name: 0, dtype: int64 We can also get the average anomaly scores. The lower the value, the more abnormal they are. Negative scores represent outliers, positive scores represent inliers. clf . decision_function ( X_test ) array ([ 0.14528263 , 0.14528263 , - 0.08450298 , 0.14528263 , 0.14528263 , 0.14528263 , 0.14528263 , 0.14528263 , 0.14528263 , - 0.14279962 , 0.14528263 , 0.14528263 , - 0.05483886 , - 0.10086102 , 0.14528263 , 0.14528263 ]) From sklearn","title":"One-Class"},{"location":"model-unsupervised3/#one-class-classification","text":"These requires the training of a normal state(s), allows outliers to be detected when they lie outside trained state. A common use for this is to detect anomalies.","title":"One-Class Classification"},{"location":"model-unsupervised3/#one-class-svm","text":"One-class SVM is an unsupervised algorithm that learns a decision function for outlier detection: classifying new data as similar or different to the training set. Besides the kernel, two other parameters are impt: nu : proportion of outliers you expect to observe gamma : determines the smoothing of the contour lines. from sklearn.svm import OneClassSVM train , test = train_test_split ( data , test_size =. 2 ) train_normal = train [ train [ 'y' ] == 0 ] train_outliers = train [ train [ 'y' ] == 1 ] outlier_prop = len ( train_outliers ) / len ( train_normal ) model = OneClassSVM ( kernel = 'rbf' , nu = outlier_prop , gamma = 0.000001 ) svm . fit ( train_normal [[ 'col1' , 'col2' , 'col3' ]])","title":"One Class SVM"},{"location":"model-unsupervised3/#isolation-forest","text":"One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. Isolation Forest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. from sklearn.ensemble import IsolationForest clf = IsolationForest ( behaviour = 'new' , max_samples = 100 , random_state = rng , contamination = 'auto' ) clf . fit ( X_train ) y_pred_test = clf . predict ( X_test ) # -1 are outliers y_pred_test # array([ 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1]) # calculate the no. of anomalies pd . DataFrame ( save )[ 0 ] . value_counts () # -1 23330 # 1 687 # Name: 0, dtype: int64 We can also get the average anomaly scores. The lower the value, the more abnormal they are. Negative scores represent outliers, positive scores represent inliers. clf . decision_function ( X_test ) array ([ 0.14528263 , 0.14528263 , - 0.08450298 , 0.14528263 , 0.14528263 , 0.14528263 , 0.14528263 , 0.14528263 , 0.14528263 , - 0.14279962 , 0.14528263 , 0.14528263 , - 0.05483886 , - 0.10086102 , 0.14528263 , 0.14528263 ]) From sklearn","title":"Isolation Forest"},{"location":"model-unsupervised4/","text":"Distance Metrics Euclidean & Cosine Euclidean distance is the straight line distance between points, while Cosine Similarity is the angle between these two points. from scipy.spatial.distance import euclidean , cosine euclidean ([ 1 , 2 ],[ 1 , 3 ]) # 1 cosine ([ 1 , 2 ],[ 1 , 3 ]) # 0.010050506338833642 Mahalanobis Distance Mahalonobis distance is the distance between a point and a distribution, not between two distinct points. Therefore, it is effectively a multivariate equivalent of the Euclidean distance. More here . Type Desc x vector of the observation (row in a dataset) m vector of mean values of independent variables (mean of each column) C^(-1) inverse covariance matrix of independent variables Multiplying by the inverse covariance (correlation) matrix essentially means dividing the input with the matrix. This is so that if features in your dataset are strongly correlated, the covariance will be high. Dividing by a large covariance will effectively reduce the distance. While powerful, its use of correlation can be detrimantal when there is multicollinearity (strong correlations among features). import pandas as pd import numpy as np from scipy.spatial.distance import mahalanobis def mahalanobisD ( normal_df , y_df ): # calculate inverse covariance from normal state x_cov = normal_df . cov () inv_cov = np . linalg . pinv ( x_cov ) # get mean of normal state df x_mean = normal_df . mean () # calculate mahalanobis distance from each row of y_df distanceMD = [] for i in range ( len ( y_df )): MD = mahalanobis ( x_mean , y_df . iloc [ i ], inv_cov ) distanceMD . append ( MD ) return distanceMD Dynamic Time Warping If two time series are identical, but one is shifted slightly along the time axis, then Euclidean distance may consider them to be very different from each other. DTW was introduced to overcome this limitation and give intuitive distance measurements between time series by ignoring both global and local shifts in the time dimension. DTW is a technique that finds the optimal alignment between two time series, if one time series may be \u201cwarped\u201d non-linearly by stretching or shrinking it along its time axis. Dynamic time warping is often used in speech recognition to determine if two waveforms represent the same spoken phrase. In a speech waveform, the duration of each spoken sound and the interval between sounds are permitted to vary, but the overall speech waveforms must be similar. There is a faster implementation of DTW called Fast DTW . The result is DTW is such that if two time-series are identical, the DTW distance is 0, else any difference will be more than 0. import numpy as np from scipy.spatial.distance import euclidean from fastdtw import fastdtw x = np . array ([[ 1 , 1 ], [ 2 , 2 ], [ 3 , 3 ], [ 4 , 4 ], [ 5 , 5 ]]) y = np . array ([[ 2 , 2 ], [ 3 , 3 ], [ 4 , 4 ]]) distance , path = fastdtw ( x , y , dist = euclidean ) print ( distance ) # 2.8284271247461903 Symbolic Aggregate approXimation SAX, developed in 2007, compares the similarity of two time-series patterns by slicing them into horizontal & vertical regions, and comparing between each of them. This can be easily explained by 4 charts shown below. There are obvious benefits using such an algorithm, for one, it will be very fast as pattern matching is aggregated. However, the biggest downside is that both time-series signals have to be of same time-length. Both signals are overlayed Then normalised. The chart is then sliced by various timeframes, Piecewise Aggregate Approximation, and each slice is compared between the two signals independently. Each signal value, i.e., y-axis is then sliced horizontally into regions, and assigned an alphabet. Lastly, we use a distance scoring metric, through a fixed lookup table to easily calculate the total scores between each pair of PAA. E.g., if the PAA fall in a region or its immediate adjacent one, we assume they are the same, i.e., distance = 0. Else, a distance value is assigned. The total distance is then computed to derice a distance metric. For this instance: SAX transform of ts1 into string through 9-points PAA: \u201cabddccbaa\u201d SAX transform of ts2 into string through 9-points PAA: \u201cabbccddba\u201d SAX distance: 0 + 0 + 0.67 + 0 + 0 + 0 + 0.67 + 0 + 0 = 1.34 This is the code from the package saxpy. Unfortunately, it does not have the option of calculating of the sax distance. import numpy as np from saxpy.znorm import znorm from saxpy.paa import paa from saxpy.sax import ts_to_string from saxpy.alphabet import cuts_for_asize def saxpy_sax ( signal , paa_segments = 3 , alphabet_size = 3 ): sig_znorm = znorm ( signal ) sig_paa = paa ( sig_znorm , paa_segments ) sax = ts_to_string ( sig_paa , cuts_for_asize ( alphabet_size )) return sax sig1a = saxpy_sax ( sig1 ) sig2a = saxpy_sax ( sig2 ) Another more mature package is tslearn. It enables the calculation of sax distance, but the sax alphabets are set as integers instead. from tslearn.piecewise import SymbolicAggregateApproximation def tslearn_sax ( sig1 , sig2 , n_segments , alphabet_size ): # Z-transform, PAA & SAX transformation sax = SymbolicAggregateApproximation ( n_segments = n_segments , alphabet_size_avg = alphabet_size ) sax_data = sax . fit_transform ([ sig1_n , sig2_n ]) # distance measure distance = sax . distance_sax ( sax_data [ 0 ], sax_data [ 1 ]) return sax_data , distance # [[[0] # [3] # [3] # [1]] # [[0] # [1] # [2] # [3]]] # 1.8471662549420924","title":"Distance"},{"location":"model-unsupervised4/#distance-metrics","text":"","title":"Distance Metrics"},{"location":"model-unsupervised4/#euclidean-cosine","text":"Euclidean distance is the straight line distance between points, while Cosine Similarity is the angle between these two points. from scipy.spatial.distance import euclidean , cosine euclidean ([ 1 , 2 ],[ 1 , 3 ]) # 1 cosine ([ 1 , 2 ],[ 1 , 3 ]) # 0.010050506338833642","title":"Euclidean &amp; Cosine"},{"location":"model-unsupervised4/#mahalanobis-distance","text":"Mahalonobis distance is the distance between a point and a distribution, not between two distinct points. Therefore, it is effectively a multivariate equivalent of the Euclidean distance. More here . Type Desc x vector of the observation (row in a dataset) m vector of mean values of independent variables (mean of each column) C^(-1) inverse covariance matrix of independent variables Multiplying by the inverse covariance (correlation) matrix essentially means dividing the input with the matrix. This is so that if features in your dataset are strongly correlated, the covariance will be high. Dividing by a large covariance will effectively reduce the distance. While powerful, its use of correlation can be detrimantal when there is multicollinearity (strong correlations among features). import pandas as pd import numpy as np from scipy.spatial.distance import mahalanobis def mahalanobisD ( normal_df , y_df ): # calculate inverse covariance from normal state x_cov = normal_df . cov () inv_cov = np . linalg . pinv ( x_cov ) # get mean of normal state df x_mean = normal_df . mean () # calculate mahalanobis distance from each row of y_df distanceMD = [] for i in range ( len ( y_df )): MD = mahalanobis ( x_mean , y_df . iloc [ i ], inv_cov ) distanceMD . append ( MD ) return distanceMD","title":"Mahalanobis Distance"},{"location":"model-unsupervised4/#dynamic-time-warping","text":"If two time series are identical, but one is shifted slightly along the time axis, then Euclidean distance may consider them to be very different from each other. DTW was introduced to overcome this limitation and give intuitive distance measurements between time series by ignoring both global and local shifts in the time dimension. DTW is a technique that finds the optimal alignment between two time series, if one time series may be \u201cwarped\u201d non-linearly by stretching or shrinking it along its time axis. Dynamic time warping is often used in speech recognition to determine if two waveforms represent the same spoken phrase. In a speech waveform, the duration of each spoken sound and the interval between sounds are permitted to vary, but the overall speech waveforms must be similar. There is a faster implementation of DTW called Fast DTW . The result is DTW is such that if two time-series are identical, the DTW distance is 0, else any difference will be more than 0. import numpy as np from scipy.spatial.distance import euclidean from fastdtw import fastdtw x = np . array ([[ 1 , 1 ], [ 2 , 2 ], [ 3 , 3 ], [ 4 , 4 ], [ 5 , 5 ]]) y = np . array ([[ 2 , 2 ], [ 3 , 3 ], [ 4 , 4 ]]) distance , path = fastdtw ( x , y , dist = euclidean ) print ( distance ) # 2.8284271247461903","title":"Dynamic Time Warping"},{"location":"model-unsupervised4/#symbolic-aggregate-approximation","text":"SAX, developed in 2007, compares the similarity of two time-series patterns by slicing them into horizontal & vertical regions, and comparing between each of them. This can be easily explained by 4 charts shown below. There are obvious benefits using such an algorithm, for one, it will be very fast as pattern matching is aggregated. However, the biggest downside is that both time-series signals have to be of same time-length. Both signals are overlayed Then normalised. The chart is then sliced by various timeframes, Piecewise Aggregate Approximation, and each slice is compared between the two signals independently. Each signal value, i.e., y-axis is then sliced horizontally into regions, and assigned an alphabet. Lastly, we use a distance scoring metric, through a fixed lookup table to easily calculate the total scores between each pair of PAA. E.g., if the PAA fall in a region or its immediate adjacent one, we assume they are the same, i.e., distance = 0. Else, a distance value is assigned. The total distance is then computed to derice a distance metric. For this instance: SAX transform of ts1 into string through 9-points PAA: \u201cabddccbaa\u201d SAX transform of ts2 into string through 9-points PAA: \u201cabbccddba\u201d SAX distance: 0 + 0 + 0.67 + 0 + 0 + 0 + 0.67 + 0 + 0 = 1.34 This is the code from the package saxpy. Unfortunately, it does not have the option of calculating of the sax distance. import numpy as np from saxpy.znorm import znorm from saxpy.paa import paa from saxpy.sax import ts_to_string from saxpy.alphabet import cuts_for_asize def saxpy_sax ( signal , paa_segments = 3 , alphabet_size = 3 ): sig_znorm = znorm ( signal ) sig_paa = paa ( sig_znorm , paa_segments ) sax = ts_to_string ( sig_paa , cuts_for_asize ( alphabet_size )) return sax sig1a = saxpy_sax ( sig1 ) sig2a = saxpy_sax ( sig2 ) Another more mature package is tslearn. It enables the calculation of sax distance, but the sax alphabets are set as integers instead. from tslearn.piecewise import SymbolicAggregateApproximation def tslearn_sax ( sig1 , sig2 , n_segments , alphabet_size ): # Z-transform, PAA & SAX transformation sax = SymbolicAggregateApproximation ( n_segments = n_segments , alphabet_size_avg = alphabet_size ) sax_data = sax . fit_transform ([ sig1_n , sig2_n ]) # distance measure distance = sax . distance_sax ( sax_data [ 0 ], sax_data [ 1 ]) return sax_data , distance # [[[0] # [3] # [3] # [1]] # [[0] # [1] # [2] # [3]]] # 1.8471662549420924","title":"Symbolic Aggregate approXimation"},{"location":"persistence/","text":"Persistance Data, models and scalers are examples of objects that can benefit greatly from storing in their native python format. For the former, it allows multiples faster loading compared to other sources since it is saved in a python format. For others, there are no other ways of saving as they are natively python objects. There are two ways of doing this. Pickle We can save data loaded into a dataframe like below. import pandas as pd df . to_pickle ( 'df.pkl' ) df = pd . read_pickle ( 'df.pkl' ) For models or scalars we need to import pickle directly. Note that the extension name can be anything. import pickle pickle . dump ( model , open ( 'model_rf.pkl' , 'wb' )) pickle . load ( open ( 'model_rf.pkl' , 'rb' )) Joblib When using scikit-learn, it may be better to use joblib\u2019s replacement of pickle (dump & load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string. More here . However, note that for small files (>4GB), pickle might be faster. Dicussion in stackoverflow . import joblib joblib . dump ( clf , 'model.joblib' ) joblib . load ( 'model.joblib' )","title":"Model Persistence"},{"location":"persistence/#persistance","text":"Data, models and scalers are examples of objects that can benefit greatly from storing in their native python format. For the former, it allows multiples faster loading compared to other sources since it is saved in a python format. For others, there are no other ways of saving as they are natively python objects. There are two ways of doing this.","title":"Persistance"},{"location":"persistence/#pickle","text":"We can save data loaded into a dataframe like below. import pandas as pd df . to_pickle ( 'df.pkl' ) df = pd . read_pickle ( 'df.pkl' ) For models or scalars we need to import pickle directly. Note that the extension name can be anything. import pickle pickle . dump ( model , open ( 'model_rf.pkl' , 'wb' )) pickle . load ( open ( 'model_rf.pkl' , 'rb' ))","title":"Pickle"},{"location":"persistence/#joblib","text":"When using scikit-learn, it may be better to use joblib\u2019s replacement of pickle (dump & load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string. More here . However, note that for small files (>4GB), pickle might be faster. Dicussion in stackoverflow . import joblib joblib . dump ( clf , 'model.joblib' ) joblib . load ( 'model.joblib' )","title":"Joblib"},{"location":"process/","text":"Machine Learning Process Lifecycle The machine learning life cycle is a lengthy one, involving not just the building models, but also testing, deployment and monitoring. However, those will be covered in another of my website . Machine Learning Life Cycle Modelling Process The key focus of this site is to zoom in to both starting of the ML process, which involves the problem scoping, data aquisition and model training. The first two cannot be emphasize enough as they will make or break your desired model outcome, as I have come to realised after some failed attempts. These are also the most time consuming, and rightly so. Imagine obtaining a poor model result just because you do not have domain understanding of which features are important for the predictors. Features & Labels In a supervised model, a dataset usually consists of two components, the features and the labels . These two have many names in both the ML and statistical worlds, so its worth it to list them out here. Names Synomyns Features X, independent variable, predictor, explanatory, input Labels y, dependent variable, target, response, output Data Splits Both the train & validation datasets are used for model training and evaluation. However, as we adjust the model hyperparameters to get the best evaluation metrics for both train & val sets, we can be overfitting to just these two data splits. To overcome this problem, it is usual to have another split for a test set , with is also known as the unseen dataset, as the model trained have not seen it before. This will further confirm that your model is well generalized. Introduction to Machine Learning with Python (book) from sklearn.cross_validation import train_test_split # split into train+val & test sets X_trainval , X_test , y_trainval , y_test = \\ train_test_split ( iris . data , iris . target , random_state = 0 ) # split train+val set into train and val set X_train , X_valid , y_train , y_valid = \\ train_test_split ( X_trainval , y_trainval , random_state = 1 ) A more advanced form of data split is k-fold cross validation, which is further elaborated later . We can see the entire process diagrammatically as shown below. Introduction to Machine Learning with Python (book) A good workflow of the model training process with the inclusion of data splits is shown below. from sklearn","title":"ML Process"},{"location":"process/#machine-learning-process","text":"","title":"Machine Learning Process"},{"location":"process/#lifecycle","text":"The machine learning life cycle is a lengthy one, involving not just the building models, but also testing, deployment and monitoring. However, those will be covered in another of my website . Machine Learning Life Cycle","title":"Lifecycle"},{"location":"process/#modelling-process","text":"The key focus of this site is to zoom in to both starting of the ML process, which involves the problem scoping, data aquisition and model training. The first two cannot be emphasize enough as they will make or break your desired model outcome, as I have come to realised after some failed attempts. These are also the most time consuming, and rightly so. Imagine obtaining a poor model result just because you do not have domain understanding of which features are important for the predictors.","title":"Modelling Process"},{"location":"process/#features-labels","text":"In a supervised model, a dataset usually consists of two components, the features and the labels . These two have many names in both the ML and statistical worlds, so its worth it to list them out here. Names Synomyns Features X, independent variable, predictor, explanatory, input Labels y, dependent variable, target, response, output","title":"Features &amp; Labels"},{"location":"process/#data-splits","text":"Both the train & validation datasets are used for model training and evaluation. However, as we adjust the model hyperparameters to get the best evaluation metrics for both train & val sets, we can be overfitting to just these two data splits. To overcome this problem, it is usual to have another split for a test set , with is also known as the unseen dataset, as the model trained have not seen it before. This will further confirm that your model is well generalized. Introduction to Machine Learning with Python (book) from sklearn.cross_validation import train_test_split # split into train+val & test sets X_trainval , X_test , y_trainval , y_test = \\ train_test_split ( iris . data , iris . target , random_state = 0 ) # split train+val set into train and val set X_train , X_valid , y_train , y_valid = \\ train_test_split ( X_trainval , y_trainval , random_state = 1 ) A more advanced form of data split is k-fold cross validation, which is further elaborated later . We can see the entire process diagrammatically as shown below. Introduction to Machine Learning with Python (book) A good workflow of the model training process with the inclusion of data splits is shown below. from sklearn","title":"Data Splits"},{"location":"rre-association/","text":"Association Rule Learning An informal definition of association rule learning is \"Customer who bought this will also buy...\". This is used for optimization of a combination of things. It is also commonly known as Market Basket Analysis . Knowing this association allows one to make recommendations to a customer. Two popular algorithms, Apriori & FPGrowth are available in the library mlxtend . Apriori Apriori association consists of three important parameters, Support, Confidence and Lift. This kaggle article explains them perfectly. Given the transactional data below: Transaction Items T1 apple, egg, milk T2 carrot, milk T3 apple, egg, carrot T4 apple, egg T5 apple, carrot Support A parameter to measure the popularity of certain item. It is a proportion of transactions in which a specific item appears. Support threshold is a key parameters in product association algos, and ranges from 0 to 1. support{apple,egg} = 3/5 or 60% Confidence A parameter to measure how likely item B will be purchased given item A is purchased. This is expressed as confidence{A->B} = support{A,B}/support{A} , and ranges from 0 to 1. confidence{apple->egg} = support{apple,egg} / support{apple} = (3/5) / (4/5) = 0.75 or 75% However, if we look at the scores in the opposite direction. confidence{egg->apple} = support{apple,egg} / support{egg} = (3/5) / (3/5) = 1 or 100% One of the drawbacks of Confidence is that it might misrepresent the importance of an association. Lift A parameter to measure how likely item B will be purchased when item A is purchased, while controlling for how popular item B is. This accounts for the popularity of both items rather than just one in confidence calculation. Unlike the confidence metric whose value may vary depending on direction (eg: confidence{A->B} may be different from confidence{B->A}), lift has no direction. This means that the lift{A,B} is always equal to the lift{B,A}. Hence, The formula for this is lift{A,B} = lift{B,A} = support{A,B} / (support{A} * support{B}) , and it can range from 0 to infinity lift{apple,egg} = lift{egg,apple} = support{apple,egg} / (support{apple} * support{egg}) = (3/5) / (4/5 * 3/5) = 1.25 The values of lift can be summarised as follows. lift explanation example 1 no relationship between A and B A and B occur together only by chance >1 positive relationship between A and B A and B occur together more often than random <1 negative relationship between A and B A and B occur together less often than random from apyori import apriori import pandas as pd import numpy as np df = pd . read_csv ( '../input/Market_Basket_Optimisation.csv' , header = None ) #Transforming the list into a list of lists # so that each transaction can be indexed easier transactions = [] for i in range ( 0 , df . shape [ 0 ]): transactions . append ([ str ( dataset . values [ i , j ]) for j in range ( 0 , 20 )]) rules = apriori ( transactions , min_support = 0.003 , min_confidence = 0.2 , min_lift = 3 , min_length = 2 ) results = list ( rules ) results = pd . DataFrame ( results ) results . head ( 5 ) To analyse the results we can plot the following graphs. FP Growth Frequent Pattern (FP) Growth is preferred to Apriori for the reason that Apriori takes more execution time for repeated scanning of the transaction dataset to mine the frequent items. FP-Growth builds a compact-tree structure and uses the tree for frequent itemset mining and generating rules, using a divide and conquer approach. Eclat Eclat is a simplified version of Apriori model, as only Support value is used, which shows how frequent a set of items occur.","title":"Association"},{"location":"rre-association/#association-rule-learning","text":"An informal definition of association rule learning is \"Customer who bought this will also buy...\". This is used for optimization of a combination of things. It is also commonly known as Market Basket Analysis . Knowing this association allows one to make recommendations to a customer. Two popular algorithms, Apriori & FPGrowth are available in the library mlxtend .","title":"Association Rule Learning"},{"location":"rre-association/#apriori","text":"Apriori association consists of three important parameters, Support, Confidence and Lift. This kaggle article explains them perfectly. Given the transactional data below: Transaction Items T1 apple, egg, milk T2 carrot, milk T3 apple, egg, carrot T4 apple, egg T5 apple, carrot Support A parameter to measure the popularity of certain item. It is a proportion of transactions in which a specific item appears. Support threshold is a key parameters in product association algos, and ranges from 0 to 1. support{apple,egg} = 3/5 or 60% Confidence A parameter to measure how likely item B will be purchased given item A is purchased. This is expressed as confidence{A->B} = support{A,B}/support{A} , and ranges from 0 to 1. confidence{apple->egg} = support{apple,egg} / support{apple} = (3/5) / (4/5) = 0.75 or 75% However, if we look at the scores in the opposite direction. confidence{egg->apple} = support{apple,egg} / support{egg} = (3/5) / (3/5) = 1 or 100% One of the drawbacks of Confidence is that it might misrepresent the importance of an association. Lift A parameter to measure how likely item B will be purchased when item A is purchased, while controlling for how popular item B is. This accounts for the popularity of both items rather than just one in confidence calculation. Unlike the confidence metric whose value may vary depending on direction (eg: confidence{A->B} may be different from confidence{B->A}), lift has no direction. This means that the lift{A,B} is always equal to the lift{B,A}. Hence, The formula for this is lift{A,B} = lift{B,A} = support{A,B} / (support{A} * support{B}) , and it can range from 0 to infinity lift{apple,egg} = lift{egg,apple} = support{apple,egg} / (support{apple} * support{egg}) = (3/5) / (4/5 * 3/5) = 1.25 The values of lift can be summarised as follows. lift explanation example 1 no relationship between A and B A and B occur together only by chance >1 positive relationship between A and B A and B occur together more often than random <1 negative relationship between A and B A and B occur together less often than random from apyori import apriori import pandas as pd import numpy as np df = pd . read_csv ( '../input/Market_Basket_Optimisation.csv' , header = None ) #Transforming the list into a list of lists # so that each transaction can be indexed easier transactions = [] for i in range ( 0 , df . shape [ 0 ]): transactions . append ([ str ( dataset . values [ i , j ]) for j in range ( 0 , 20 )]) rules = apriori ( transactions , min_support = 0.003 , min_confidence = 0.2 , min_lift = 3 , min_length = 2 ) results = list ( rules ) results = pd . DataFrame ( results ) results . head ( 5 ) To analyse the results we can plot the following graphs.","title":"Apriori"},{"location":"rre-association/#fp-growth","text":"Frequent Pattern (FP) Growth is preferred to Apriori for the reason that Apriori takes more execution time for repeated scanning of the transaction dataset to mine the frequent items. FP-Growth builds a compact-tree structure and uses the tree for frequent itemset mining and generating rules, using a divide and conquer approach.","title":"FP Growth"},{"location":"rre-association/#eclat","text":"Eclat is a simplified version of Apriori model, as only Support value is used, which shows how frequent a set of items occur.","title":"Eclat"},{"location":"rre-content/","text":"Content-Based Recommenders Product Similarity Product similarity recommendation is a content-based method which aims to recommends products by finding the most similar products to a query product based on the content. this content may be product title, description, images, category/subcategory, specification, etc. We can use item2vec, a shallow, single layer neural network to do this. This is based on Word2Vec, where single words are replaced with the item content. Each sentence of words are replaced by buckets of items. More on its architecture in these two articles 1 , 2 . Architecture from gensim.models import Word2Vec import pandas as pd model = Word2Vec ( df , min_count = 1 , vector_size = 50 , workers = 3 , \\ window = 3 , sg = 1 ) model . save ( \"word2vec.model\" ) model = Word2Vec . load ( \"word2vec.model\" ) # get numpy vector of a word vector = model . wv [ 'computer' ] # get other similar words, in cosine similarity scores ranked sims = model . wv . most_similar ( 'computer' , topn = 10 ) The arguments are as stated. arg desc vector_size no. of dimensions of the embeddings. default 100 window max distance between a target word and words around the target word. default 5. min_count no. count of words to consider when training the model; words with occurrence less than this count will be ignored. default 5 workers no. of partitions during training. default 3 sg training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. Evaluation We can use mAP (Mean Average Precision), MRR (Mean Reciprocal Rank), or NDCG (Normalized Discounted Cumulative Gain) Convert Item Mapping in Model While we used product names for the embedding mapping in the model, productid is usually the unique id for querying. We can do that by changing the mapping within the model, and avoid the need to have an additional mapping table and increase the latency. import os import pandas as pd from gensim import models from gensim.models import Word2Vec from gensim.models import KeyedVectors def convert_word2vec_mapping ( model , mapping , modelpath = \"newmapping.model\" , modeltemp = \"modeltemp.txt\" ): \"\"\"Replace word2vec model product name mapping with SKU Args: model (gensim model object): word2vec model mapping (df): dataframe with column names 'Product Name' & 'SKU' modelpath (str): path to final word2vec model modeltemp (str): path to temp model in text format Out: (gensim keyedVector model object) Notes: loading of a kv model has different syntax it does not have neural network weights etc. models.KeyedVectors.load(\"test.model\") - <https://github.com/RaRe-Technologies/gensim/issues/1936> - <https://stackoverflow.com/questions/58393090/how-to-save-as-a-gensim-word2vec-file> - <https://stackoverflow.com/questions/40936197/rename-gensim-word2vec-words-with-mapping> \"\"\" # get list of product names & vectors vectors = model . wv . vectors vocab = list ( model . wv . vocab . keys ()) # create a new model file if exist if os . path . isfile ( modeltemp ): os . remove ( modeltemp ) # save new mapping model as text with open ( modeltemp , \"a\" ) as file : file . write ( \" {} {} \\n \" . format ( vectors . shape [ 0 ], vectors . shape [ 1 ])) for vo , ve in zip ( vocab , vectors ): # convert np array of embeddings into string ve = [ str ( i ) for i in list ( ve )] ve = \" \" . join ( ve ) # query for SKU from product name sku = mapping [ mapping [ \"Product Name\" ] == vo ][ \"SKU\" ] . tolist () # validation for sku if len ( sku ) == 1 : sku = sku [ 0 ] file . write ( \" {} {} \\n \" . format ( sku , ve )) elif len ( sku ) == 0 : raise ValueError ( \"There are no SKUs for {} \" . format ( vo )) else : raise ValueError ( \"There is more than 1 SKUs for {} \" . format ( vo )) # load binary text model file newmodel = models . KeyedVectors . load_word2vec_format ( modeltemp , binary = False ) newmodel . save ( modelpath ) # delete temp model file os . remove ( modeltemp ) loadedmodel = \\ convert_word2vec_mapping ( loaded , mapping , modelpath = \"word2vec-map/newmapping.model\" ) The downside is that this model does not include model weights, so an original copy of the model needs to be kept if retraining is required. The synatx is also changed since it is just a KeyVector now. It is however, much smaller and faster. from gensim.models import KeyedVectors from gensim import models newmapping = models . KeyedVectors . load ( \"newmapping_cy.model\" ) newmapping . most_similar ( \"76577\" , topn = 5 ) [( '12345' , 0.9807120561599731 ), ( '12341' , 0.9798750281333923 ), ( '32142' , 0.9789506196975708 ), ( '54356' , 0.97857666015625 ), ( '53463' , 0.9785518050193787 )]","title":"Content"},{"location":"rre-content/#content-based-recommenders","text":"","title":"Content-Based Recommenders"},{"location":"rre-content/#product-similarity","text":"Product similarity recommendation is a content-based method which aims to recommends products by finding the most similar products to a query product based on the content. this content may be product title, description, images, category/subcategory, specification, etc. We can use item2vec, a shallow, single layer neural network to do this. This is based on Word2Vec, where single words are replaced with the item content. Each sentence of words are replaced by buckets of items. More on its architecture in these two articles 1 , 2 . Architecture from gensim.models import Word2Vec import pandas as pd model = Word2Vec ( df , min_count = 1 , vector_size = 50 , workers = 3 , \\ window = 3 , sg = 1 ) model . save ( \"word2vec.model\" ) model = Word2Vec . load ( \"word2vec.model\" ) # get numpy vector of a word vector = model . wv [ 'computer' ] # get other similar words, in cosine similarity scores ranked sims = model . wv . most_similar ( 'computer' , topn = 10 ) The arguments are as stated. arg desc vector_size no. of dimensions of the embeddings. default 100 window max distance between a target word and words around the target word. default 5. min_count no. count of words to consider when training the model; words with occurrence less than this count will be ignored. default 5 workers no. of partitions during training. default 3 sg training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.","title":"Product Similarity"},{"location":"rre-content/#evaluation","text":"We can use mAP (Mean Average Precision), MRR (Mean Reciprocal Rank), or NDCG (Normalized Discounted Cumulative Gain)","title":"Evaluation"},{"location":"rre-content/#convert-item-mapping-in-model","text":"While we used product names for the embedding mapping in the model, productid is usually the unique id for querying. We can do that by changing the mapping within the model, and avoid the need to have an additional mapping table and increase the latency. import os import pandas as pd from gensim import models from gensim.models import Word2Vec from gensim.models import KeyedVectors def convert_word2vec_mapping ( model , mapping , modelpath = \"newmapping.model\" , modeltemp = \"modeltemp.txt\" ): \"\"\"Replace word2vec model product name mapping with SKU Args: model (gensim model object): word2vec model mapping (df): dataframe with column names 'Product Name' & 'SKU' modelpath (str): path to final word2vec model modeltemp (str): path to temp model in text format Out: (gensim keyedVector model object) Notes: loading of a kv model has different syntax it does not have neural network weights etc. models.KeyedVectors.load(\"test.model\") - <https://github.com/RaRe-Technologies/gensim/issues/1936> - <https://stackoverflow.com/questions/58393090/how-to-save-as-a-gensim-word2vec-file> - <https://stackoverflow.com/questions/40936197/rename-gensim-word2vec-words-with-mapping> \"\"\" # get list of product names & vectors vectors = model . wv . vectors vocab = list ( model . wv . vocab . keys ()) # create a new model file if exist if os . path . isfile ( modeltemp ): os . remove ( modeltemp ) # save new mapping model as text with open ( modeltemp , \"a\" ) as file : file . write ( \" {} {} \\n \" . format ( vectors . shape [ 0 ], vectors . shape [ 1 ])) for vo , ve in zip ( vocab , vectors ): # convert np array of embeddings into string ve = [ str ( i ) for i in list ( ve )] ve = \" \" . join ( ve ) # query for SKU from product name sku = mapping [ mapping [ \"Product Name\" ] == vo ][ \"SKU\" ] . tolist () # validation for sku if len ( sku ) == 1 : sku = sku [ 0 ] file . write ( \" {} {} \\n \" . format ( sku , ve )) elif len ( sku ) == 0 : raise ValueError ( \"There are no SKUs for {} \" . format ( vo )) else : raise ValueError ( \"There is more than 1 SKUs for {} \" . format ( vo )) # load binary text model file newmodel = models . KeyedVectors . load_word2vec_format ( modeltemp , binary = False ) newmodel . save ( modelpath ) # delete temp model file os . remove ( modeltemp ) loadedmodel = \\ convert_word2vec_mapping ( loaded , mapping , modelpath = \"word2vec-map/newmapping.model\" ) The downside is that this model does not include model weights, so an original copy of the model needs to be kept if retraining is required. The synatx is also changed since it is just a KeyVector now. It is however, much smaller and faster. from gensim.models import KeyedVectors from gensim import models newmapping = models . KeyedVectors . load ( \"newmapping_cy.model\" ) newmapping . most_similar ( \"76577\" , topn = 5 ) [( '12345' , 0.9807120561599731 ), ( '12341' , 0.9798750281333923 ), ( '32142' , 0.9789506196975708 ), ( '54356' , 0.97857666015625 ), ( '53463' , 0.9785518050193787 )]","title":"Convert Item Mapping in Model"},{"location":"rre-intro/","text":"Recommender Systems A Recommender System aims to find and suggest items of likely interest based on the users\u2019 preferences Types There are various types of recommender systems, each with their own pros and cons. Terms There are various terms that are used in this branch of study which we will come across frequently. Noun Desc Long Tail Business strategy of selling niche products to many customers Cold Start A new user does not have any available data, so we can't make a personalised recommendation Serendipity Recommend an item that a customer likes, even though is not sought by them Implicit vs Explicit Directly given, e.g. user input rating VS inferred data from e.g. page-views, purchases etc.","title":"Introduction"},{"location":"rre-intro/#recommender-systems","text":"A Recommender System aims to find and suggest items of likely interest based on the users\u2019 preferences","title":"Recommender Systems"},{"location":"rre-intro/#types","text":"There are various types of recommender systems, each with their own pros and cons.","title":"Types"},{"location":"rre-intro/#terms","text":"There are various terms that are used in this branch of study which we will come across frequently. Noun Desc Long Tail Business strategy of selling niche products to many customers Cold Start A new user does not have any available data, so we can't make a personalised recommendation Serendipity Recommend an item that a customer likes, even though is not sought by them Implicit vs Explicit Directly given, e.g. user input rating VS inferred data from e.g. page-views, purchases etc.","title":"Terms"}]}