{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This documentation summarises various machine learning techniques in Python. While they cover the entire process of machine learning, only classical models are introduced. I have two accompanying websites to fill in those gaps: Neural Networks (WIP) AI Engineering A lot of the content are compiled from various resources, so please cite them appropriately if you are using.","title":"Introduction"},{"location":"#introduction","text":"This documentation summarises various machine learning techniques in Python. While they cover the entire process of machine learning, only classical models are introduced. I have two accompanying websites to fill in those gaps: Neural Networks (WIP) AI Engineering A lot of the content are compiled from various resources, so please cite them appropriately if you are using.","title":"Introduction"},{"location":"data-classimbalance/","text":"Class Imbalance In domains like predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. Techniques are available to correct for this. The imbalance-learn package provides an excellent range of algorithms for adjusting for imbalanced data. Install with pip install -U imbalanced-learn or conda install -c conda-forge imbalanced-learn . An important thing to note is that resampling must be done AFTER the train-test split , so as to prevent data leakage. Over-Sampling SMOTE (synthetic minority over-sampling technique) is a common and popular up-sampling technique. from imblearn.over_sampling import SMOTE smote = SMOTE () X_resampled , y_resampled = smote . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) ADASYN is one of the more advanced over sampling algorithms. from imblearn.over_sampling import ADASYN ada = ADASYN () X_resampled , y_resampled = ada . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) Under-Sampling from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler () X_resampled , y_resampled = rus . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) Cost-Sensitive Classification One can also make the classifier aware of the imbalanced data by incorporating the weights of the classes into a cost function. Intuitively, we want to give higher weight to minority class and lower weight to majority class. http://albahnsen.github.io/CostSensitiveClassification/index.html","title":"Class Imbalance"},{"location":"data-classimbalance/#class-imbalance","text":"In domains like predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. Techniques are available to correct for this. The imbalance-learn package provides an excellent range of algorithms for adjusting for imbalanced data. Install with pip install -U imbalanced-learn or conda install -c conda-forge imbalanced-learn . An important thing to note is that resampling must be done AFTER the train-test split , so as to prevent data leakage.","title":"Class Imbalance"},{"location":"data-classimbalance/#over-sampling","text":"SMOTE (synthetic minority over-sampling technique) is a common and popular up-sampling technique. from imblearn.over_sampling import SMOTE smote = SMOTE () X_resampled , y_resampled = smote . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) ADASYN is one of the more advanced over sampling algorithms. from imblearn.over_sampling import ADASYN ada = ADASYN () X_resampled , y_resampled = ada . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled )","title":"Over-Sampling"},{"location":"data-classimbalance/#under-sampling","text":"from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler () X_resampled , y_resampled = rus . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled )","title":"Under-Sampling"},{"location":"data-classimbalance/#cost-sensitive-classification","text":"One can also make the classifier aware of the imbalanced data by incorporating the weights of the classes into a cost function. Intuitively, we want to give higher weight to minority class and lower weight to majority class. http://albahnsen.github.io/CostSensitiveClassification/index.html","title":"Cost-Sensitive Classification"},{"location":"datasets/","text":"Datasets Playground datasets by sklearn, statesmodel, vega allows one to practise our ML modelling without the hassle of acquiring good data, which usually is the most time-consuming process. We will also cover Kaggle, the most important data science competition platform, and an excellent resource to learn from the best. Statsmodels In statsmodels , many R datasets can be obtained from the function sm.datasets.get_rdataset() . To view each dataset's description, print(duncan_prestige.__doc__) . import statsmodels.api as sm prestige = sm . datasets . get_rdataset ( \"Duncan\" , \"car\" , cache = True ) . data print prestige . head () # type income education prestige # accountant prof 62 86 82 # pilot prof 72 76 83 # architect prof 75 92 90 # author prof 55 90 76 # chemist prof 64 86 90 Sklearn There are five common toy datasets here. For others, see . To view each dataset's description, use print boston['DESCR'] . Noun Desc load_boston Load and return the boston house-prices dataset (regression) load_iris Load and return the iris dataset (classification) load_diabetes Load and return the diabetes dataset load_digits Load and return the digits dataset (classification) load_linnerud Load and return the linnerud dataset (multivariate regression) from sklearn.datasets import load_iris iris = load_iris () # Load iris into a dataframe and set the field names df = pd . DataFrame ( iris [ 'data' ], columns = iris [ 'feature_names' ]) df . head () # sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) # 0 5.1 3.5 1.4 0.2 # 1 4.9 3.0 1.4 0.2 # 2 4.7 3.2 1.3 0.2 # 3 4.6 3.1 1.5 0.2 # 4 5.0 3.6 1.4 0.2 print ( iris . target_names [: 5 ]) # ['setosa' 'versicolor' 'virginica'] print ( iris . target ) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 # 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # 2 2] # Change target to target_names & merge with main dataframe df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) print df [ 'species' ] . head () sepal length ( cm ) sepal width ( cm ) petal length ( cm ) petal width ( cm ) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name : species , dtype : category Categories ( 3 , object ): [ setosa , versicolor , virginica ] Vega-Datasets vega_datasets have to be installed via pip install vega_datasets . from vega_datasets import data df = data . iris () df . head () petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa To list all datasets, use list_datasets() data . list_datasets () [ '7zip' , 'airports' , 'anscombe' , 'barley' , 'birdstrikes' , 'budget' , \\ 'budgets' , 'burtin' , 'cars' , 'climate' , 'co2-concentration' , 'countries' , \\ 'crimea' , 'disasters' , 'driving' , 'earthquakes' , 'ffox' , 'flare' , \\ 'flare-dependencies' , 'flights-10k' , 'flights-200k' , 'flights-20k' , \\ 'flights-2k' , 'flights-3m' , 'flights-5k' , 'flights-airport' , 'gapminder' , \\ 'gapminder-health-income' , 'gimp' , 'github' , 'graticule' , 'income' , 'iris' , \\ 'jobs' , 'londonBoroughs' , 'londonCentroids' , 'londonTubeLines' , 'lookup_groups' , \\ 'lookup_people' , 'miserables' , 'monarchs' , 'movies' , 'normal-2d' , 'obesity' , \\ 'points' , 'population' , 'population_engineers_hurricanes' , 'seattle-temps' , \\ 'seattle-weather' , 'sf-temps' , 'sp500' , 'stocks' , 'udistrict' , 'unemployment' , \\ 'unemployment-across-industries' , 'us-10m' , 'us-employment' , 'us-state-capitals' , \\ 'weather' , 'weball26' , 'wheat' , 'world-110m' , 'zipcodes' ] Kaggle Kaggle is the most recognised online data science competition, with attractive rewards and recognition for being the top competitor. With a point system that encourages sharing, one can learnt from the top practitioners in the world. Progession System There are 4 types of expertise medals for specific work, namely Competition, Dataset, Notebook, and Discussion medals. For expertise, it is possible to obtain bronze, silver and gold medals. Performance Tier is an overall recognition for each of the expertise stated above, base on the number of medals accumulated. The various rankings are Novice, Contributor, Expert, Master, and Grandmaster. More at https://www.kaggle.com/progression Online Notebook Kaggle's notebook has a dedicated GPU and decent RAM for deep-learning neural networks. For installation of new packages, check \"internet\" under \"Settings\" in the right panel first, then in the notebook cell, !pip install package . To read dataset, you can see the file path at the right panel for \"Data\". It goes something like /kaggle/input/competition_folder_name . To download/export the prediction for submission, we can save the prediction like df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False) . To do a direct submission, we can commit the notebook, with the output saving directly as submission.csv , e.g., df_submission.to_csv(r'submisson.csv', index=False) .","title":"Datasets"},{"location":"datasets/#datasets","text":"Playground datasets by sklearn, statesmodel, vega allows one to practise our ML modelling without the hassle of acquiring good data, which usually is the most time-consuming process. We will also cover Kaggle, the most important data science competition platform, and an excellent resource to learn from the best.","title":"Datasets"},{"location":"datasets/#statsmodels","text":"In statsmodels , many R datasets can be obtained from the function sm.datasets.get_rdataset() . To view each dataset's description, print(duncan_prestige.__doc__) . import statsmodels.api as sm prestige = sm . datasets . get_rdataset ( \"Duncan\" , \"car\" , cache = True ) . data print prestige . head () # type income education prestige # accountant prof 62 86 82 # pilot prof 72 76 83 # architect prof 75 92 90 # author prof 55 90 76 # chemist prof 64 86 90","title":"Statsmodels"},{"location":"datasets/#sklearn","text":"There are five common toy datasets here. For others, see . To view each dataset's description, use print boston['DESCR'] . Noun Desc load_boston Load and return the boston house-prices dataset (regression) load_iris Load and return the iris dataset (classification) load_diabetes Load and return the diabetes dataset load_digits Load and return the digits dataset (classification) load_linnerud Load and return the linnerud dataset (multivariate regression) from sklearn.datasets import load_iris iris = load_iris () # Load iris into a dataframe and set the field names df = pd . DataFrame ( iris [ 'data' ], columns = iris [ 'feature_names' ]) df . head () # sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) # 0 5.1 3.5 1.4 0.2 # 1 4.9 3.0 1.4 0.2 # 2 4.7 3.2 1.3 0.2 # 3 4.6 3.1 1.5 0.2 # 4 5.0 3.6 1.4 0.2 print ( iris . target_names [: 5 ]) # ['setosa' 'versicolor' 'virginica'] print ( iris . target ) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 # 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # 2 2] # Change target to target_names & merge with main dataframe df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) print df [ 'species' ] . head () sepal length ( cm ) sepal width ( cm ) petal length ( cm ) petal width ( cm ) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name : species , dtype : category Categories ( 3 , object ): [ setosa , versicolor , virginica ]","title":"Sklearn"},{"location":"datasets/#vega-datasets","text":"vega_datasets have to be installed via pip install vega_datasets . from vega_datasets import data df = data . iris () df . head () petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa To list all datasets, use list_datasets() data . list_datasets () [ '7zip' , 'airports' , 'anscombe' , 'barley' , 'birdstrikes' , 'budget' , \\ 'budgets' , 'burtin' , 'cars' , 'climate' , 'co2-concentration' , 'countries' , \\ 'crimea' , 'disasters' , 'driving' , 'earthquakes' , 'ffox' , 'flare' , \\ 'flare-dependencies' , 'flights-10k' , 'flights-200k' , 'flights-20k' , \\ 'flights-2k' , 'flights-3m' , 'flights-5k' , 'flights-airport' , 'gapminder' , \\ 'gapminder-health-income' , 'gimp' , 'github' , 'graticule' , 'income' , 'iris' , \\ 'jobs' , 'londonBoroughs' , 'londonCentroids' , 'londonTubeLines' , 'lookup_groups' , \\ 'lookup_people' , 'miserables' , 'monarchs' , 'movies' , 'normal-2d' , 'obesity' , \\ 'points' , 'population' , 'population_engineers_hurricanes' , 'seattle-temps' , \\ 'seattle-weather' , 'sf-temps' , 'sp500' , 'stocks' , 'udistrict' , 'unemployment' , \\ 'unemployment-across-industries' , 'us-10m' , 'us-employment' , 'us-state-capitals' , \\ 'weather' , 'weball26' , 'wheat' , 'world-110m' , 'zipcodes' ]","title":"Vega-Datasets"},{"location":"datasets/#kaggle","text":"Kaggle is the most recognised online data science competition, with attractive rewards and recognition for being the top competitor. With a point system that encourages sharing, one can learnt from the top practitioners in the world. Progession System There are 4 types of expertise medals for specific work, namely Competition, Dataset, Notebook, and Discussion medals. For expertise, it is possible to obtain bronze, silver and gold medals. Performance Tier is an overall recognition for each of the expertise stated above, base on the number of medals accumulated. The various rankings are Novice, Contributor, Expert, Master, and Grandmaster. More at https://www.kaggle.com/progression Online Notebook Kaggle's notebook has a dedicated GPU and decent RAM for deep-learning neural networks. For installation of new packages, check \"internet\" under \"Settings\" in the right panel first, then in the notebook cell, !pip install package . To read dataset, you can see the file path at the right panel for \"Data\". It goes something like /kaggle/input/competition_folder_name . To download/export the prediction for submission, we can save the prediction like df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False) . To do a direct submission, we can commit the notebook, with the output saving directly as submission.csv , e.g., df_submission.to_csv(r'submisson.csv', index=False) .","title":"Kaggle"}]}