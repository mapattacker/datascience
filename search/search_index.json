{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This documentation summarises various machine learning techniques in Python. While they cover the entire process of machine learning, only classical models are introduced. I have two accompanying websites to fill in those gaps: Neural Networks (WIP) AI Engineering A lot of the content are compiled from various resources, so please cite them appropriately if you are using.","title":"Introduction"},{"location":"#introduction","text":"This documentation summarises various machine learning techniques in Python. While they cover the entire process of machine learning, only classical models are introduced. I have two accompanying websites to fill in those gaps: Neural Networks (WIP) AI Engineering A lot of the content are compiled from various resources, so please cite them appropriately if you are using.","title":"Introduction"},{"location":"data-classimbalance/","text":"Class Imbalance In domains like predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. Techniques are available to correct for this. The imbalance-learn package provides an excellent range of algorithms for adjusting for imbalanced data. Install with pip install -U imbalanced-learn or conda install -c conda-forge imbalanced-learn . An important thing to note is that resampling must be done AFTER the train-test split , so as to prevent data leakage. Over-Sampling SMOTE (synthetic minority over-sampling technique) is a common and popular up-sampling technique. from imblearn.over_sampling import SMOTE smote = SMOTE () X_resampled , y_resampled = smote . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) ADASYN is one of the more advanced over sampling algorithms. from imblearn.over_sampling import ADASYN ada = ADASYN () X_resampled , y_resampled = ada . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) Under-Sampling from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler () X_resampled , y_resampled = rus . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) Cost-Sensitive Classification One can also make the classifier aware of the imbalanced data by incorporating the weights of the classes into a cost function. Intuitively, we want to give higher weight to minority class and lower weight to majority class. http://albahnsen.github.io/CostSensitiveClassification/index.html","title":"Class Imbalance"},{"location":"data-classimbalance/#class-imbalance","text":"In domains like predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. Techniques are available to correct for this. The imbalance-learn package provides an excellent range of algorithms for adjusting for imbalanced data. Install with pip install -U imbalanced-learn or conda install -c conda-forge imbalanced-learn . An important thing to note is that resampling must be done AFTER the train-test split , so as to prevent data leakage.","title":"Class Imbalance"},{"location":"data-classimbalance/#over-sampling","text":"SMOTE (synthetic minority over-sampling technique) is a common and popular up-sampling technique. from imblearn.over_sampling import SMOTE smote = SMOTE () X_resampled , y_resampled = smote . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled ) ADASYN is one of the more advanced over sampling algorithms. from imblearn.over_sampling import ADASYN ada = ADASYN () X_resampled , y_resampled = ada . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled )","title":"Over-Sampling"},{"location":"data-classimbalance/#under-sampling","text":"from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler () X_resampled , y_resampled = rus . fit_sample ( X_train , y_train ) clf = LogisticRegression () clf . fit ( X_resampled , y_resampled )","title":"Under-Sampling"},{"location":"data-classimbalance/#cost-sensitive-classification","text":"One can also make the classifier aware of the imbalanced data by incorporating the weights of the classes into a cost function. Intuitively, we want to give higher weight to minority class and lower weight to majority class. http://albahnsen.github.io/CostSensitiveClassification/index.html","title":"Cost-Sensitive Classification"},{"location":"data-scaling/","text":"Feature Scaling Scaling is an important concept needed to change all features to the same scale. This allows for faster convergence during model training, and more uniform influence for all weights. Types of Scaling Introduction to Machine Learning in Python Tree-based models is not dependent on scaling, but non-tree models models, very often are hugely dependent on it. Outliers can affect certain scalers, and it is important to either remove them or choose a scalar that is robust towards them. sklearn sklearn's examples description by benalexkeen Standard Scaler It standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as z = (x - u) / s . import pandas pd from sklearn.preprocessing import StandardScaler X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) # note that the test set using the fitted scaler in train dataset # to transform in the test set X_test_scaled = scaler . transform ( X_test ) Min Max Scale Another way to normalise is to use the Min Max Scaler, which changes all features to be between 0 and 1, as defined below: .. figure:: images/minmaxscaler.png import pandas pd from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () from sklearn.linear_model import Ridge X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) linridge = Ridge ( alpha = 20.0 ) . fit ( X_train_scaled , y_train ) RobustScaler Works similarly to standard scaler except that it uses median and quartiles, instead of mean and variance. Good as it ignores data points that are outliers. Normalizer Scales each data point such that the feature vector has a Euclidean length of 1. Often used when the direction of the data matters, not the length of the feature vector. Pipeline Scaling have a chance of leaking the part of the test data in train-test split into the training data. This is especially inevitable when using cross-validation. We can scale the train and test datasets separately to avoid this.However, a more convenient way is to use the pipeline function in sklearn, which wraps the scaler and classifier together, and scale them separately during cross validation. Any other functions can also be input here, e.g., rolling window feature extraction, which also have the potential to have data leakage. from sklearn.pipeline import Pipeline # \"scaler\" & \"svm\" can be any name. But they must be placed in the correct order of processing pipe = Pipeline ([( \"scaler\" , MinMaxScaler ()), ( \"svm\" , SVC ())]) pipe . fit ( X_train , y_train ) Pipeline ( steps = [( 'scaler' , MinMaxScaler ( copy = True , feature_range = ( 0 , 1 ))), ( 'svm' , SVC ( C = 1.0 , decision_function_shape = None , degree = 3 , gamma = 'auto' , kernel = 'rbf' , max_iter =- 1 , probability = False , random_state = None , shrinking = True , tol = 0.001 , verbose = False ))]) pipe . score ( X_test , y_test ) # 0.95104895104895104 Persistance To save the fitted scaler to normalize new datasets, we can save it using pickle or joblib for reusing in the future.","title":"Feature Scaling"},{"location":"data-scaling/#feature-scaling","text":"Scaling is an important concept needed to change all features to the same scale. This allows for faster convergence during model training, and more uniform influence for all weights.","title":"Feature Scaling"},{"location":"data-scaling/#types-of-scaling","text":"Introduction to Machine Learning in Python Tree-based models is not dependent on scaling, but non-tree models models, very often are hugely dependent on it. Outliers can affect certain scalers, and it is important to either remove them or choose a scalar that is robust towards them. sklearn sklearn's examples description by benalexkeen","title":"Types of Scaling"},{"location":"data-scaling/#standard-scaler","text":"It standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as z = (x - u) / s . import pandas pd from sklearn.preprocessing import StandardScaler X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) # note that the test set using the fitted scaler in train dataset # to transform in the test set X_test_scaled = scaler . transform ( X_test )","title":"Standard Scaler"},{"location":"data-scaling/#min-max-scale","text":"Another way to normalise is to use the Min Max Scaler, which changes all features to be between 0 and 1, as defined below: .. figure:: images/minmaxscaler.png import pandas pd from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () from sklearn.linear_model import Ridge X_train , X_test , y_train , y_test = train_test_split ( X_crime , y_crime , random_state = 0 ) X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) linridge = Ridge ( alpha = 20.0 ) . fit ( X_train_scaled , y_train )","title":"Min Max Scale"},{"location":"data-scaling/#robustscaler","text":"Works similarly to standard scaler except that it uses median and quartiles, instead of mean and variance. Good as it ignores data points that are outliers.","title":"RobustScaler"},{"location":"data-scaling/#normalizer","text":"Scales each data point such that the feature vector has a Euclidean length of 1. Often used when the direction of the data matters, not the length of the feature vector.","title":"Normalizer"},{"location":"data-scaling/#pipeline","text":"Scaling have a chance of leaking the part of the test data in train-test split into the training data. This is especially inevitable when using cross-validation. We can scale the train and test datasets separately to avoid this.However, a more convenient way is to use the pipeline function in sklearn, which wraps the scaler and classifier together, and scale them separately during cross validation. Any other functions can also be input here, e.g., rolling window feature extraction, which also have the potential to have data leakage. from sklearn.pipeline import Pipeline # \"scaler\" & \"svm\" can be any name. But they must be placed in the correct order of processing pipe = Pipeline ([( \"scaler\" , MinMaxScaler ()), ( \"svm\" , SVC ())]) pipe . fit ( X_train , y_train ) Pipeline ( steps = [( 'scaler' , MinMaxScaler ( copy = True , feature_range = ( 0 , 1 ))), ( 'svm' , SVC ( C = 1.0 , decision_function_shape = None , degree = 3 , gamma = 'auto' , kernel = 'rbf' , max_iter =- 1 , probability = False , random_state = None , shrinking = True , tol = 0.001 , verbose = False ))]) pipe . score ( X_test , y_test ) # 0.95104895104895104","title":"Pipeline"},{"location":"data-scaling/#persistance","text":"To save the fitted scaler to normalize new datasets, we can save it using pickle or joblib for reusing in the future.","title":"Persistance"},{"location":"datasets/","text":"Datasets Playground datasets by sklearn, statesmodel, vega allows one to practise our ML modelling without the hassle of acquiring good data, which usually is the most time-consuming process. We will also cover Kaggle, the most important data science competition platform, and an excellent resource to learn from the best. Statsmodels In statsmodels , many R datasets can be obtained from the function sm.datasets.get_rdataset() . To view each dataset's description, print(duncan_prestige.__doc__) . import statsmodels.api as sm prestige = sm . datasets . get_rdataset ( \"Duncan\" , \"car\" , cache = True ) . data print prestige . head () # type income education prestige # accountant prof 62 86 82 # pilot prof 72 76 83 # architect prof 75 92 90 # author prof 55 90 76 # chemist prof 64 86 90 Sklearn There are five common toy datasets here. For others, see . To view each dataset's description, use print boston['DESCR'] . Noun Desc load_boston Load and return the boston house-prices dataset (regression) load_iris Load and return the iris dataset (classification) load_diabetes Load and return the diabetes dataset load_digits Load and return the digits dataset (classification) load_linnerud Load and return the linnerud dataset (multivariate regression) from sklearn.datasets import load_iris iris = load_iris () # Load iris into a dataframe and set the field names df = pd . DataFrame ( iris [ 'data' ], columns = iris [ 'feature_names' ]) df . head () # sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) # 0 5.1 3.5 1.4 0.2 # 1 4.9 3.0 1.4 0.2 # 2 4.7 3.2 1.3 0.2 # 3 4.6 3.1 1.5 0.2 # 4 5.0 3.6 1.4 0.2 print ( iris . target_names [: 5 ]) # ['setosa' 'versicolor' 'virginica'] print ( iris . target ) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 # 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # 2 2] # Change target to target_names & merge with main dataframe df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) print df [ 'species' ] . head () sepal length ( cm ) sepal width ( cm ) petal length ( cm ) petal width ( cm ) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name : species , dtype : category Categories ( 3 , object ): [ setosa , versicolor , virginica ] Vega-Datasets vega_datasets have to be installed via pip install vega_datasets . from vega_datasets import data df = data . iris () df . head () petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa To list all datasets, use list_datasets() data . list_datasets () [ '7zip' , 'airports' , 'anscombe' , 'barley' , 'birdstrikes' , 'budget' , \\ 'budgets' , 'burtin' , 'cars' , 'climate' , 'co2-concentration' , 'countries' , \\ 'crimea' , 'disasters' , 'driving' , 'earthquakes' , 'ffox' , 'flare' , \\ 'flare-dependencies' , 'flights-10k' , 'flights-200k' , 'flights-20k' , \\ 'flights-2k' , 'flights-3m' , 'flights-5k' , 'flights-airport' , 'gapminder' , \\ 'gapminder-health-income' , 'gimp' , 'github' , 'graticule' , 'income' , 'iris' , \\ 'jobs' , 'londonBoroughs' , 'londonCentroids' , 'londonTubeLines' , 'lookup_groups' , \\ 'lookup_people' , 'miserables' , 'monarchs' , 'movies' , 'normal-2d' , 'obesity' , \\ 'points' , 'population' , 'population_engineers_hurricanes' , 'seattle-temps' , \\ 'seattle-weather' , 'sf-temps' , 'sp500' , 'stocks' , 'udistrict' , 'unemployment' , \\ 'unemployment-across-industries' , 'us-10m' , 'us-employment' , 'us-state-capitals' , \\ 'weather' , 'weball26' , 'wheat' , 'world-110m' , 'zipcodes' ] Kaggle Kaggle is the most recognised online data science competition, with attractive rewards and recognition for being the top competitor. With a point system that encourages sharing, one can learnt from the top practitioners in the world. Progession System The progression system in Kaggle are as follows. There are 4 types of expertise medals for specific work, namely: Competition Dataset Notebook Discussion For each expertise, it is possible to obtain bronze, silver and gold medals. Performance Tier is an overall recognition for each of the expertise stated above, base on the number of medals accumulated. The various rankings are: Novice Contributor Expert Master Grandmaster Online Notebook Kaggle's notebook has a dedicated GPU and decent RAM for deep-learning neural networks. For installation of new packages, check \"internet\" under \"Settings\" in the right panel first, then in the notebook cell, !pip install package . To read dataset, you can see the file path at the right panel for \"Data\". It goes something like /kaggle/input/competition_folder_name . To download/export the prediction for submission, we can save the prediction like df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False) . To do a direct submission, we can commit the notebook, with the output saving directly as submission.csv , e.g., df_submission.to_csv(r'submisson.csv', index=False) .","title":"Datasets"},{"location":"datasets/#datasets","text":"Playground datasets by sklearn, statesmodel, vega allows one to practise our ML modelling without the hassle of acquiring good data, which usually is the most time-consuming process. We will also cover Kaggle, the most important data science competition platform, and an excellent resource to learn from the best.","title":"Datasets"},{"location":"datasets/#statsmodels","text":"In statsmodels , many R datasets can be obtained from the function sm.datasets.get_rdataset() . To view each dataset's description, print(duncan_prestige.__doc__) . import statsmodels.api as sm prestige = sm . datasets . get_rdataset ( \"Duncan\" , \"car\" , cache = True ) . data print prestige . head () # type income education prestige # accountant prof 62 86 82 # pilot prof 72 76 83 # architect prof 75 92 90 # author prof 55 90 76 # chemist prof 64 86 90","title":"Statsmodels"},{"location":"datasets/#sklearn","text":"There are five common toy datasets here. For others, see . To view each dataset's description, use print boston['DESCR'] . Noun Desc load_boston Load and return the boston house-prices dataset (regression) load_iris Load and return the iris dataset (classification) load_diabetes Load and return the diabetes dataset load_digits Load and return the digits dataset (classification) load_linnerud Load and return the linnerud dataset (multivariate regression) from sklearn.datasets import load_iris iris = load_iris () # Load iris into a dataframe and set the field names df = pd . DataFrame ( iris [ 'data' ], columns = iris [ 'feature_names' ]) df . head () # sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) # 0 5.1 3.5 1.4 0.2 # 1 4.9 3.0 1.4 0.2 # 2 4.7 3.2 1.3 0.2 # 3 4.6 3.1 1.5 0.2 # 4 5.0 3.6 1.4 0.2 print ( iris . target_names [: 5 ]) # ['setosa' 'versicolor' 'virginica'] print ( iris . target ) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 # 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # 2 2] # Change target to target_names & merge with main dataframe df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) print df [ 'species' ] . head () sepal length ( cm ) sepal width ( cm ) petal length ( cm ) petal width ( cm ) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name : species , dtype : category Categories ( 3 , object ): [ setosa , versicolor , virginica ]","title":"Sklearn"},{"location":"datasets/#vega-datasets","text":"vega_datasets have to be installed via pip install vega_datasets . from vega_datasets import data df = data . iris () df . head () petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa To list all datasets, use list_datasets() data . list_datasets () [ '7zip' , 'airports' , 'anscombe' , 'barley' , 'birdstrikes' , 'budget' , \\ 'budgets' , 'burtin' , 'cars' , 'climate' , 'co2-concentration' , 'countries' , \\ 'crimea' , 'disasters' , 'driving' , 'earthquakes' , 'ffox' , 'flare' , \\ 'flare-dependencies' , 'flights-10k' , 'flights-200k' , 'flights-20k' , \\ 'flights-2k' , 'flights-3m' , 'flights-5k' , 'flights-airport' , 'gapminder' , \\ 'gapminder-health-income' , 'gimp' , 'github' , 'graticule' , 'income' , 'iris' , \\ 'jobs' , 'londonBoroughs' , 'londonCentroids' , 'londonTubeLines' , 'lookup_groups' , \\ 'lookup_people' , 'miserables' , 'monarchs' , 'movies' , 'normal-2d' , 'obesity' , \\ 'points' , 'population' , 'population_engineers_hurricanes' , 'seattle-temps' , \\ 'seattle-weather' , 'sf-temps' , 'sp500' , 'stocks' , 'udistrict' , 'unemployment' , \\ 'unemployment-across-industries' , 'us-10m' , 'us-employment' , 'us-state-capitals' , \\ 'weather' , 'weball26' , 'wheat' , 'world-110m' , 'zipcodes' ]","title":"Vega-Datasets"},{"location":"datasets/#kaggle","text":"Kaggle is the most recognised online data science competition, with attractive rewards and recognition for being the top competitor. With a point system that encourages sharing, one can learnt from the top practitioners in the world.","title":"Kaggle"},{"location":"datasets/#progession-system","text":"The progression system in Kaggle are as follows. There are 4 types of expertise medals for specific work, namely: Competition Dataset Notebook Discussion For each expertise, it is possible to obtain bronze, silver and gold medals. Performance Tier is an overall recognition for each of the expertise stated above, base on the number of medals accumulated. The various rankings are: Novice Contributor Expert Master Grandmaster","title":"Progession System"},{"location":"datasets/#online-notebook","text":"Kaggle's notebook has a dedicated GPU and decent RAM for deep-learning neural networks. For installation of new packages, check \"internet\" under \"Settings\" in the right panel first, then in the notebook cell, !pip install package . To read dataset, you can see the file path at the right panel for \"Data\". It goes something like /kaggle/input/competition_folder_name . To download/export the prediction for submission, we can save the prediction like df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False) . To do a direct submission, we can commit the notebook, with the output saving directly as submission.csv , e.g., df_submission.to_csv(r'submisson.csv', index=False) .","title":"Online Notebook"}]}